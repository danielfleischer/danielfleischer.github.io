<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://danielfleischer.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://danielfleischer.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-08-27T16:35:02+00:00</updated><id>https://danielfleischer.github.io/feed.xml</id><title type="html">blank</title><subtitle>Personal website for Daniel Fleischer. </subtitle><entry><title type="html">Built an MCP server for LLMs to search email from terminal | Daniel Fleischer posted on the topic | LinkedIn</title><link href="https://danielfleischer.github.io/blog/2025/built-an-mcp-server-for-llms-to-search-email-from-terminal-daniel-fleischer-posted-on-the-topic-linkedin/" rel="alternate" type="text/html" title="Built an MCP server for LLMs to search email from terminal | Daniel Fleischer posted on the topic | LinkedIn"/><published>2025-06-23T00:00:00+00:00</published><updated>2025-06-23T00:00:00+00:00</updated><id>https://danielfleischer.github.io/blog/2025/built-an-mcp-server-for-llms-to-search-email-from-terminal--daniel-fleischer-posted-on-the-topic--linkedin</id><content type="html" xml:base="https://danielfleischer.github.io/blog/2025/built-an-mcp-server-for-llms-to-search-email-from-terminal-daniel-fleischer-posted-on-the-topic-linkedin/"><![CDATA[<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>          Agree &amp; Join LinkedIn
        
  By clicking Continue to join or sign in, you agree to LinkedIn’s User Agreement, Privacy Policy, and Cookie Policy.

            AI Research Engineer @ Intel Labs
        📬 I built an MCP server that lets LLMs search my email from the terminal
</code></pre></div></div> <p>The server connects Claude to email search via the mu CLI tool. Now I just ask it things like: “Find emails with PDF attachments from last April” ⚡</p> <p>🛠 No custom frontend. No heavy framework. Just a CLI tool made smarter.</p> <p>💡 I learned that MCP servers are basically API translators — they take complex developer SDKs and flatten them into simple function calls that LLMs can actually use.</p> <p>🎯 The bigger picture: This pattern can breathe new life into existing CLI tools and services. Complex APIs → Simple, declarative functions → Natural language queries.</p> <p>This isn’t a product — just an experiment in stitching new capabilities into existing workflows. Code here: https://lnkd.in/eT2fJBSv</p> <p>mu email indexer and searcher: https://github.com/djcb/mu</p> <p>#MCP #LLM #EmailSearch #OpenSource #AI</p> <p>What existing tools would you want to make LLM-friendly? 🤔 To view or add a comment, sign in</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>        1,428 followers
      
                Create your free account or sign in to continue your search
              
          or
        
  By clicking Continue to join or sign in, you agree to LinkedIn’s User Agreement, Privacy Policy, and Cookie Policy.

            New to LinkedIn? Join now
          
                      or
                    
                New to LinkedIn? Join now
              
  By clicking Continue to join or sign in, you agree to LinkedIn’s User Agreement, Privacy Policy, and Cookie Policy.
</code></pre></div></div>]]></content><author><name></name></author><summary type="html"><![CDATA[📬 I built an MCP server that lets LLMs search my email from the terminal The server connects Claude to email search via the mu CLI tool. Now I just ask it things like: "Find emails with PDF attachments from last April" ⚡ 🛠 No custom frontend. No heavy framework. Just a CLI tool made smarter. 💡 I learned that MCP servers are basically API translators — they take complex developer SDKs and flatten them into simple function calls that LLMs can actually use. 🎯 The bigger picture: This pattern can breathe new life into existing CLI tools and services. Complex APIs → Simple, declarative functions → Natural language queries. This isn’t a product — just an experiment in stitching new capabilities into existing workflows. Code here: https://lnkd.in/eT2fJBSv mu email indexer and searcher: https://github.com/djcb/mu #MCP #LLM #EmailSearch #OpenSource #AI What existing tools would you want to make LLM-friendly? 🤔]]></summary></entry><entry><title type="html">Summarize Hacker News Posts with Haystack &amp;amp; OPEA | Haystack</title><link href="https://danielfleischer.github.io/blog/2025/summarize-hacker-news-posts-with-haystack-opea-haystack/" rel="alternate" type="text/html" title="Summarize Hacker News Posts with Haystack &amp;amp; OPEA | Haystack"/><published>2025-06-10T00:00:00+00:00</published><updated>2025-06-10T00:00:00+00:00</updated><id>https://danielfleischer.github.io/blog/2025/summarize-hacker-news-posts-with-haystack--opea--haystack</id><content type="html" xml:base="https://danielfleischer.github.io/blog/2025/summarize-hacker-news-posts-with-haystack-opea-haystack/"><![CDATA[<p>Build a RAG pipeline to fetch live Hacker News posts and summarize them with a local LLM endpointWelcome to this step-by-step tutorial where we’ll build a simple Retrieval-Augmented Generation (RAG) pipeline using Haystack and OPEA. We’ll fetch the newest Hacker News posts, feed them to a lightweight LLM endpoint (OPEAGenerator), and generate concise one-sentence summaries (based on this notebook). Let’s dive in! 🎉In modern GenAI applications, having a flexible, performant, and scalable platform is essential. OPEA (Open Platform for Enterprise AI) is an open, model-agnostic framework for building and operating composable GenAI solutions. It provides:In this demo, we’ll use an OPEA LLM endpoint in a Haystack pipeline, giving you:In this tutorial, we’ll build a simple RAG pipeline that fetches the newest Hacker News posts, sends them to a local OPEA endpoint running a Qwen/Qwen2.5-7B-Instruct demo model, and produces concise one-sentence summaries. Of course, you can replace our example model with any other OPEA-served model, making this pattern both lightweight for prototyping and powerful for real-world deployments. Let’s get started! 🚀Make sure you have:NOTE: As a reference, here is a Docker Compose recipe to get you started. OPEA LLM service can be configured to use a variety of model serving backends like TGI, vLLM, ollama, OVMS… and offers validated runtime settings for good performance on various hardware’s including Intel Gaudi. In this example, it creates an OPEA LLM service with a TGI backend. See the documentation for LLM Generation. The code is based on OPEA LLM example and OPEA TGI example.To run, call LLM_MODEL_ID=Qwen/Qwen2.5-7B-Instruct docker compose up.We’ll create a custom Haystack component, HackernewsNewestFetcher, that:We use the OPEAGenerator to call our LLM over HTTP. Here, we point to a local endpoint serving the Qwen/Qwen2.5-7B-Instruct model:Using PromptBuilder, we define a Jinja-style template that:We wire up the components in a Pipeline:Fetch and summarize the top 2 newest Hacker News posts:Beautiful, concise summaries in seconds! ✨In this tutorial, we built a full RAG pipeline:Feel free to extend this setup with more advanced retrieval, caching, or different LLM backends. Happy coding! 🛠️🔥 Building products, technology and solutions for LLM-enabled applications.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Build a RAG pipeline to fetch live Hacker News posts and summarize them with a local LLM endpoint]]></summary></entry><entry><title type="html">המודל שלכם הוזה? כך תבנו בעצמכם מערכת RAG | גיקטיים</title><link href="https://danielfleischer.github.io/blog/2024/rag/" rel="alternate" type="text/html" title="המודל שלכם הוזה? כך תבנו בעצמכם מערכת RAG | גיקטיים"/><published>2024-12-01T00:00:00+00:00</published><updated>2024-12-01T00:00:00+00:00</updated><id>https://danielfleischer.github.io/blog/2024/-------rag--</id><content type="html" xml:base="https://danielfleischer.github.io/blog/2024/rag/"><![CDATA[<p>חיפושים חמים: מערכות RAG יכולות לפתור את תופעת ההזיות של LLMs, אבל כדי לבנות אחת טובה תצטרכו לקבל הרבה החלטות קריטיות (ולקוות שתגיעו הכי קרוב למושלם) האתגר העיקרי בבניית מערכת RAG הוא להגיע לרמת איכות ודיוק מקסימליים (צילום: Dreamstime)מאת דניאל פליישר, חוקר AI ב-Intel Labsמודלי שפה גדולים (LLMs) מציעים פתרונות מתקדמים לאתגרי פיתוח, אך לעתים, מה לעשות, הם שוגים. המטרה של מערכות RAG היא לפתור את הבעיה הזאת, באמצעות גישה חדשה לשיפור דיוק ואמינות המודלים. איך הן עושות את זה, האם ההייפ מוצדק וכיצד נוכל לבנות מערכת RAG משלנו? כדי לענות על השאלות האלה, בדקתי את האתגרים בגישה זו אל מול היתרונות שהיא מציעה בשדרוג האינטראקציה שלנו עם AI.לפני שנצלול, בואו נדבר רגע על LLMs. למעשה, מדובר בייצוג מתמטי של שפה טבעית באמצעות הסתברויות: מודל השפה יכול לחשב הסתברות לכל משפט בשפה שבה אימנו אותו, וכך הוא יכול לייצר משפטים חדשים, מילה אחר מילה. מודל השפה למעשה “ממשיך את המשפט”: מזינים התחלה של משפט, והוא ממשיך. השימושים הם כמעט אינסופיים: אפשר לבקש מהמודל לענות על שאלות, לסכם טקסטים, לחבר שירים, לכתוב פונקציות בשפות תכנות שונות ואפילו לייצר תמונות בצורה סינתטית.אבל לא הכל מושלם, ואחד הפגמים בתפקוד של LLMs הוא ההזיות המוכרות לכולנו – אותם מקרים שבהם מודלי שפה מספקים תשובות שגויות עובדתית, לא רלוונטיות או מופרכות לחלוטין. כדי להוסיף חטא על פשע, מודלי השפה לא תמיד מסייגים את תשובותיהם ועונים בביטחון מלא, ולכן ההזיות הן מסוכנות וקשות לגילוי.אחת הדרכים להתמודד עם תופעת ההזיות היא RAG: Retrieval Augmented Generation (או בעברית, “כתיבת טקסט בעזרת שליפה והעשרה בידע”). בשיטה זו, מודל השפה נעזר בידע חיצוני שאותו הוא שולף או מקבל לצורך השלמת המשימה, כדי שהתשובות יהיו מדויקות יותר. RAG הוא שם כולל לכל הטכניקות לבניית מערכות שבהן מחברים מודלי שפה לידע חיצוני לצורך שיפור התוצאות.מערכת RAG כוללת מאגר ידע כגון בסיס נתונים, אוסף של מסמכים ואפילו כלי חיפוש בגוגל. כשאנחנו מזינים בקשה למערכת, היא מאתרת פיסות מידע רלוונטיות בתוך מאגרי הידע. אלה עוברים יחד עם הבקשה המקורית אל מודל השפה, והוא עונה על הבקשה – בתקווה שהתשובה תהיה מלאה, מדויקת ורלוונטית, בזכות המידע שהמודל קיבל. אחזור מידע (retrieval) רלוונטי והעברתו למודל השפה משפרים באופן מוכח ומשמעותי את הביצועים של מודלי שפה במגוון משימות המצריכות ידע – כולל מענה על שאלות (Q&amp;A), מיון (Classification), סיכום ועוד.למערכות RAG ישנם שימושים שונים, והיתרון המובהק שלהן הוא היכולת לרתום מאגרי ידע במטרה להשלים את המשימה. מאגרים אלה הם הדרך העיקרית להוסיף ידע חדש ולעדכן ידע קיים במודלי השפה, ולכן הם מוצלחים כל כך. בנוסף, בחירה דינמית של מאגרי הידע פותחת את הדלת למודלי שפה מותאמים אישית.קחו לדוגמה עוזר אישי (digital assistant). במקרה הזה מערכת ה-RAG מחוברת למאגרי ידע מקצועיים. בזכות היכולות של מודל השפה, המערכת יכולה לשוחח בשפה טבעית על מגוון נושאים טכניים, לייעץ ולענות על שאלות. בנוסף, המערכת יכולה להפנות את המשתמש למקורות הידע עצמם (באמצעות ציטוט, citation), כמעין עוזר מחקר.דוגמה נוספת היא שימוש בידע של המשתמש עצמו, כלומר – מאגר הידע מבוסס על הדאטה של המשתמש, וכך מערכת ה-RAG מתאימה לו את תשובותיה. המשתמש יכול להעלות אוסף קבצים אישיים למערכת ולבקש ממנה לסכם נושאים המופיעים במסמכים, לשאול היכן נמצא דיון בנושא כזה או אחר, לבקש שתרכיב מצגת סיכום מהמסמכים וכן הלאה.כאשר ניגשים לבנות מערכת RAG צריך לקבל החלטות רבות. בשלב הראשון עלינו לבחור מודל שפה – פתוח או סגור. מודל סגור הוא מודל מסחרי איתו עובדים בעזרת ממשק API, ואין לנו גישה לאופן פעולת המודל או המשקולות שלו. לעומתו, במודל פתוח גם הקוד וגם המשקולות נגישים לבחינה, אימון והגשה (inference). את המודל הפתוח ניתן להתאים לעולם התוכן שלנו בעזרת אימון fine tuning. בחלק מהמקרים הגשה עצמית של מודלים פתוחים עשויה להיות עדיפה על פני מודלים סגורים, אך היא מצריכה מיומנות.החלק השני בבניית מערכת RAG הוא חיבור למאגרי ידע. לחיבור הזה ישנם היבטים רבים, הכוללים את אופן אינדוקס הידע, עיבוד מקדים של הטקסט והתמונות, חלוקה לפסקאות או משפטים, ניקוי, עיבוד של דאטה טבלאי, המרת הדאטה לייצוג וקטורי (vector embedding) שיכול לשפר את איכות החיפוש, חיפוש מבוסס מילים, חיפוש סמנטי או שילוב שלהם, מספר הדוגמאות לאחזור, מיון רלוונטיות, סינון, שיכתוב הדוגמאות, סיכום ועוד. זה ממש על קצה המזלג, ורק על השלב הזה יכולתי לכתוב מאמר שלם. החלק הזה קריטי, כי כמו שאומרים – garbage in, garbage out: אם אחזור הידע יהיה לא מדויק ולא רלוונטי מספיק, איכות התשובות של מודל השפה תפגענה מיד, והסיכוי להזיות יגדל.לאחר מכן ניתן להוסיף מרכיבים למערכת: מודלי שפה נוספים שנבחרים בצורה דינמית על פי אופי המשימה; שימוש בכמה מאגרי ידע במקביל; שימוש בכלים המודדים את איכות המסמכים שנמצאו, כך שניתן יהיה ללקט את קטעי המידע הרלוונטיים ביותר מתוך המסמכים שחזרו; מודלי שפה שיודעים לבקש ביצוע אחזור נוסף אם הם לא מצאו עדיין את התשובה שחיפשו; והרשימה ממשיכה, כיאה לתחום מחקר פעיל ביותר.האתגר העיקרי בבניית מערכת RAG הוא להגיע לרמת איכות ודיוק מקסימליים. כל חלקי המערכת משפיעים על התשובות הסופיות, ולכן מוכרחים לתכנן אותה בקפדנות. כחלק מההכנה, כדאי לבחון ולהשוות מודלי שפה על מנת להגיע לשילוב ראוי של עלות מול דיוק. בחירת מאגרי הידע, אופי האחסון, אחזור והצגת הידע למודל הם קריטיים בייצור תשובות נכונות ורלוונטיות בזמן ריצה סביר. כדי לבחון את טיב המערכת שבניתם, ניתן להשתמש בדוגמאות מתויגות (דוגמאות המכילות תשובות ידועות מראש) ולבצע בחינה ידנית או אוטומטית של התוצאות.המורכבות הגדולה נובעת בעיקר מההתנהגות הלא צפויה לעתים של LLMs. לדוגמה, בחלק קטן מהמקרים מודל השפה ישגה, אפילו שהידע שקיבל מכיל את התשובה הנכונה. הסיבות לא ברורות נובעות מסתירה בין הידע הפנימי של המודל לידע המוצג בפניו. הצוות שלנו חוקר דרכים בהן ניתן לשלוט בהתנהגות המודלים במקרים כאלה, בעזרת הוראות מותאמות ואימון נוסף.יחד עם זאת, ניתן לשפר את איכות המערכת כולה באמצעות התאמת המודל לביצוע משימות RAG. לצורך כך, הצוות שלנו פיתח כלי קוד פתוח שמאפשר לאמן ולשפר את יכולות ה-RAG של מודלי שפה.כדי לבנות מערכת RAG איכותית ומדויקת, נדרשת הבנה עמוקה של ההיבטים השונים שלה. כמובן שאי אפשר לוותר על תהליך ניסוי וטעיה, שעוזר לשפוך אור על הפשרות השונות הכרוכות בעיצוב המערכת. רק כך נוכל לבנות את המערכת האידיאלית לבעיה שאותה אנחנו מנסים לפתור.מערכות RAG מייצגות ארכיטקטורה חדשה המשלבת מודלי שפה עם מאגרי נתונים. בזכות השילוב הזה יש להם פוטנציאל לבצע משימות עתירות ידע, כגון עוזרים דיגיטליים מותאמים אישית, צ’אטבוט בשירות לקוחות ומערכות ידע ארגוניות. ואם לשפוט לפי המחקר הפעיל בתחום, זוהי רק ההתחלה.הכותב הוא חוקר במעבדת ה־NLP בארגון Intel Labs. במעבדה נחקרות סוגיות הקשורות למודלי שפה כגון RAG ,Efficient Inference, עבודה עם קונטקסטים ארוכים, שימוש בסוכנים ועוד.אינטל ממשיכה להוביל את תחום הבינה המלאכותית עם הפתרונות המתקדמים ביותר לתעשייה. מעבדי Xeon מהדור השישי והמאיצים הייעודיים Gaudi 3 מאפשרים לארגונים להאיץ את פיתוח והטמעת יישומי AI בקנה מידה גדול, תוך שמירה על יעילות גבוהה ותמורה כלכלית יוצאת דופן. אינטל מציעה גישה פתוחה וגמישה המאפשרת שילוב חלק של חומרה ותוכנה ממגוון ספקים, ובכך נותנת לארגונים את הכלים הדרושים להם להאצת השימוש ב- GenAI ובמודלים גדולים, כמו גם להקטנת התלות במערכות קנייניות של יצרנים אחרים. מרכזי הפיתוח של אינטל בישראל, שממוקמים בחיפה, פתח תקווה, ירושלים וקריית גת, משחקים תפקיד מפתח בעיצוב הדור הבא של טכנולוגיות עיבוד ו-AI , וממשיכים להניע את החדשנות הגלובלית של החברה באמצעות שילוב של ביצועים, גמישות וחדשנות מתמדת.כאן אפשר לבחור תחומי עניין, ואנחנו נתאים לך כתבות באופן אישי. הכתבות יופיעו כאן וברחבי האתר, והסימון שלנו יהיה הנה הכתבות שהתאמנו לך אישית. רוצה לרענן העדפות? בבקשה, אנחנו לא שופטים זה המקום להכיר את החברות, המשרדים וכל מי שעושה את ההייטק בישראל (ויש גם מלא משרות פתוחות!) #תוכן מקודםהניוזלטר שלנו עושה את האקסטרה מייל עם העדכונים והחדשות של השבוע© כל הזכויות שמורות לגיקטייםפיתוח אתריםdesigned by designed by  | פיתוח אתריםבגלל זה אנחנו מקפידים שהן לא יציקו, אבל הן מאפשרות לנו לתת לכם תוכן בחינם.</p> <p>פרסומות עוזרות לנו להתקיים ולהתמקד במה שחשוב: ליצור עבורך תוכן מקצועי ומעניין. כדי להמשיך ליהנות מגיקטיים, כדאי להסיר את החסימה מהאתר שלנו. אנחנו מבטיחים לא להציף.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[מערכות RAG יכולות לפתור את תופעת ההזיות של LLMs, אבל כדי לבנות אחת טובה תצטרכו לקבל הרבה החלטות קריטיות (ולקוות שתגיעו הכי קרוב למושלם)]]></summary></entry><entry><title type="html">Intel Labs Introduces RAG-FiT Open-Source Framework for Retrieval Augmented Generation in LLMs - Intel Community</title><link href="https://danielfleischer.github.io/blog/2024/intel-labs-introduces-rag-fit-open-source-framework-for-retrieval-augmented-generation-in-llms-intel-community/" rel="alternate" type="text/html" title="Intel Labs Introduces RAG-FiT Open-Source Framework for Retrieval Augmented Generation in LLMs - Intel Community"/><published>2024-10-09T00:00:00+00:00</published><updated>2024-10-09T00:00:00+00:00</updated><id>https://danielfleischer.github.io/blog/2024/intel-labs-introduces-rag-fit-open-source-framework-for-retrieval-augmented-generation-in-llms---intel-community</id><content type="html" xml:base="https://danielfleischer.github.io/blog/2024/intel-labs-introduces-rag-fit-open-source-framework-for-retrieval-augmented-generation-in-llms-intel-community/"><![CDATA[<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>                        Success!  Subscription added.
                    
                   
                
                    
                        Success!  Subscription removed.
                    
                    
                
                    
                        Sorry, you must verify to complete this action. Please click the verification link in your email. You may re-send via your
                        profile.
                    
                Scott Bair is a key voice at Intel Labs, sharing insights into innovative research for inventing tomorrow’s technology. Intel Labs researchers Daniel Fleischer, Moshe Berchansky, and Moshe Wasserblat collaborated on RAG-FiT.HighlightsIntel Labs introduces RAG-FiT, an open-source framework for augmenting large language models (LLMs) for retrieval-augmented generation (RAG) use cases. Available under an Apache 2.0 license, RAG-FiT integrates data creation, training, inference, and evaluation into a single workflow, assisting in the creation of data-augmented datasets for training and evaluating LLMs in RAG settings. This integration enables rapid prototyping and experimentation with various RAG techniques, allowing users to easily generate datasets and train RAG models using internal or specialized knowledge sources.The library assists in creating data to train models using parameter-efficient fine-tuning (PEFT), which allows users to finetune a subset of parameters in a model. The Python-based framework is designed to serve as an end-to-end experimentation environment, enabling users to quickly prototype and experiment with different RAG techniques, including data selection, aggregation and filtering, retrieval, text processing, document ranking, few-shot generation, prompt design using templates, fine-tuning, inference, and evaluation.To demonstrate the effectiveness of the RAG-FiT framework (formerly known as RAG Foundry), Intel Labs researchers augmented and fine-tuned Llama 3.0 and Phi-3 models with diverse RAG configurations, showcasing consistent improvements across three knowledge-intensive question-answering tasks.Using RAG Systems to Address LLM LimitationsDespite their impressive capabilities, LLMs have inherent limitations. These models can produce plausible sounding but incorrect or nonsensical answers, struggle with factual accuracy, lack access to up-to-date information after their training cutoff, and struggle in attending to relevant information in large contexts.RAG enhances LLMs performance by integrating external information using retrieval mechanisms. Retrieving specific data from knowledge bases outside the model can effectively address knowledge limitations, which in turn can reduce hallucinations, improve the relevance of generated content, provide interpretability and could be vastly more cost efficient. Furthermore, recent research indicates that fine-tuning LLMs for RAG can achieve state-of-the-art performance, surpassing that of larger proprietary models.How RAG-FiT WorksAs an experimentation environment for researchers, the backbone of the RAG-FiT library consists of four distinct modules: data creation, training, inference, and evaluation. Each module is encapsulated and controlled by a configuration file, ensuring compatibility between the output of one module and the input of the next file. This modular approach allows isolation and independent experimentation on each step, enabling the production of multiple outputs and the concurrent execution of numerous experiments. Evaluation can be conducted on the generated outputs as well as on any feature within the data, including retrieval, ranking, and reasoning.Figure 1. In the RAG-FiT framework, the data augmentation module saves RAG interactions into a dedicated dataset, which is then used for training, inference, and evaluation.Dataset creation: The processing module facilitates the creation of context-enhanced datasets by persisting RAG interactions, which are essential for RAG-oriented training and inference. These interactions encompass dataset loading, column normalization, data aggregation, information retrieval, template-based prompt creation, and various other forms of pre-processing. The processed data can be saved in a consistent, model-independent format, along with all associated metadata, ensuring compatibility and reproducibility across different models and experiments.The processing module supports the handling of multiple datasets at once through global dataset sharing. This feature allows each step of the pipeline to access any of the loaded datasets, enhancing flexibility and allowing for complex processing procedures. Furthermore, the module includes step caching, which caches each pipeline step locally. This improves compute efficiency, and facilitates easy reproduction of results.Training: Users can train any model on the augmented datasets. A training module is used to fine-tune models from the datasets created by the previous processing module. The training module relies on the well-established training framework, TRL, for transformer reinforcement learning. The module also supports advanced efficient training techniques, such as PEFT and low-rank adaptation (LoRA) to customize the LLM for specific use cases without retraining the entire model.Inference: The inference module can generate predictions using the augmented datasets with trained or untrained LLMs. Inference is conceptually separated from the evaluation step, since it is more computationally demanding than evaluation. Additionally, users can run multiple evaluations on a single prepared inference results file.Evaluation: Custom metrics can be easily implemented or users can run current metrics, including Exact Match (EM), F1 Score, ROUGE, BERTScore, DeepEval, Ragas, Hugging Face Evaluate, and classification. Users can run metrics locally on each example, or globally on the entire dataset, such as recall for classification-based metrics. In addition to input and output texts, metrics can utilize any feature in the dataset, such as retrieval results, reasoning, citations, and attributions. In addition, the evaluation module uses a processing step called an Answer Processor, which can implement custom logic and perform many tasks, including cleaning and aligning outputs.Performance of RAG-FiT Augmentation TechniquesTo illustrate the utility of the framework, Intel Labs researchers conducted experiments involving retrieval, fine-tuning, chain-of-thought (CoT) reasoning, and a negative distractor documents technique. The team compared Llama 3.0 and Phi-3, two widely accepted baseline models, using enhancement methods across TriviaQA, PubMedQA, and ASQA, three knowledge-intensive question-answering datasets. The TriviaQA and PubMedQA datasets contain relevant context, while for the ASQA dataset, retrieval was done over a Wikipedia corpus using a dense retriever.The team measured and reported EM for TriviaQA, STR-EM for ASQA, and accuracy and F1 Score for PubMedQA. In addition, researchers evaluated two Ragas metrics: faithfulness (the relation between the generated text and the context) and relevancy (the generated text and the query). Overall, the two models showed consistent improvements across the three knowledge-intensive question-answering tasks.Figure 2. Evaluation results of baseline and different RAG settings for the three datasets and two models tested. In bold are the best configurations per dataset, based on the main metrics.For TriviaQA, retrieved context improved the results, fine-tuning the RAG setting boosted the results, but fine-tuning on CoT reasoning (which includes training on a combination of gold passages and distractor passages) decreased performance. For this dataset, the best method is model dependent. For ASQA, every method improved upon the baseline, CoT reasoning produced consistent improvement in both models, as well as fine-tuning of the CoT configuration, which performed best. Finally, for PubMedQA, almost all methods improved upon the baseline (with one exception), CoT reasoning improved on the untrained RAG setting, but for fine-tuning, the RAG method performed best in both models.Finally, the faithfulness and relevancy scores often did not correlate with the main metrics, or with each other, possibly indicating they capture different aspects of the retrieval and generated results, and represent a trade-off in performance.The results demonstrate the usefulness of RAG techniques for improving performance, as well as the need to carefully evaluate different aspects of a RAG system on a diverse set of datasets.
					You must be a registered user to add a comment. If you've already registered, sign in. Otherwise, register and sign in.
				Community support is provided Monday to Friday. Other contact methods are available here.Intel does not verify all solutions, including but not limited to any file transfers that may appear in this community. Accordingly, Intel disclaims all express and implied warranties, including without limitation, the implied warranties of merchantability, fitness for a particular purpose, and non-infringement, as well as any warranty arising from course of performance, course of dealing, or usage in trade.For more complete information about compiler optimizations, see our Optimization Notice.
</code></pre></div></div>]]></content><author><name></name></author><summary type="html"><![CDATA[Scott Bair is a key voice at Intel Labs , sharing insights into innovative research for inventing tomorrow’s technology. Intel Labs researchers]]></summary></entry><entry><title type="html">Open Domain Q&amp;amp;A using Dense Retrievers in fastRAG | by Daniel Fleischer | Medium</title><link href="https://danielfleischer.github.io/blog/2023/open-domain-qa-using-dense-retrievers-in-fastrag-by-daniel-fleischer-medium/" rel="alternate" type="text/html" title="Open Domain Q&amp;amp;A using Dense Retrievers in fastRAG | by Daniel Fleischer | Medium"/><published>2023-08-23T00:00:00+00:00</published><updated>2023-08-23T00:00:00+00:00</updated><id>https://danielfleischer.github.io/blog/2023/open-domain-qa-using-dense-retrievers-in-fastrag--by-daniel-fleischer--medium</id><content type="html" xml:base="https://danielfleischer.github.io/blog/2023/open-domain-qa-using-dense-retrievers-in-fastrag-by-daniel-fleischer-medium/"><![CDATA[<p>Sign upSign inSign upSign in–1ListenShareRetrieval augmented generation is an advanced technique in the field of natural language processing that combines the power of information retrieval and generative models. It aims to generate more informative and contextually appropriate responses to user queries by leveraging the retrieval of relevant passages or documents. This technique has significant potential in various applications, among them is Open Domain Question Answering; it is a field of research that focuses on developing systems capable of comprehending and answering a wide range of questions posed by users. It involves extracting relevant information from vast amounts of unstructured data using information retrieval techniques.Information retrieval used to rely on sparse techniques based on word statistics. In the traditional approach, documents were represented by a bag-of-words model, where the presence or absence of specific words determined the relevance of a document to a query. Score functions, like BM25 and TF-IDF, use word frequencies to score documents, balancing how frequently a keyword appears in a document versus how prevalent the word is in general. One popular DB is the Elasticsearch DB which uses the Lucene text search engine; it is based on word statistics and edit distance (a syntax method).With advancements in natural language processing and machine learning, information retrieval has shifted towards denser representations using embeddings. Embeddings capture the semantic meaning of words and phrases, allowing for a more nuanced understanding of the content; this enables more accurate matching of queries with relevant documents, as embeddings can capture subtle semantic similarities that traditional word statistics fail to capture. The shift from sparse to dense representations has significantly improved the performance and precision of retrieval systems. For an introduction to neural IR, see (Mitra and Craswell 2018); for a review of IR for Q&amp;A, see (Abbasiantaeb and Momtazi 2021).This blog serves as an introduction to dense retrieval and we focus on two dense document retrieval models: Dense Passage Retrieval (DPR, Karpukhin et al. 2020) and Contextualized Late Interaction over BERT (ColBERT, Khattab and Zaharia 2020). Both models use Semantic Search to find the relevant documents. Semantic search means we use a text’s dense representation to measure similarity between a given query and the potential relevant documents. The two models use different methods of storing the documents’ vectors and measuring similarity between queries and documents. We will compare the models by measuring accuracy and latency on a known benchmark, called Natural Questions (NQ, Kwiatkowski et al. 2019), a collection of user submitted questions where answers can be found in Wikipedia articles.We would like to introduce fastRAG, a framework developed at Intel Labs and released as an open-source software recently. The goal of the framework is to enable rapid research and development of retrieval-augmented generative AI applications. These can be used for generative tasks such as question answering, summarization, dialogue systems, and content creation, while utilizing information-retrieval components to anchor LLM output using external knowledge.An application is represented by a pipeline, typically comprised of a knowledge-base (KB), retriever, ranker and a reader, typically an LLM, which “reads” the query and retrieved documents, and generates an output. One can experiment with different architectures, models, benchmarking the results for performance and latency. Several of the models we offer are better suited for Intel hardware, achieving lower latency with comparable accuracy; on that in the next blog post.In the field of information retrieval, relatively recent updates promote the use of transformer encoder models as retrievals: documents in the knowledge-base are encoded as vectors and stored in an index. At runtime, the query is encoded as a vector and vector similarity search is used to find the most relevant documents. Similar process is used in re-ranking retrieved documents, where the encoding is done on-the-fly, specifically for the retrieved documents.Among the dense retrievals there are several approaches. One approach is to use a single token’s embeddings as a representative of the entire document. DPR (Karpukhin et al. 2020) is an example of that approach, where the encoders are trained to “summarize” the entire document in the first token’s embeddings. The method is a form of a bi-encoder, since it uses two encoders, one for the query and another for the documents; see illustration.Another approach is called Late Interaction, as defined first in ColBERT (Khattab and Zaharia 2020). The idea is to save (and index) the encoded vectors for all the words in the documents. At run-time the query vectors are compared with all the documents words’ vectors (hence the “late” in late interaction) thus retrieving more relevant documents than DPR. Notice that indexing every token, instead of just the first token for each document, can increase the index size.Later refinements to this work, namely ColBERT v2 and PLAID (Santhanam, Khattab, Saad-Falcon, et al. 2022; Santhanam, Khattab, Potts, et al. 2022) helped reduce the index size and latency time thanks to two main improvements: first is quantization and compression of the vectors in the index. Secondly is a set of heuristics that cluster the vectors using the K-means algorithm, hierarchically choose the relevant documents’ tokens for the query tokens based on the clusters’ centroids. ColBERT v2 with PLAID index achieves state of the art retrieval performance with a low latency, close to the order of sparse retrieval (BM25, Lucene, Elasticsearch, etc.) but with much higher accuracy.The first step is creating a documents store of the type PLAIDDocumentStore. The store requires three paths: checkpoint, collection and an index.A ColBERT checkpoint is an encoder model, fine tuned for the task of retrieving. It’s based on a BERT architecture. One can download a trained checkpoint, for example here, trained by the paper authors. Encoders can be fine-tuned using these instructions: training. Next is the collection of documents which comprise the corpus. The collection should be a signle tsv file with columns: id, text, title (optional). Finally, the index is the vectors index created using the same checkpoint, encoding all tokens in the corpus, compressing and saving the result.We provide a script to create a PLAID vector index using a ColBERT encoder and a documents collection in here.Once we have all the ingredients, we initialize the document store:Next we define a retriever using the document store we just define:We define a pipeline; it has the following form:We can use the Haystack pipeline API to connect with external components. In this example the pipeline contains just the retriever:Running the queries through the pipeline is very easy:The results is a hash map with documents key containing a list of results: documents with relevancy scores.To test ColBERT, we will use the Natural Questions benchmark (Kwiatkowski et al. 2019). The external knowledge is a collection of Wikipedia passages.As a baseline, we’ll use the original implementation of DPR, together with a checkpoint that was fine-tuned on Natural Questions, see download instructions. DPR model is released under the CC-BY-NC 4.0 license.DPR uses the Faiss vector search library (Johnson, Douze, and Jégou 2019). We test two configurations for storing the vectors: flat and HNSW. Flat is slow but accurate, since an exhaustive similarity search is done. HNSW (Malkov and Yashunin 2018) is an approximate vector search method; the vectors are organized into a graph to enable faster than linear search. Building an optimal HNSW graph requires some parameter tuning; these control the trade-off between speed, accuracy and index size.For ColBERT, we use the ColBERTv2 checkpoint from here, which was fine-tuned on the MS MARCO (Bajaj et al. 2018) dataset, which comprised of Bing questions and answers based on web search results.We report recall and MRR values for k values of 5, 10, 20, 50, and 100. We also measure latency (at k=100) (ms/query), and report the vector index size in GBs, as there is a trade-off between performance and accuracy.Measurements done on an Intel AWS instance, with a Xeon processor. AWS instance type is r6i.16xlarge; 32 cores, 512GB RAM, Intel(R) Xeon(R) Platinum 8375C CPU @ 2.90GHz. Conda 23.1.0, python 3.9.16, AMI image ami-0f1a5f5ada0e7da53, Amazon Linux 2, version 5.10.177-158.645.amzn2.x86_64. The Wikipedia text collection size is 13GB.First, we note there is a difference in accuracy between the ColBERT model and DPR. The quality of the embeddings generated from a trained encoder is crucial for high quality retrieval. As the DPR encoders were fine-tuned on the Natural Questions dataset, this is probably one of the reasons explaining the difference.Next, we compare the two indexing methods for DPR: flat and HNSW. Flat index query takes 35x longer than HNSW, at almost 1.5 seconds per query. HNSW is faster, with only a small accuracy penalty; however, index size is bigger, at ~2.3x the size of the flat index.It is notable that although ColBERT encodes and stores all the documents tokens, thanks to its optimizations, the index size is comparable to a flat Faiss index, storing only the first token’s embedding for each document.One of the goals was to present the clear trade off between accuracy and performance, more specifically, between recall, latency and memory usage (the index size, as these are stored in-memory). To summarize, we introduced two dense retrieval algorithms, ColBERT with a PLAID index and DPR. We tested these on the open-domain Q&amp;A dataset Natural Questions, measuring accuracy and latency.Experience the capabilities of ColBERT in fastRAG through the following Notebook example. Familiarize yourself with fastRAG by exploring our user-friendly UI demos at Running Demos in fastRAG. Start using the ColBERT encoder, accessible from the HuggingFace hub. Easily create a document index, as detailed in our guide at Indexing in fastRAG. Furthermore, we offer full support for the DPR retriever; see example DPR configuration. Unleash the potential of fastRAG and revolutionize your workflow today!Tests done by Intel on March 14th, 2023.Performance varies by use, configuration and other factors. Learn more at www.Intel.com/PerformanceIndex.© Intel Corporation. Intel, the Intel logo, and other Intel marks are trademarks of Intel Corporation or its subsidiaries. Other names and brands may be claimed as the property of others.Abbasiantaeb, Zahra, and Saeedeh Momtazi. 2021. “Text-Based Question Answering from Information Retrieval and Deep Neural Network Perspectives: A Survey.” Wires Data Mining and Knowledge Discovery 11 (6): e1412. https://doi.org/10.1002/widm.1412.Bajaj, Payal, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, et al. 2018. “MS MARCO: A Human Generated MAchine Reading COmprehension Dataset.” October 31, 2018. https://doi.org/10.48550/arXiv.1611.09268.Johnson, Jeff, Matthijs Douze, and Hervé Jégou. 2019. “Billion-Scale Similarity Search with GPUs.” Ieee Transactions on Big Data 7 (3): 535–47. https://doi.org/10.1109/TBDATA.2019.2921572.Karpukhin, Vladimir, Barlas Oğuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. “Dense Passage Retrieval for Open-Domain Question Answering.” September 30, 2020. https://doi.org/10.48550/arXiv.2004.04906.Khattab, Omar, and Matei Zaharia. 2020. “ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT.” June 4, 2020. https://doi.org/10.48550/arXiv.2004.12832.Kwiatkowski, Tom, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, et al. 2019. “Natural Questions: A Benchmark for Question Answering Research.” Transactions of the Association for Computational Linguistics 7 (August): 453–66. https://doi.org/10.1162/tacl_a_00276.Malkov, Yu A., and D. A. Yashunin. 2018. “Efficient and Robust Approximate Nearest Neighbor Search Using Hierarchical Navigable Small World Graphs.” August 14, 2018. https://doi.org/10.48550/arXiv.1603.09320.Mitra, Bhaskar, and Nick Craswell. 2018. “An Introduction to Neural Information Retrieval.” Foundations and Trends® in Information Retrieval 13 (1): 1–126. https://doi.org/10.1561/1500000061.Santhanam, Keshav, Omar Khattab, Christopher Potts, and Matei Zaharia. 2022. “PLAID: An Efficient Engine for Late Interaction Retrieval.” May 19, 2022. https://doi.org/10.48550/arXiv.2205.09707.Santhanam, Keshav, Omar Khattab, Jon Saad-Falcon, Christopher Potts, and Matei Zaharia. 2022. “ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction.” July 10, 2022. https://doi.org/10.48550/arXiv.2112.01488.—-1Research scientist at Intel LabsHelpStatusAboutCareersPressBlogPrivacyRulesTermsText to speech</p>]]></content><author><name></name></author><summary type="html"><![CDATA[We introduce 2 dense retrieval algorithms, ColBERT with a PLAID index and DPR. We tested these on the open-domain Q&A dataset Natural Questions, measuring accuracy and latency.]]></summary></entry></feed>