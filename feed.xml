<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://danielfleischer.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://danielfleischer.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-08-30T07:33:52+00:00</updated><id>https://danielfleischer.github.io/feed.xml</id><title type="html">blank</title><subtitle>Personal website for Daniel Fleischer. </subtitle><entry><title type="html">Jujutsu Impressions</title><link href="https://danielfleischer.github.io/blog/2025/jj/" rel="alternate" type="text/html" title="Jujutsu Impressions"/><published>2025-08-29T00:00:00+00:00</published><updated>2025-08-29T00:00:00+00:00</updated><id>https://danielfleischer.github.io/blog/2025/jj</id><content type="html" xml:base="https://danielfleischer.github.io/blog/2025/jj/"><![CDATA[<h4 id="tldr">TL;DR</h4> <ul> <li>JJ does things differently from Git, but it‚Äôs compatible enough to try. The core unit is the persistent <strong>change</strong>, which evolves through snapshots.</li> <li>Actions are logged as operations, enabling undo and restore to any point in time.</li> <li>No Git-style branches. Changes have persistent IDs, and you use bookmarks for Git-forge workflows.</li> <li>You can colocate <code class="language-plaintext highlighter-rouge">jj</code> with an existing Git repo, keeping your Git workflow intact while <code class="language-plaintext highlighter-rouge">jj</code> handles history.</li> <li>Curious? Follow the tutorial or experiment on a current repo to see the difference firsthand.</li> </ul> <hr/> <p>In this post I‚Äôll share some insights about using the <a href="https://jj-vcs.github.io/jj/latest/">Jujutsu version control system</a>, which I‚Äôll call <code class="language-plaintext highlighter-rouge">jj</code> for the rest of the post.</p> <p><code class="language-plaintext highlighter-rouge">jj</code> is a version control system, currently built on top of git, using its building blocks. However, it‚Äôs not just a new porcelain; it defines new abstractions and data structures of its own.</p> <h4 id="concept-the-change-as-the-atomic-unit">Concept: The Change as the Atomic Unit</h4> <p>The most important concept is the <strong>change</strong>. The <strong>change</strong> defines an atomic unit of, well, change. Its analog is the git commit. But in <code class="language-plaintext highlighter-rouge">jj</code>, a change can develop in time. Technically, it‚Äôs a git commit that keeps getting amended, the difference being that the <strong>change</strong> keeps its identity via a persistent ID and a description field. A <strong>change</strong> updates whenever <code class="language-plaintext highlighter-rouge">jj</code> takes a snapshot of the repo‚Äîevery time you call <code class="language-plaintext highlighter-rouge">jj</code>. If you think about it, <code class="language-plaintext highlighter-rouge">jj</code> can‚Äôt lose work, as it always starts by taking a snapshot, even for informative commands like <code class="language-plaintext highlighter-rouge">jj log</code>.</p> <p>Thus the <strong>change</strong> represents an amended commit, the ‚Äúlast‚Äù change. However, <code class="language-plaintext highlighter-rouge">jj</code> lets you inspect the internal, previous commits inside the <strong>change</strong>, by calling <code class="language-plaintext highlighter-rouge">jj evolog</code>. These internal commits are not synced to git forges and are not automatically garbage collected, like git amended commits are.</p> <p>When we are done with a <strong>change</strong>, we can give it a description (which we can do at any time using <code class="language-plaintext highlighter-rouge">jj description</code>) and create an empty <strong>change</strong> on top of it, ready to receive new modifications. We do that using <code class="language-plaintext highlighter-rouge">jj commit</code>.</p> <p>For example, when you initialize a repo, the initial current <strong>change</strong> is empty. You add files, edit them. When you are ready, you give this change a name via <code class="language-plaintext highlighter-rouge">jj description</code> and then start a new change via <code class="language-plaintext highlighter-rouge">jj new</code>, or do both at the same time via <code class="language-plaintext highlighter-rouge">jj commit</code>.</p> <h4 id="graph-branches-and-bookmarks">Graph, Branches, and Bookmarks</h4> <p>Moving around (<code class="language-plaintext highlighter-rouge">git checkout</code>) is done with <code class="language-plaintext highlighter-rouge">jj edit</code>. But we need to be careful, as any edit we make after the jump will get snapshots (‚Äúamended‚Äù) on top of the current <strong>change</strong>, modifying it. If you want to jump in to do some work, it‚Äôs better to use <code class="language-plaintext highlighter-rouge">jj new</code>.</p> <p>You might see the claim there are no branches in <code class="language-plaintext highlighter-rouge">jj</code>. There aren‚Äôt branches in the usual git sense. In <code class="language-plaintext highlighter-rouge">jj</code>s graph there are ‚Äúbranches‚Äù, but they don‚Äôt need names. The persistent <strong>change</strong> IDs serve as feature names and jump addresses (using <code class="language-plaintext highlighter-rouge">edit</code> or <code class="language-plaintext highlighter-rouge">new</code>). Nevertheless, branch names are introduced in <code class="language-plaintext highlighter-rouge">jj</code> in order to be compatible with git forges; they are called <strong>bookmarks</strong>, and they need to be moved explicitly across the <strong>changes</strong>.</p> <h4 id="logging-and-time-travel">Logging and Time Travel</h4> <p><code class="language-plaintext highlighter-rouge">jj</code> logs what‚Äôs happening using <strong>operations</strong>: these are the commands you enter and the current <em>change</em> you‚Äôre in, in addition to some metadata. The logs enable handy features like <code class="language-plaintext highlighter-rouge">jj undo</code> or the deeper <code class="language-plaintext highlighter-rouge">jj op restore</code> (restore to any point in time). For browsing the operation log, see <code class="language-plaintext highlighter-rouge">jj op log</code>.</p> <p>There is a <a href="https://jj-vcs.github.io/jj/latest/git-command-table/">comparison</a> table between <code class="language-plaintext highlighter-rouge">git</code> and <code class="language-plaintext highlighter-rouge">jj</code> commands, which could be useful but it‚Äôs important to not fixate on how <code class="language-plaintext highlighter-rouge">git</code> is doing things in order to be open to the new paradigm <code class="language-plaintext highlighter-rouge">jj</code> represents. However, some features seem to have been added in response to git users‚Äô needs or workflows, or perhaps <code class="language-plaintext highlighter-rouge">jj</code> developers rediscovered the same needs.</p> <h4 id="mixing-jj-with-git">Mixing JJ with Git</h4> <p>I chose one existing git project and converted it to a mixed usage of <code class="language-plaintext highlighter-rouge">jj</code> and <code class="language-plaintext highlighter-rouge">git</code>, using <code class="language-plaintext highlighter-rouge">jj git init --colocate</code>. It means <code class="language-plaintext highlighter-rouge">jj</code> initialize its presence in an existing repo and it will keep updating the <code class="language-plaintext highlighter-rouge">.git</code> folder with what‚Äôs happening, at a level compatible with git constructs; for example, the <strong>changes</strong> are saved as git commits, <code class="language-plaintext highlighter-rouge">jj git fetch</code> is fetching from git forges into <code class="language-plaintext highlighter-rouge">.git/</code>, etc. I haven‚Äôt used rebasing, squashing or other history-changing operations so I can‚Äôt comment on how easy they are to use, maybe next time.</p> <p>If you found it interesting, give it a try. There‚Äôs the <a href="https://jj-vcs.github.io/jj/latest/tutorial/">tutorial</a>, or you can run it on an existing git repo.</p>]]></content><author><name></name></author><category term="software"/><category term="git"/><summary type="html"><![CDATA[My first impression of using the Jujutsu version control system.]]></summary></entry><entry><title type="html">Built an MCP server for LLMs to search email from terminal | Daniel Fleischer posted on the topic | LinkedIn</title><link href="https://danielfleischer.github.io/blog/2025/built-an-mcp-server-for-llms-to-search-email-from-terminal-daniel-fleischer-posted-on-the-topic-linkedin/" rel="alternate" type="text/html" title="Built an MCP server for LLMs to search email from terminal | Daniel Fleischer posted on the topic | LinkedIn"/><published>2025-06-23T00:00:00+00:00</published><updated>2025-06-23T00:00:00+00:00</updated><id>https://danielfleischer.github.io/blog/2025/built-an-mcp-server-for-llms-to-search-email-from-terminal--daniel-fleischer-posted-on-the-topic--linkedin</id><content type="html" xml:base="https://danielfleischer.github.io/blog/2025/built-an-mcp-server-for-llms-to-search-email-from-terminal-daniel-fleischer-posted-on-the-topic-linkedin/"><![CDATA[<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>          Agree &amp; Join LinkedIn
        
  By clicking Continue to join or sign in, you agree to LinkedIn‚Äôs User Agreement, Privacy Policy, and Cookie Policy.

            AI Research Engineer @ Intel Labs
        üì¨ I built an MCP server that lets LLMs search my email from the terminal
</code></pre></div></div> <p>The server connects Claude to email search via the mu CLI tool. Now I just ask it things like: ‚ÄúFind emails with PDF attachments from last April‚Äù ‚ö°</p> <p>üõ† No custom frontend. No heavy framework. Just a CLI tool made smarter.</p> <p>üí° I learned that MCP servers are basically API translators ‚Äî they take complex developer SDKs and flatten them into simple function calls that LLMs can actually use.</p> <p>üéØ The bigger picture: This pattern can breathe new life into existing CLI tools and services. Complex APIs ‚Üí Simple, declarative functions ‚Üí Natural language queries.</p> <p>This isn‚Äôt a product ‚Äî just an experiment in stitching new capabilities into existing workflows. Code here: https://lnkd.in/eT2fJBSv</p> <p>mu email indexer and searcher: https://github.com/djcb/mu</p> <p>#MCP #LLM #EmailSearch #OpenSource #AI</p> <p>What existing tools would you want to make LLM-friendly? ü§î To view or add a comment, sign in</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>        1,429 followers
      
                Create your free account or sign in to continue your search
              
          or
        
  By clicking Continue to join or sign in, you agree to LinkedIn‚Äôs User Agreement, Privacy Policy, and Cookie Policy.

            New to LinkedIn? Join now
          
                      or
                    
                New to LinkedIn? Join now
              
  By clicking Continue to join or sign in, you agree to LinkedIn‚Äôs User Agreement, Privacy Policy, and Cookie Policy.
</code></pre></div></div>]]></content><author><name></name></author><summary type="html"><![CDATA[üì¨ I built an MCP server that lets LLMs search my email from the terminal The server connects Claude to email search via the mu CLI tool. Now I just ask it things like: "Find emails with PDF attachments from last April" ‚ö° üõ† No custom frontend. No heavy framework. Just a CLI tool made smarter. üí° I learned that MCP servers are basically API translators ‚Äî they take complex developer SDKs and flatten them into simple function calls that LLMs can actually use. üéØ The bigger picture: This pattern can breathe new life into existing CLI tools and services. Complex APIs ‚Üí Simple, declarative functions ‚Üí Natural language queries. This isn‚Äôt a product ‚Äî just an experiment in stitching new capabilities into existing workflows. Code here: https://lnkd.in/eT2fJBSv mu email indexer and searcher: https://github.com/djcb/mu #MCP #LLM #EmailSearch #OpenSource #AI What existing tools would you want to make LLM-friendly? ü§î]]></summary></entry><entry><title type="html">Summarize Hacker News Posts with Haystack &amp;amp; OPEA | Haystack</title><link href="https://danielfleischer.github.io/blog/2025/summarize-hacker-news-posts-with-haystack-opea-haystack/" rel="alternate" type="text/html" title="Summarize Hacker News Posts with Haystack &amp;amp; OPEA | Haystack"/><published>2025-06-10T00:00:00+00:00</published><updated>2025-06-10T00:00:00+00:00</updated><id>https://danielfleischer.github.io/blog/2025/summarize-hacker-news-posts-with-haystack--opea--haystack</id><content type="html" xml:base="https://danielfleischer.github.io/blog/2025/summarize-hacker-news-posts-with-haystack-opea-haystack/"><![CDATA[<p>Build a RAG pipeline to fetch live Hacker News posts and summarize them with a local LLM endpointWelcome to this step-by-step tutorial where we‚Äôll build a simple Retrieval-Augmented Generation (RAG) pipeline using Haystack and OPEA. We‚Äôll fetch the newest Hacker News posts, feed them to a lightweight LLM endpoint (OPEAGenerator), and generate concise one-sentence summaries (based on this notebook). Let‚Äôs dive in! üéâIn modern GenAI applications, having a flexible, performant, and scalable platform is essential. OPEA (Open Platform for Enterprise AI) is an open, model-agnostic framework for building and operating composable GenAI solutions. It provides:In this demo, we‚Äôll use an OPEA LLM endpoint in a Haystack pipeline, giving you:In this tutorial, we‚Äôll build a simple RAG pipeline that fetches the newest Hacker News posts, sends them to a local OPEA endpoint running a Qwen/Qwen2.5-7B-Instruct demo model, and produces concise one-sentence summaries. Of course, you can replace our example model with any other OPEA-served model, making this pattern both lightweight for prototyping and powerful for real-world deployments. Let‚Äôs get started! üöÄMake sure you have:NOTE: As a reference, here is a Docker Compose recipe to get you started. OPEA LLM service can be configured to use a variety of model serving backends like TGI, vLLM, ollama, OVMS‚Ä¶ and offers validated runtime settings for good performance on various hardware‚Äôs including Intel Gaudi. In this example, it creates an OPEA LLM service with a TGI backend. See the documentation for LLM Generation. The code is based on OPEA LLM example and OPEA TGI example.To run, call LLM_MODEL_ID=Qwen/Qwen2.5-7B-Instruct docker compose up.We‚Äôll create a custom Haystack component, HackernewsNewestFetcher, that:We use the OPEAGenerator to call our LLM over HTTP. Here, we point to a local endpoint serving the Qwen/Qwen2.5-7B-Instruct model:Using PromptBuilder, we define a Jinja-style template that:We wire up the components in a Pipeline:Fetch and summarize the top 2 newest Hacker News posts:Beautiful, concise summaries in seconds! ‚ú®In this tutorial, we built a full RAG pipeline:Feel free to extend this setup with more advanced retrieval, caching, or different LLM backends. Happy coding! üõ†Ô∏èüî• Building products, technology and solutions for LLM-enabled applications.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Build a RAG pipeline to fetch live Hacker News posts and summarize them with a local LLM endpoint]]></summary></entry><entry><title type="html">Attention Required! | Cloudflare</title><link href="https://danielfleischer.github.io/blog/2024/attention-required-cloudflare/" rel="alternate" type="text/html" title="Attention Required! | Cloudflare"/><published>2024-12-01T00:00:00+00:00</published><updated>2024-12-01T00:00:00+00:00</updated><id>https://danielfleischer.github.io/blog/2024/attention-required--cloudflare</id><content type="html" xml:base="https://danielfleischer.github.io/blog/2024/attention-required-cloudflare/"><![CDATA[<p>This website is using a security service to protect itself from online attacks. The action you just performed triggered the security solution. There are several actions that could trigger this block including submitting a certain word or phrase, a SQL command or malformed data.You can email the site owner to let them know you were blocked. Please include what you were doing when this page came up and the Cloudflare Ray ID found at the bottom of this page. Cloudflare Ray ID: 97729a47da4ff096 ‚Ä¢</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    Your IP:
    Click to reveal
    52.176.124.180
    ‚Ä¢
  
  Performance &amp; security by Cloudflare
</code></pre></div></div>]]></content><author><name></name></author></entry><entry><title type="html">Intel Labs Introduces RAG-FiT Open-Source Framework for Retrieval Augmented Generation in LLMs - Intel Community</title><link href="https://danielfleischer.github.io/blog/2024/intel-labs-introduces-rag-fit-open-source-framework-for-retrieval-augmented-generation-in-llms-intel-community/" rel="alternate" type="text/html" title="Intel Labs Introduces RAG-FiT Open-Source Framework for Retrieval Augmented Generation in LLMs - Intel Community"/><published>2024-10-09T00:00:00+00:00</published><updated>2024-10-09T00:00:00+00:00</updated><id>https://danielfleischer.github.io/blog/2024/intel-labs-introduces-rag-fit-open-source-framework-for-retrieval-augmented-generation-in-llms---intel-community</id><content type="html" xml:base="https://danielfleischer.github.io/blog/2024/intel-labs-introduces-rag-fit-open-source-framework-for-retrieval-augmented-generation-in-llms-intel-community/"><![CDATA[<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>                        Success!  Subscription added.
                    
                   
                
                    
                        Success!  Subscription removed.
                    
                    
                
                    
                        Sorry, you must verify to complete this action. Please click the verification link in your email. You may re-send via your
                        profile.
                    
                Scott Bair is a key voice at Intel Labs, sharing insights into innovative research for inventing tomorrow‚Äôs technology.¬†Intel Labs researchers Daniel Fleischer, Moshe Berchansky, and Moshe Wasserblat collaborated on RAG-FiT.HighlightsIntel Labs introduces RAG-FiT, an open-source framework for augmenting large language models (LLMs) for retrieval-augmented generation (RAG) use cases. Available under an Apache 2.0 license, RAG-FiT integrates data creation, training, inference, and evaluation into a single workflow, assisting in the creation of data-augmented datasets for training and evaluating LLMs in RAG settings. This integration enables rapid prototyping and experimentation with various RAG techniques, allowing users to easily generate datasets and train RAG models using internal or specialized knowledge sources.The library assists in creating data to train models using parameter-efficient fine-tuning (PEFT), which allows users to finetune a subset of parameters in a model. The Python-based framework is designed to serve as an end-to-end experimentation environment, enabling users to quickly prototype and experiment with different RAG techniques, including data selection, aggregation and filtering, retrieval, text processing, document ranking, few-shot generation, prompt design using templates, fine-tuning, inference, and evaluation.To demonstrate the effectiveness of the RAG-FiT framework (formerly known as RAG Foundry), Intel Labs researchers augmented and fine-tuned Llama 3.0 and Phi-3 models with diverse RAG configurations, showcasing consistent improvements across three knowledge-intensive question-answering tasks.Using RAG Systems to Address LLM LimitationsDespite their impressive capabilities, LLMs have inherent limitations. These models can produce plausible sounding but incorrect or nonsensical answers, struggle with factual accuracy, lack access to up-to-date information after their training cutoff, and struggle in attending to relevant information in large contexts.RAG enhances LLMs performance by integrating external information using retrieval mechanisms. Retrieving specific data from knowledge bases outside the model can effectively address knowledge limitations, which in turn can reduce hallucinations, improve the relevance of generated content, provide interpretability and could be vastly more cost efficient. Furthermore, recent research indicates that fine-tuning LLMs for RAG can achieve state-of-the-art performance, surpassing that of larger proprietary models.How RAG-FiT WorksAs an experimentation environment for researchers, the backbone of the RAG-FiT library consists of four distinct modules: data creation, training, inference, and evaluation. Each module is encapsulated and controlled by a configuration file, ensuring compatibility between the output of one module and the input of the next file. This modular approach allows isolation and independent experimentation on each step, enabling the production of multiple outputs and the concurrent execution of numerous experiments. Evaluation can be conducted on the generated outputs as well as on any feature within the data, including retrieval, ranking, and reasoning.Figure 1. In the RAG-FiT framework, the data augmentation module saves RAG interactions into a dedicated dataset, which is then used for training, inference, and evaluation.Dataset creation: The processing module facilitates the creation of context-enhanced datasets by persisting RAG interactions, which are essential for RAG-oriented training and inference. These interactions encompass dataset loading, column normalization, data aggregation, information retrieval, template-based prompt creation, and various other forms of pre-processing. The processed data can be saved in a consistent, model-independent format, along with all associated metadata, ensuring compatibility and reproducibility across different models and experiments.The processing module supports the handling of multiple datasets at once through global dataset sharing. This feature allows each step of the pipeline to access any of the loaded datasets, enhancing flexibility and allowing for complex processing procedures. Furthermore, the module includes step caching, which caches each pipeline step locally. This improves compute efficiency, and facilitates easy reproduction of results.Training: Users can train any model on the augmented datasets. A training module is used to fine-tune models from the datasets created by the previous processing module. The training module relies on the well-established training framework, TRL, for transformer reinforcement learning. The module also supports advanced efficient training techniques, such as PEFT and low-rank adaptation (LoRA) to customize the LLM for specific use cases without retraining the entire model.Inference: The inference module can generate predictions using the augmented datasets with trained or untrained LLMs. Inference is conceptually separated from the evaluation step, since it is more computationally demanding than evaluation. Additionally, users can run multiple evaluations on a single prepared inference results file.Evaluation: Custom metrics can be easily implemented or users can run current metrics, including Exact Match (EM), F1 Score, ROUGE, BERTScore, DeepEval, Ragas, Hugging Face Evaluate, and classification. Users can run metrics locally on each example, or globally on the entire dataset, such as recall for classification-based metrics. In addition to input and output texts, metrics can utilize any feature in the dataset, such as retrieval results, reasoning, citations, and attributions. In addition, the evaluation module uses a processing step called an Answer Processor, which can implement custom logic and perform many tasks, including cleaning and aligning outputs.Performance of RAG-FiT Augmentation TechniquesTo illustrate the utility of the framework, Intel Labs researchers conducted experiments involving retrieval, fine-tuning, chain-of-thought (CoT) reasoning, and a negative distractor documents technique. The team compared Llama 3.0 and Phi-3, two widely accepted baseline models, using enhancement methods across TriviaQA, PubMedQA, and ASQA, three knowledge-intensive question-answering datasets. The TriviaQA and PubMedQA datasets contain relevant context, while for the ASQA dataset, retrieval was done over a Wikipedia corpus using a dense retriever.The team measured and reported EM for TriviaQA, STR-EM for ASQA, and accuracy and F1 Score for PubMedQA. In addition, researchers evaluated two Ragas metrics: faithfulness (the relation between the generated text and the context) and relevancy (the generated text and the query). Overall, the two models showed consistent improvements across the three knowledge-intensive question-answering tasks.Figure 2. Evaluation results of baseline and different RAG settings for the three datasets and two models tested. In bold are the best configurations per dataset, based on the main metrics.For TriviaQA, retrieved context improved the results, fine-tuning the RAG setting boosted the results, but fine-tuning on CoT reasoning (which includes training on a combination of gold passages and distractor passages) decreased performance. For this dataset, the best method is model dependent. For ASQA, every method improved upon the baseline, CoT reasoning produced consistent improvement in both models, as well as fine-tuning of the CoT configuration, which performed best. Finally, for PubMedQA, almost all methods improved upon the baseline (with one exception), CoT reasoning improved on the untrained RAG setting, but for fine-tuning, the RAG method performed best in both models.Finally, the faithfulness and relevancy scores often did not correlate with the main metrics, or with each other, possibly indicating they capture different aspects of the retrieval and generated results, and represent a trade-off in performance.The results demonstrate the usefulness of RAG techniques for improving performance, as well as the need to carefully evaluate different aspects of a RAG system on a diverse set of datasets.
					You must be a registered user to add a comment. If you've already registered, sign in. Otherwise, register and sign in.
				Community support is provided Monday to Friday. Other contact methods are available here.Intel does not verify all solutions, including but not limited to any file transfers that may appear in this community. Accordingly, Intel disclaims all express and implied warranties, including without limitation, the implied warranties of merchantability, fitness for a particular purpose, and non-infringement, as well as any warranty arising from course of performance, course of dealing, or usage in trade.For more complete information about compiler optimizations, see our Optimization Notice.
</code></pre></div></div>]]></content><author><name></name></author><summary type="html"><![CDATA[Scott Bair is a key voice at Intel Labs , sharing insights into innovative research for inventing tomorrow‚Äôs technology. Intel Labs researchers]]></summary></entry><entry><title type="html">Open Domain Q&amp;amp;A using Dense Retrievers in fastRAG | by Daniel Fleischer | Medium</title><link href="https://danielfleischer.github.io/blog/2023/open-domain-qa-using-dense-retrievers-in-fastrag-by-daniel-fleischer-medium/" rel="alternate" type="text/html" title="Open Domain Q&amp;amp;A using Dense Retrievers in fastRAG | by Daniel Fleischer | Medium"/><published>2023-08-23T00:00:00+00:00</published><updated>2023-08-23T00:00:00+00:00</updated><id>https://danielfleischer.github.io/blog/2023/open-domain-qa-using-dense-retrievers-in-fastrag--by-daniel-fleischer--medium</id><content type="html" xml:base="https://danielfleischer.github.io/blog/2023/open-domain-qa-using-dense-retrievers-in-fastrag-by-daniel-fleischer-medium/"><![CDATA[<p>Sign upSign inSign upSign in‚Äì1ListenShareRetrieval augmented generation is an advanced technique in the field of natural language processing that combines the power of information retrieval and generative models. It aims to generate more informative and contextually appropriate responses to user queries by leveraging the retrieval of relevant passages or documents. This technique has significant potential in various applications, among them is Open Domain Question Answering; it is a field of research that focuses on developing systems capable of comprehending and answering a wide range of questions posed by users. It involves extracting relevant information from vast amounts of unstructured data using information retrieval techniques.Information retrieval used to rely on sparse techniques based on word statistics. In the traditional approach, documents were represented by a bag-of-words model, where the presence or absence of specific words determined the relevance of a document to a query. Score functions, like BM25 and TF-IDF, use word frequencies to score documents, balancing how frequently a keyword appears in a document versus how prevalent the word is in general. One popular DB is the Elasticsearch DB which uses the Lucene text search engine; it is based on word statistics and edit distance (a syntax method).With advancements in natural language processing and machine learning, information retrieval has shifted towards denser representations using embeddings. Embeddings capture the semantic meaning of words and phrases, allowing for a more nuanced understanding of the content; this enables more accurate matching of queries with relevant documents, as embeddings can capture subtle semantic similarities that traditional word statistics fail to capture. The shift from sparse to dense representations has significantly improved the performance and precision of retrieval systems. For an introduction to neural IR, see (Mitra and Craswell 2018); for a review of IR for Q&amp;A, see (Abbasiantaeb and Momtazi 2021).This blog serves as an introduction to dense retrieval and we focus on two dense document retrieval models: Dense Passage Retrieval (DPR, Karpukhin et al. 2020) and Contextualized Late Interaction over BERT (ColBERT, Khattab and Zaharia 2020). Both models use Semantic Search to find the relevant documents. Semantic search means we use a text‚Äôs dense representation to measure similarity between a given query and the potential relevant documents. The two models use different methods of storing the documents‚Äô vectors and measuring similarity between queries and documents. We will compare the models by measuring accuracy and latency on a known benchmark, called Natural Questions (NQ, Kwiatkowski et al. 2019), a collection of user submitted questions where answers can be found in Wikipedia articles.We would like to introduce fastRAG, a framework developed at Intel Labs and released as an open-source software recently. The goal of the framework is to enable rapid research and development of retrieval-augmented generative AI applications. These can be used for generative tasks such as question answering, summarization, dialogue systems, and content creation, while utilizing information-retrieval components to anchor LLM output using external knowledge.An application is represented by a pipeline, typically comprised of a knowledge-base (KB), retriever, ranker and a reader, typically an LLM, which ‚Äúreads‚Äù the query and retrieved documents, and generates an output. One can experiment with different architectures, models, benchmarking the results for performance and latency. Several of the models we offer are better suited for Intel hardware, achieving lower latency with comparable accuracy; on that in the next blog post.In the field of information retrieval, relatively recent updates promote the use of transformer encoder models as retrievals: documents in the knowledge-base are encoded as vectors and stored in an index. At runtime, the query is encoded as a vector and vector similarity search is used to find the most relevant documents. Similar process is used in re-ranking retrieved documents, where the encoding is done on-the-fly, specifically for the retrieved documents.Among the dense retrievals there are several approaches. One approach is to use a single token‚Äôs embeddings as a representative of the entire document. DPR (Karpukhin et al. 2020) is an example of that approach, where the encoders are trained to ‚Äúsummarize‚Äù the entire document in the first token‚Äôs embeddings. The method is a form of a bi-encoder, since it uses two encoders, one for the query and another for the documents; see illustration.Another approach is called Late Interaction, as defined first in ColBERT (Khattab and Zaharia 2020). The idea is to save (and index) the encoded vectors for all the words in the documents. At run-time the query vectors are compared with all the documents words‚Äô vectors (hence the ‚Äúlate‚Äù in late interaction) thus retrieving more relevant documents than DPR. Notice that indexing every token, instead of just the first token for each document, can increase the index size.Later refinements to this work, namely ColBERT v2 and PLAID (Santhanam, Khattab, Saad-Falcon, et al. 2022; Santhanam, Khattab, Potts, et al. 2022) helped reduce the index size and latency time thanks to two main improvements: first is quantization and compression of the vectors in the index. Secondly is a set of heuristics that cluster the vectors using the K-means algorithm, hierarchically choose the relevant documents‚Äô tokens for the query tokens based on the clusters‚Äô centroids. ColBERT v2 with PLAID index achieves state of the art retrieval performance with a low latency, close to the order of sparse retrieval (BM25, Lucene, Elasticsearch, etc.) but with much higher accuracy.The first step is creating a documents store of the type PLAIDDocumentStore. The store requires three paths: checkpoint, collection and an index.A ColBERT checkpoint is an encoder model, fine tuned for the task of retrieving. It‚Äôs based on a BERT architecture. One can download a trained checkpoint, for example here, trained by the paper authors. Encoders can be fine-tuned using these instructions: training. Next is the collection of documents which comprise the corpus. The collection should be a signle tsv file with columns: id, text, title (optional). Finally, the index is the vectors index created using the same checkpoint, encoding all tokens in the corpus, compressing and saving the result.We provide a script to create a PLAID vector index using a ColBERT encoder and a documents collection in here.Once we have all the ingredients, we initialize the document store:Next we define a retriever using the document store we just define:We define a pipeline; it has the following form:We can use the Haystack pipeline API to connect with external components. In this example the pipeline contains just the retriever:Running the queries through the pipeline is very easy:The results is a hash map with documents key containing a list of results: documents with relevancy scores.To test ColBERT, we will use the Natural Questions benchmark (Kwiatkowski et al. 2019). The external knowledge is a collection of Wikipedia passages.As a baseline, we‚Äôll use the original implementation of DPR, together with a checkpoint that was fine-tuned on Natural Questions, see download instructions. DPR model is released under the CC-BY-NC 4.0 license.DPR uses the Faiss vector search library (Johnson, Douze, and J√©gou 2019). We test two configurations for storing the vectors: flat and HNSW. Flat is slow but accurate, since an exhaustive similarity search is done. HNSW (Malkov and Yashunin 2018) is an approximate vector search method; the vectors are organized into a graph to enable faster than linear search. Building an optimal HNSW graph requires some parameter tuning; these control the trade-off between speed, accuracy and index size.For ColBERT, we use the ColBERTv2 checkpoint from here, which was fine-tuned on the MS MARCO (Bajaj et al. 2018) dataset, which comprised of Bing questions and answers based on web search results.We report recall and MRR values for k values of 5, 10, 20, 50, and 100. We also measure latency (at k=100) (ms/query), and report the vector index size in GBs, as there is a trade-off between performance and accuracy.Measurements done on an Intel AWS instance, with a Xeon processor. AWS instance type is r6i.16xlarge; 32 cores, 512GB RAM, Intel(R) Xeon(R) Platinum 8375C CPU @ 2.90GHz. Conda 23.1.0, python 3.9.16, AMI image ami-0f1a5f5ada0e7da53, Amazon Linux 2, version 5.10.177-158.645.amzn2.x86_64. The Wikipedia text collection size is 13GB.First, we note there is a difference in accuracy between the ColBERT model and DPR. The quality of the embeddings generated from a trained encoder is crucial for high quality retrieval. As the DPR encoders were fine-tuned on the Natural Questions dataset, this is probably one of the reasons explaining the difference.Next, we compare the two indexing methods for DPR: flat and HNSW. Flat index query takes 35x longer than HNSW, at almost 1.5 seconds per query. HNSW is faster, with only a small accuracy penalty; however, index size is bigger, at ~2.3x the size of the flat index.It is notable that although ColBERT encodes and stores all the documents tokens, thanks to its optimizations, the index size is comparable to a flat Faiss index, storing only the first token‚Äôs embedding for each document.One of the goals was to present the clear trade off between accuracy and performance, more specifically, between recall, latency and memory usage (the index size, as these are stored in-memory). To summarize, we introduced two dense retrieval algorithms, ColBERT with a PLAID index and DPR. We tested these on the open-domain Q&amp;A dataset Natural Questions, measuring accuracy and latency.Experience the capabilities of ColBERT in fastRAG through the following Notebook example. Familiarize yourself with fastRAG by exploring our user-friendly UI demos at Running Demos in fastRAG. Start using the ColBERT encoder, accessible from the HuggingFace hub. Easily create a document index, as detailed in our guide at Indexing in fastRAG. Furthermore, we offer full support for the DPR retriever; see example DPR configuration. Unleash the potential of fastRAG and revolutionize your workflow today!Tests done by Intel on March 14th, 2023.Performance varies by use, configuration and other factors. Learn more at www.Intel.com/PerformanceIndex.¬© Intel Corporation. Intel, the Intel logo, and other Intel marks are trademarks of Intel Corporation or its subsidiaries. Other names and brands may be claimed as the property of others.Abbasiantaeb, Zahra, and Saeedeh Momtazi. 2021. ‚ÄúText-Based Question Answering from Information Retrieval and Deep Neural Network Perspectives: A Survey.‚Äù Wires Data Mining and Knowledge Discovery 11 (6): e1412. https://doi.org/10.1002/widm.1412.Bajaj, Payal, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, et al. 2018. ‚ÄúMS MARCO: A Human Generated MAchine Reading COmprehension Dataset.‚Äù October 31, 2018. https://doi.org/10.48550/arXiv.1611.09268.Johnson, Jeff, Matthijs Douze, and Herv√© J√©gou. 2019. ‚ÄúBillion-Scale Similarity Search with GPUs.‚Äù Ieee Transactions on Big Data 7 (3): 535‚Äì47. https://doi.org/10.1109/TBDATA.2019.2921572.Karpukhin, Vladimir, Barlas Oƒüuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. ‚ÄúDense Passage Retrieval for Open-Domain Question Answering.‚Äù September 30, 2020. https://doi.org/10.48550/arXiv.2004.04906.Khattab, Omar, and Matei Zaharia. 2020. ‚ÄúColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT.‚Äù June 4, 2020. https://doi.org/10.48550/arXiv.2004.12832.Kwiatkowski, Tom, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, et al. 2019. ‚ÄúNatural Questions: A Benchmark for Question Answering Research.‚Äù Transactions of the Association for Computational Linguistics 7 (August): 453‚Äì66. https://doi.org/10.1162/tacl_a_00276.Malkov, Yu A., and D. A. Yashunin. 2018. ‚ÄúEfficient and Robust Approximate Nearest Neighbor Search Using Hierarchical Navigable Small World Graphs.‚Äù August 14, 2018. https://doi.org/10.48550/arXiv.1603.09320.Mitra, Bhaskar, and Nick Craswell. 2018. ‚ÄúAn Introduction to Neural Information Retrieval.‚Äù Foundations and Trends¬Æ in Information Retrieval 13 (1): 1‚Äì126. https://doi.org/10.1561/1500000061.Santhanam, Keshav, Omar Khattab, Christopher Potts, and Matei Zaharia. 2022. ‚ÄúPLAID: An Efficient Engine for Late Interaction Retrieval.‚Äù May 19, 2022. https://doi.org/10.48550/arXiv.2205.09707.Santhanam, Keshav, Omar Khattab, Jon Saad-Falcon, Christopher Potts, and Matei Zaharia. 2022. ‚ÄúColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction.‚Äù July 10, 2022. https://doi.org/10.48550/arXiv.2112.01488.‚Äî-1Research scientist at Intel LabsHelpStatusAboutCareersPressBlogPrivacyRulesTermsText to speech</p>]]></content><author><name></name></author><summary type="html"><![CDATA[We introduce 2 dense retrieval algorithms, ColBERT with a PLAID index and DPR. We tested these on the open-domain Q&A dataset Natural Questions, measuring accuracy and latency.]]></summary></entry></feed>