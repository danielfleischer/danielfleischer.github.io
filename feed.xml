<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://danielfleischer.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://danielfleischer.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-09-07T17:40:12+00:00</updated><id>https://danielfleischer.github.io/feed.xml</id><title type="html">blank</title><subtitle>Personal website for Daniel Fleischer. </subtitle><entry><title type="html">Breaking Language Barriers in Mathematical AI: Introducing Hebrew Math Tutor</title><link href="https://danielfleischer.github.io/blog/2025/breaking-language-barriers-in-mathematical-ai-introducing-hebrew-math-tutor/" rel="alternate" type="text/html" title="Breaking Language Barriers in Mathematical AI: Introducing Hebrew Math Tutor"/><published>2025-09-07T00:00:00+00:00</published><updated>2025-09-07T00:00:00+00:00</updated><id>https://danielfleischer.github.io/blog/2025/breaking-language-barriers-in-mathematical-ai-introducing-hebrew-math-tutor</id><content type="html" xml:base="https://danielfleischer.github.io/blog/2025/breaking-language-barriers-in-mathematical-ai-introducing-hebrew-math-tutor/"><![CDATA[<p>Hebrew Math Tutor (Intel/hebrew-math-tutor-v1) brings advanced mathematical problem-solving capabilities directly to Hebrew speakers, providing detailed step-by-step reasoning entirely in Hebrew without sacrificing the computational accuracy that makes these models valuable for education.Advanced mathematical AI models like those trained on competition mathematics datasets have shown remarkable problem-solving abilities. However, they primarily operate in English, creating barriers for non-English speaking educational communities. Hebrew speakers, in particular, have faced challenges accessing these powerful educational tools in their native language.Simply translating outputs isnâ€™t enoughâ€”effective mathematical tutoring requires natural language flow, culturally appropriate explanations, and seamless integration of Hebrew text with mathematical notation. This requires a more sophisticated approach.Hebrew Math Tutor addresses these challenges through targeted fine-tuning of Qwen3-4B-Thinking-2507, a powerful 4-billion parameter mathematical reasoning model. Our approach focuses on three key principles:The model provides complete mathematical explanations in natural Hebrew while preserving mathematical notation and formal expressions. It understands Hebrew mathematical terminology and can explain complex concepts using appropriate pedagogical language.By carefully fine-tuning rather than training from scratch, we maintain the modelâ€™s core mathematical reasoning capabilities while adapting its communication style to Hebrew.At ~4 billion parameters, the model strikes an optimal balance between capability and computational efficiency, making it practical for educational applications and research prototyping.Creating an effective Hebrew math model required more than simple translation. Our methodology involved:We selected ~10,000 high-quality problems from the OpenMathReasoning dataset, translating questions and answers to Hebrew while preserving the original reasoning chains and mathematical notation.We fine-tuned the model over 3 epochs with optimized parameters (learning rate 5e-6, 0.1 warmup, cosine scheduling) to adapt the output language while maintaining the underlying reasoning capabilities.The modelâ€™s internal <think>...</think> reasoning blocks remain in English, as these represent core computational processes that would require more extensive training to modify.We evaluated Hebrew Math Tutor against its base model on three challenging mathematical benchmarks: MATH500 (curriculum problems), AIME24, and AIME25 (competition mathematics). The results demonstrate significant improvements in Hebrew language output while maintaining strong technical performance.ğŸš€ Dramatic Hebrew Language Gains: Hebrew answer production jumped from 35-75% to 95-100% across all benchmarksâ€”a transformative improvement for Hebrew-speaking users.ğŸ“ˆ Consistent Accuracy Improvements: Notable gains in pass@16 scores on Hebrew evaluations, showing the model doesnâ€™t just translate but actually improves problem-solving in Hebrew contexts.ğŸ”„ Preserved Core Capabilities: Maintained competitive English performance, demonstrating that Hebrew specialization didnâ€™t compromise the modelâ€™s fundamental mathematical abilities.âš–ï¸ Nuanced Majority Vote Results: While performance improved on MATH500 and remained stable on AIME24, thereâ€™s an interesting decrease in maj@16 on AIME25 that provides insights for future training approaches.Hebrew Math Tutor opens new possibilities across multiple domains:Hebrew Math Tutor integrates seamlessly with the Transformers ecosystem:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Hebrew Math Tutor in action: A Streamlit interface showing detailed step-by-step reasoning in Hebrew. The expandable reasoning sections allow users to dive deep into the mathematical process or focus on final answers. While Hebrew Math Tutor represents significant progress, responsible deployment requires careful consideration:The model works best as an educational aid rather than a replacement for qualified instruction. We recommend implementing human oversight, providing clear disclaimers about AI-generated content, and ensuring compliance with relevant privacy regulations in educational applications.Hebrew Math Tutor demonstrates that language barriers in AI can be effectively addressed through thoughtful fine-tuning approaches. This work represents more than just a Hebrew mathematical modelâ€”it's a proof of concept for making advanced AI capabilities truly accessible across linguistic communities.The techniques developed here can be adapted for other languages, creating a pathway toward more inclusive mathematical AI tools. As we continue to refine these approaches, we're moving closer to a future where language is no longer a barrier to accessing the most advanced educational technologies.Hebrew Math Tutor is available now under the Apache-2.0 license. We encourage the community to:ğŸš€ Start exploring Hebrew Math Tutor today and experience mathematical AI that truly speaks your language.Built with gratitude upon the foundational work of Qwen3-4B-Thinking-2507 and the OpenMathReasoning dataset.Â·
				Sign up or
				log in to comment
</code></pre></div></div>]]></content><author><name></name></author><summary type="html"><![CDATA[A Blog post by Daniel Fleischer on Hugging Face]]></summary></entry><entry><title type="html">Jujutsu Impressions</title><link href="https://danielfleischer.github.io/blog/2025/jj/" rel="alternate" type="text/html" title="Jujutsu Impressions"/><published>2025-08-29T00:00:00+00:00</published><updated>2025-08-29T00:00:00+00:00</updated><id>https://danielfleischer.github.io/blog/2025/jj</id><content type="html" xml:base="https://danielfleischer.github.io/blog/2025/jj/"><![CDATA[<h4 id="tldr">TL;DR</h4> <ul> <li>JJ does things differently from Git, but itâ€™s compatible enough to try. The core unit is the persistent <strong>change</strong>, which evolves through snapshots.</li> <li>Actions are logged as operations, enabling undo and restore to any point in time.</li> <li>No Git-style branches. Changes have persistent IDs, and you use bookmarks for Git-forge workflows.</li> <li>You can colocate <code class="language-plaintext highlighter-rouge">jj</code> with an existing Git repo, keeping your Git workflow intact while <code class="language-plaintext highlighter-rouge">jj</code> handles history.</li> <li>Curious? Follow the tutorial or experiment on a current repo to see the difference firsthand.</li> </ul> <hr/> <p>In this post Iâ€™ll share some insights about using the <a href="https://jj-vcs.github.io/jj/latest/">Jujutsu version control system</a>, which Iâ€™ll call <code class="language-plaintext highlighter-rouge">jj</code> for the rest of the post.</p> <p><code class="language-plaintext highlighter-rouge">jj</code> is a version control system, currently built on top of git, using its building blocks. However, itâ€™s not just a new porcelain; it defines new abstractions and data structures of its own.</p> <h4 id="concept-the-change-as-the-atomic-unit">Concept: The Change as the Atomic Unit</h4> <p>The most important concept is the <strong>change</strong>. The <strong>change</strong> defines an atomic unit of, well, change. Its analog is the git commit. But in <code class="language-plaintext highlighter-rouge">jj</code>, a change can develop in time. Technically, itâ€™s a git commit that keeps getting amended, the difference being that the <strong>change</strong> keeps its identity via a persistent ID and a description field. A <strong>change</strong> updates whenever <code class="language-plaintext highlighter-rouge">jj</code> takes a snapshot of the repoâ€”every time you call <code class="language-plaintext highlighter-rouge">jj</code>. If you think about it, <code class="language-plaintext highlighter-rouge">jj</code> canâ€™t lose work, as it always starts by taking a snapshot, even for informative commands like <code class="language-plaintext highlighter-rouge">jj log</code>.</p> <p>Thus the <strong>change</strong> represents an amended commit, the â€œlastâ€ change. However, <code class="language-plaintext highlighter-rouge">jj</code> lets you inspect the internal, previous commits inside the <strong>change</strong>, by calling <code class="language-plaintext highlighter-rouge">jj evolog</code>. These internal commits are not synced to git forges and are not automatically garbage collected, like git amended commits are.</p> <p>When we are done with a <strong>change</strong>, we can give it a description (which we can do at any time using <code class="language-plaintext highlighter-rouge">jj description</code>) and create an empty <strong>change</strong> on top of it, ready to receive new modifications. We do that using <code class="language-plaintext highlighter-rouge">jj commit</code>.</p> <p>For example, when you initialize a repo, the initial current <strong>change</strong> is empty. You add files, edit them. When you are ready, you give this change a name via <code class="language-plaintext highlighter-rouge">jj description</code> and then start a new change via <code class="language-plaintext highlighter-rouge">jj new</code>, or do both at the same time via <code class="language-plaintext highlighter-rouge">jj commit</code>.</p> <h4 id="graph-branches-and-bookmarks">Graph, Branches, and Bookmarks</h4> <p>Moving around (<code class="language-plaintext highlighter-rouge">git checkout</code>) is done with <code class="language-plaintext highlighter-rouge">jj edit</code>. But we need to be careful, as any edit we make after the jump will get snapshots (â€œamendedâ€) on top of the current <strong>change</strong>, modifying it. If you want to jump in to do some work, itâ€™s better to use <code class="language-plaintext highlighter-rouge">jj new</code>.</p> <p>You might see the claim there are no branches in <code class="language-plaintext highlighter-rouge">jj</code>. There arenâ€™t branches in the usual git sense. In <code class="language-plaintext highlighter-rouge">jj</code>s graph there are â€œbranchesâ€, but they donâ€™t need names. The persistent <strong>change</strong> IDs serve as feature names and jump addresses (using <code class="language-plaintext highlighter-rouge">edit</code> or <code class="language-plaintext highlighter-rouge">new</code>). Nevertheless, branch names are introduced in <code class="language-plaintext highlighter-rouge">jj</code> in order to be compatible with git forges; they are called <strong>bookmarks</strong>, and they need to be moved explicitly across the <strong>changes</strong>.</p> <h4 id="logging-and-time-travel">Logging and Time Travel</h4> <p><code class="language-plaintext highlighter-rouge">jj</code> logs whatâ€™s happening using <strong>operations</strong>: these are the commands you enter and the current <em>change</em> youâ€™re in, in addition to some metadata. The logs enable handy features like <code class="language-plaintext highlighter-rouge">jj undo</code> or the deeper <code class="language-plaintext highlighter-rouge">jj op restore</code> (restore to any point in time). For browsing the operation log, see <code class="language-plaintext highlighter-rouge">jj op log</code>.</p> <p>There is a <a href="https://jj-vcs.github.io/jj/latest/git-command-table/">comparison</a> table between <code class="language-plaintext highlighter-rouge">git</code> and <code class="language-plaintext highlighter-rouge">jj</code> commands, which could be useful but itâ€™s important to not fixate on how <code class="language-plaintext highlighter-rouge">git</code> is doing things in order to be open to the new paradigm <code class="language-plaintext highlighter-rouge">jj</code> represents. However, some features seem to have been added in response to git usersâ€™ needs or workflows, or perhaps <code class="language-plaintext highlighter-rouge">jj</code> developers rediscovered the same needs.</p> <h4 id="mixing-jj-with-git">Mixing JJ with Git</h4> <p>I chose one existing git project and converted it to a mixed usage of <code class="language-plaintext highlighter-rouge">jj</code> and <code class="language-plaintext highlighter-rouge">git</code>, using <code class="language-plaintext highlighter-rouge">jj git init --colocate</code>. It means <code class="language-plaintext highlighter-rouge">jj</code> initialize its presence in an existing repo and it will keep updating the <code class="language-plaintext highlighter-rouge">.git</code> folder with whatâ€™s happening, at a level compatible with git constructs; for example, the <strong>changes</strong> are saved as git commits, <code class="language-plaintext highlighter-rouge">jj git fetch</code> is fetching from git forges into <code class="language-plaintext highlighter-rouge">.git/</code>, etc. I havenâ€™t used rebasing, squashing or other history-changing operations so I canâ€™t comment on how easy they are to use, maybe next time.</p> <p>If you found it interesting, give it a try. Thereâ€™s the <a href="https://jj-vcs.github.io/jj/latest/tutorial/">tutorial</a>, or you can run it on an existing git repo.</p>]]></content><author><name></name></author><category term="software"/><category term="git"/><summary type="html"><![CDATA[My first impression of using the Jujutsu version control system.]]></summary></entry><entry><title type="html">Built an MCP server for LLMs to search email from terminal | Daniel Fleischer posted on the topic | LinkedIn</title><link href="https://danielfleischer.github.io/blog/2025/built-an-mcp-server-for-llms-to-search-email-from-terminal-daniel-fleischer-posted-on-the-topic-linkedin/" rel="alternate" type="text/html" title="Built an MCP server for LLMs to search email from terminal | Daniel Fleischer posted on the topic | LinkedIn"/><published>2025-06-23T00:00:00+00:00</published><updated>2025-06-23T00:00:00+00:00</updated><id>https://danielfleischer.github.io/blog/2025/built-an-mcp-server-for-llms-to-search-email-from-terminal--daniel-fleischer-posted-on-the-topic--linkedin</id><content type="html" xml:base="https://danielfleischer.github.io/blog/2025/built-an-mcp-server-for-llms-to-search-email-from-terminal-daniel-fleischer-posted-on-the-topic-linkedin/"><![CDATA[<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>          Agree &amp; Join LinkedIn
        
  By clicking Continue to join or sign in, you agree to LinkedInâ€™s User Agreement, Privacy Policy, and Cookie Policy.

            AI Research Engineer @ Intel Labs
        ğŸ“¬ I built an MCP server that lets LLMs search my email from the terminal
</code></pre></div></div> <p>The server connects Claude to email search via the mu CLI tool. Now I just ask it things like: â€œFind emails with PDF attachments from last Aprilâ€ âš¡</p> <p>ğŸ›  No custom frontend. No heavy framework. Just a CLI tool made smarter.</p> <p>ğŸ’¡ I learned that MCP servers are basically API translators â€” they take complex developer SDKs and flatten them into simple function calls that LLMs can actually use.</p> <p>ğŸ¯ The bigger picture: This pattern can breathe new life into existing CLI tools and services. Complex APIs â†’ Simple, declarative functions â†’ Natural language queries.</p> <p>This isnâ€™t a product â€” just an experiment in stitching new capabilities into existing workflows. Code here: https://lnkd.in/eT2fJBSv</p> <p>mu email indexer and searcher: https://github.com/djcb/mu</p> <p>#MCP #LLM #EmailSearch #OpenSource #AI</p> <p>What existing tools would you want to make LLM-friendly? ğŸ¤” To view or add a comment, sign in</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>            Cloud, DevOps, and Full-stack professional
        Tired of manually writing function-calling definitions for every single API endpoint you want your LLM to use?
</code></pre></div></div> <p>I built Conduit to solve this. Itâ€™s an open-source tool that automatically exposes any existing GraphQL API to an LLM as a ready-to-use toolset.</p> <p>Conduit leverages GraphQLâ€™s strong typing and introspection capabilities to dynamically generate a tool manifest via the Model Context Protocol (MCP). An AI agent can connect to Conduit, understand the available tools (your GraphQL queries/mutations), and execute them.</p> <p>This automates the most tedious part of giving AI agents new capabilities, allowing you to focus on the agentâ€™s logic instead of the boilerplate. The entire project is written in TypeScript.</p> <p>If youâ€™re building with LLMs and need to connect to GraphQL data sources, this might save you a lot of time.</p> <p>Contributions and feedback are welcome! GitHub: https://lnkd.in/gr38VWv3 #LLM #FunctionCalling #AIAgents #MachineLearning #GraphQL #API #TypeScript #NodeJS #MLOps #AIengineering To view or add a comment, sign in</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>            Founder of Yearbook  | Flutter Application developer | Content Creator
        Are You Tired of Structuring AI Prompts 
</code></pre></div></div> <p>Microsoft got your back with newly launched POML (Prompt Orchestration Markup Language) Language , Its like html and based on xml and helps you structure your ptompts easily so you Can get the desired Output for the tokens spent.</p> <p>Hereâ€™s How to Use it , 1) Install POML by running pip install poml or npm install poml js â€“ for node js 2) Install VS code extension by searching â€˜pomlâ€™ 3) Crete a POML file and write your Promot 4) Test it with your preferred AI Model</p> <p>Thats it gor today follow me on linkedin For Business and Tech Insights.</p> <p>#poml #html #ai #prompt #promptengineering #future #microsoft To view or add a comment, sign in</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>            SDE at Anonimo | EX - HeyCoach, GFG | 3â­@Codechef(max 1615) | Pupil@codeforces(max 1242) | Web Development, React| Node Js| KLU'25
        After MCP, A2A, &amp; AG-UI, there's another Agent protocol that is  ACP (Agent Communication Protocol) and the best Part is it is fully Open Source
</code></pre></div></div> <p>But before we dive into ACP, letâ€™s quickly revisit the earlier 3 ğŸ‘‡</p> <ol> <li> <p>MCP (Model Context Protocol) Problem before MCP: Every new tool = custom integration code With MCP: One universal adapter for APIs, tools, files. -&gt; Think of it as the USB-C for agents â€” plug anything in, it just works.</p> </li> <li> <p>A2A (Agent-to-Agent) Problem before A2A: Agents couldnâ€™t hand off tasks cleanly â†’ worked in silos. With A2A: Agents collaborate like a team with walkie-talkiesÂ  -&gt; Funny Part is: Google made this A2A protocol just to show theyâ€™re updated in the market, like saying yes, after MCP, weâ€™ve launched something new.</p> </li> <li> <p>AG-UI (Agent â†’ UI) Problem before AG-UI: Each agent gave random outputs â†’ frontend devs had to write messy glue code. With AG-UI: Agents return standard UI components (forms, tables, charts). -&gt; Basically â†’ one design system for AI outputs</p> </li> <li> <p>ACP (Agent Communication Protocol) â€” by IBM ACP is like a traffic control system for coordinating agents at scale. Itâ€™s open source, RESTful, and even ships with a Python SDK!</p> </li> </ol> <p>Key ACP Features (solution-architect friendly):</p> <ol> <li>Supports both stateful and stateless agents</li> <li>Uses JSON-RPC for structured, reliable comms</li> <li>Handles natural language interfaces (so humans â†” agents is smooth)</li> <li>Offers real-time streaming (great for live dashboards, chat)</li> <li>Flexible deployment + legacy integration (you can fit it in old infra too) -&gt; Â In short: ACP = the dispatcher + rulebook for multiple agents working together.</li> </ol> <p>#AI #ArtificialIntelligence #AIAgents #AgenticAI #MultiAgentSystems #AICommunity #OpenSourceAI #AIStandards To view or add a comment, sign in</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>            I help labs reduce costs and improve the quality of diagnostics by the means of AI
        New release of AgentCraft (it's like Cursor, but for n8n). This Chrome extension integrates directly into the n8n interface and helps  build, configure, and debug workflows up to 10x faster with AI assistance.
</code></pre></div></div> <p>Key Features: ğŸ” Search &amp; insert ready-to-use workflows ğŸ’¬ MCP-powered chat with n8n documentation access âš¡ One-click workflow generation from natural language ğŸ”§ AI-powered node configuration ğŸŒ Generate and import CURL into HTTP node ğŸ› ï¸ AI Fixer for errors in workflows &amp; âš¡Generate JavaScript code ğŸ“‘ JSON auto-fixing ğŸ¤– Prompt generator for AI assistants ğŸ›ï¸ Trigger emulation with test data ğŸ—’ï¸ Auto-generated sticky notes ğŸ’¾ Autosave &amp; backup history</p> <p>Link - https://lnkd.in/ej-JkZKs To view or add a comment, sign in</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>            On Prem GenAI Digital Transformation for BFSI Sector
        We tested Microsoftâ€™s new POML (Prompt Orchestration Markup Language) at Ekkel AI on a few internal workflows to see how it performs in practice.
</code></pre></div></div> <p>Where it worked well: - Multi-step workflows: Breaking prompts into <role>, <task>, and <example> tags made our multi-agent setups easier to manage. - Reusable templates: We could reuse the same prompt structure across different projects just by swapping in variables and data sources. - Context embedding: Adding documents and tables directly into the prompt through POML tags improved accuracy for context-heavy tasks. - Version control: The structured format made it easier to track prompt changes in Git, which is harder with plain text prompts.</example></task></role></p> <p>Where it could improve: - Tooling maturity: The VS Code extension works well, but more integration with prompt testing platforms would help. - Language support: Official .NET SDK support would be valuable for certain enterprise stacks.</p> <p>Overall, POML is promising, especially for large-scale, repeatable prompt workflows. For smaller or ad-hoc tasks, plain prompts are still quicker. To view or add a comment, sign in</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>            Senior Data scientist, 5+ years, CV, NLP, Classic ML, Gen AI
        MCP is getting serious: first benchmark for real toolâ€‘use
</code></pre></div></div> <p>The Model Context Protocol (MCP) launched about nine months ago (Novâ€¯2024) and has been spreading fast across major platforms. Itâ€™s the â€œplugâ€ that lets models talk to real tools and data.</p> <p>Now thereâ€™s a dedicated benchmark: MCPâ€‘Universe (Salesforce AI Research). It tests models against real MCP servers across 6 domains - Location navigation, Repository management, Financial analysis, 3D design, Browser automation, Web searching - using executionâ€‘based checks on 231 tasks.</p> <p>A detail I like: tasks are multiâ€‘step. Runs typically take ~6-8 tool calls, which puts pressure on longâ€‘context handling and planning. In early results, GPTâ€‘5 tops the overall leaderboard and leads in most domains (with Grokâ€‘4 slightly ahead in browser automation).</p> <p>Bottom line: MCP is maturing fast â€” and now itâ€™s measurable. What gets measured gets improved, so expect rapid progress in tool use, planning, and context management next.</p> <p>Link: https://lnkd.in/g4KcpJFW To view or add a comment, sign in</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>            Babysitting AI agents
        Claude Code is a general purpose agent disguised as a coding agent.
</code></pre></div></div> <p>It searches the web. It integrates with tools through MCP. It executes multi-step workflows. All the basics youâ€™d expect.</p> <p>But itâ€™s not just a general-purpose agent. Itâ€™s a code-first agent, not just an agent that helps you code, but one that codes to expand what it can do.</p> <p>Having bash access, Claude Code controls your operating system directly. It can install packages, script CLI tools together, and automate anything you can do in a terminal. Combined with file system access, it can search, read, and modify files across your computer. Your entire system becomes its workspace, if you allow it.</p> <p>More importantly, it writes code. When other agents hit a wall because no tool exists for a task, Claude Code can build one. It can write parsers for unusual file formats. It can reverse-engineer undocumented APIs and build clients. It can create data processing pipelines, automation scripts, whatever the task demands. The tool doesnâ€™t exist until it needs to exist.</p> <p>Itâ€™s also composable, a capability unique to CLI-based agents. Claude Code can spawn specialized versions of itself as subagents, each with full tool access. Give it web search and file creation, and it can replicate features like â€œDeep Researchâ€. What others hard-code, it builds from primitives.</p> <p>Code-first agents donâ€™t just use tools. They script them together, build new ones, and orchestrate multiple instances of themselves to create capabilities that didnâ€™t exist before. Other agents work within their constraints. Claude Code can read its own source code, analyze how it works, and modify its configuration files to change its behavior.</p> <p>#anthropic #agent #claudecode #llm #ai To view or add a comment, sign in</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>            3 followers
        Here we are with the post about Request validation and validation in general. As it is required now to validate the tool CreateReservation query and API request to create reservation, I decided to use as much Laravel features as possible. Just write validation in the laravel v12 documentation ğŸ“œ,Â and the guidelines are pretty good I could tell you.
</code></pre></div></div> <p>The only thing I havenâ€™t spotted, before AI actually introduced me to it is make:rule artisan command, which is just a lifesaver if you know what I mean. The code looks much cleaner when all the additional rules lies outside of the StoreRequest. Even with AI, human beings are necessary to build up the rules, validation, and think ahead. So, with the custom validation within the policies of #EasyBookr it was necessary to inject several important rules:</p> <p>ğŸ•£15 minutes rule: I strongly believe that it makes sense to limit people with the timing divided by 15, meaning that you wonâ€™t be able to create reservation that is less then 15 minutes long. It does not make any sense. Also that will keep system cleaner, customers less confused and easier for everyone to track the time correctly.</p> <p>ğŸ“†Â Dates validation: Fixed rate type cannot have end date but only start date because it is fixed, so only one date is necessary to book the time correctly.</p> <p>ğŸ“…Â Employee schedule rule: Not only the correct date but also time should be correctly picked, since we all have our own working hours. That should also be included into account.</p> <p>ğŸ¤”Â Have I included all the rules into account? Letâ€™s see. Good to start on. I wonâ€™t introduce all the rules here, of course, some basic ones wonâ€™t be included into the post, but are included into the project to prevent security breakdowns. To view or add a comment, sign in</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>            Magento Developer at Scandesign Media A/S
        Validation rules. Are they widely used? I got to know them only when I reached the API flow. However, they may be more commonly used without a package, such as Filament? Don't know. I got to know it only because of AI Tools and API scope, but not on the frontend. 
            3 followers
        Here we are with the post about Request validation and validation in general. As it is required now to validate the tool CreateReservation query and API request to create reservation, I decided to use as much Laravel features as possible. Just write validation in the laravel v12 documentation ğŸ“œ,Â and the guidelines are pretty good I could tell you.
</code></pre></div></div> <p>The only thing I havenâ€™t spotted, before AI actually introduced me to it is make:rule artisan command, which is just a lifesaver if you know what I mean. The code looks much cleaner when all the additional rules lies outside of the StoreRequest. Even with AI, human beings are necessary to build up the rules, validation, and think ahead. So, with the custom validation within the policies of #EasyBookr it was necessary to inject several important rules:</p> <p>ğŸ•£15 minutes rule: I strongly believe that it makes sense to limit people with the timing divided by 15, meaning that you wonâ€™t be able to create reservation that is less then 15 minutes long. It does not make any sense. Also that will keep system cleaner, customers less confused and easier for everyone to track the time correctly.</p> <p>ğŸ“†Â Dates validation: Fixed rate type cannot have end date but only start date because it is fixed, so only one date is necessary to book the time correctly.</p> <p>ğŸ“…Â Employee schedule rule: Not only the correct date but also time should be correctly picked, since we all have our own working hours. That should also be included into account.</p> <p>ğŸ¤”Â Have I included all the rules into account? Letâ€™s see. Good to start on. I wonâ€™t introduce all the rules here, of course, some basic ones wonâ€™t be included into the post, but are included into the project to prevent security breakdowns. To view or add a comment, sign in</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>            Leading Developer Advocacy @ CrewAI â€¢ Automating business workflows with AI
        iâ€™ve wired up a bunch of MCP servers but not for semantic code search.
</code></pre></div></div> <p>i decided to try out the new Claude Context MCP plugin by Zilliz using Claude Code and I found it quite interesting and I think you will as well.</p> <p>so what is it? itâ€™s a powerful semantic code search tool that gives your AI coding assistants deep understanding of your entire codebase. Instead of grep or basic keyword search, you can ask natural language questions about your codebase and get actual relevant code back.</p> <p>the setup is interesting (mermaid diagram in comments), it takes your code: â€¢ parses it through AST (Abstract Syntax Tree)[1] (chunk by structure) â€¢ chunks it intelligently â€¢ embeds the code using openai/voyage/ollama/geminiÂ  â€¢ stores everything in Milvus, created by Zilliz vector db which is well known for being a beast in handling large-scale vector search across billions of embeddings</p> <p>to top it all off, it uses one of my favorite data structures, Merkle Trees. This allows it to perform incremental indexing on new changes and not the entire codebase.</p> <p>since it works through MCP, it integrates with claude code, cursor, vscode, and other MCP clients. Iâ€™ve added the links to the repo + setup instructions in the chat.</p> <p>have you tried using this server yet?</p> <p>[1] an Abstract Syntax Tree (AST) is a hierarchical representation of your codeâ€™s structure. To view or add a comment, sign in</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>        1,432 followers
      
                Create your free account or sign in to continue your search
              
          or
        
  By clicking Continue to join or sign in, you agree to LinkedInâ€™s User Agreement, Privacy Policy, and Cookie Policy.

            New to LinkedIn? Join now
          
                      or
                    
                New to LinkedIn? Join now
              
  By clicking Continue to join or sign in, you agree to LinkedInâ€™s User Agreement, Privacy Policy, and Cookie Policy.
</code></pre></div></div>]]></content><author><name></name></author><summary type="html"><![CDATA[ğŸ“¬ I built an MCP server that lets LLMs search my email from the terminal The server connects Claude to email search via the mu CLI tool. Now I just ask it things like: "Find emails with PDF attachments from last April" âš¡ ğŸ›  No custom frontend. No heavy framework. Just a CLI tool made smarter. ğŸ’¡ I learned that MCP servers are basically API translators â€” they take complex developer SDKs and flatten them into simple function calls that LLMs can actually use. ğŸ¯ The bigger picture: This pattern can breathe new life into existing CLI tools and services. Complex APIs â†’ Simple, declarative functions â†’ Natural language queries. This isnâ€™t a product â€” just an experiment in stitching new capabilities into existing workflows. Code here: https://lnkd.in/eT2fJBSv mu email indexer and searcher: https://github.com/djcb/mu #MCP #LLM #EmailSearch #OpenSource #AI What existing tools would you want to make LLM-friendly? ğŸ¤”]]></summary></entry><entry><title type="html">Summarize Hacker News Posts with Haystack &amp;amp; OPEA | Haystack</title><link href="https://danielfleischer.github.io/blog/2025/summarize-hacker-news-posts-with-haystack-opea-haystack/" rel="alternate" type="text/html" title="Summarize Hacker News Posts with Haystack &amp;amp; OPEA | Haystack"/><published>2025-06-10T00:00:00+00:00</published><updated>2025-06-10T00:00:00+00:00</updated><id>https://danielfleischer.github.io/blog/2025/summarize-hacker-news-posts-with-haystack--opea--haystack</id><content type="html" xml:base="https://danielfleischer.github.io/blog/2025/summarize-hacker-news-posts-with-haystack-opea-haystack/"><![CDATA[<p>Build a RAG pipeline to fetch live Hacker News posts and summarize them with a local LLM endpointWelcome to this step-by-step tutorial where weâ€™ll build a simple Retrieval-Augmented Generation (RAG) pipeline using Haystack and OPEA. Weâ€™ll fetch the newest Hacker News posts, feed them to a lightweight LLM endpoint (OPEAGenerator), and generate concise one-sentence summaries (based on this notebook). Letâ€™s dive in! ğŸ‰In modern GenAI applications, having a flexible, performant, and scalable platform is essential. OPEA (Open Platform for Enterprise AI) is an open, model-agnostic framework for building and operating composable GenAI solutions. It provides:In this demo, weâ€™ll use an OPEA LLM endpoint in a Haystack pipeline, giving you:In this tutorial, weâ€™ll build a simple RAG pipeline that fetches the newest Hacker News posts, sends them to a local OPEA endpoint running a Qwen/Qwen2.5-7B-Instruct demo model, and produces concise one-sentence summaries. Of course, you can replace our example model with any other OPEA-served model, making this pattern both lightweight for prototyping and powerful for real-world deployments. Letâ€™s get started! ğŸš€Make sure you have:NOTE: As a reference, here is a Docker Compose recipe to get you started. OPEA LLM service can be configured to use a variety of model serving backends like TGI, vLLM, ollama, OVMSâ€¦ and offers validated runtime settings for good performance on various hardwareâ€™s including Intel Gaudi. In this example, it creates an OPEA LLM service with a TGI backend. See the documentation for LLM Generation. The code is based on OPEA LLM example and OPEA TGI example.To run, call LLM_MODEL_ID=Qwen/Qwen2.5-7B-Instruct docker compose up.Weâ€™ll create a custom Haystack component, HackernewsNewestFetcher, that:We use the OPEAGenerator to call our LLM over HTTP. Here, we point to a local endpoint serving the Qwen/Qwen2.5-7B-Instruct model:Using PromptBuilder, we define a Jinja-style template that:We wire up the components in a Pipeline:Fetch and summarize the top 2 newest Hacker News posts:Beautiful, concise summaries in seconds! âœ¨In this tutorial, we built a full RAG pipeline:Feel free to extend this setup with more advanced retrieval, caching, or different LLM backends. Happy coding! ğŸ› ï¸ğŸ”¥ Building products, technology and solutions for LLM-enabled applications.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Build a RAG pipeline to fetch live Hacker News posts and summarize them with a local LLM endpoint]]></summary></entry><entry><title type="html">×”××•×“×œ ×©×œ×›× ×”×•×–×”? ×›×š ×ª×‘× ×• ×‘×¢×¦××›× ××¢×¨×›×ª RAG | ×’×™×§×˜×™×™×</title><link href="https://danielfleischer.github.io/blog/2024/rag/" rel="alternate" type="text/html" title="×”××•×“×œ ×©×œ×›× ×”×•×–×”? ×›×š ×ª×‘× ×• ×‘×¢×¦××›× ××¢×¨×›×ª RAG | ×’×™×§×˜×™×™×"/><published>2024-12-01T00:00:00+00:00</published><updated>2024-12-01T00:00:00+00:00</updated><id>https://danielfleischer.github.io/blog/2024/-------rag--</id><content type="html" xml:base="https://danielfleischer.github.io/blog/2024/rag/"><![CDATA[<p>×—×™×¤×•×©×™× ×—××™×: ××¢×¨×›×•×ª RAG ×™×›×•×œ×•×ª ×œ×¤×ª×•×¨ ××ª ×ª×•×¤×¢×ª ×”×”×–×™×•×ª ×©×œ LLMs, ××‘×œ ×›×“×™ ×œ×‘× ×•×ª ××—×ª ×˜×•×‘×” ×ª×¦×˜×¨×›×• ×œ×§×‘×œ ×”×¨×‘×” ×”×—×œ×˜×•×ª ×§×¨×™×˜×™×•×ª (×•×œ×§×•×•×ª ×©×ª×’×™×¢×• ×”×›×™ ×§×¨×•×‘ ×œ××•×©×œ×) ×”××ª×’×¨ ×”×¢×™×§×¨×™ ×‘×‘× ×™×™×ª ××¢×¨×›×ª RAG ×”×•× ×œ×”×’×™×¢ ×œ×¨××ª ××™×›×•×ª ×•×“×™×•×§ ××§×¡×™××œ×™×™× (×¦×™×œ×•×: Dreamstime)×××ª ×“× ×™××œ ×¤×œ×™×™×©×¨, ×—×•×§×¨ AI ×‘-Intel Labs××•×“×œ×™ ×©×¤×” ×’×“×•×œ×™× (LLMs) ××¦×™×¢×™× ×¤×ª×¨×•× ×•×ª ××ª×§×“××™× ×œ××ª×’×¨×™ ×¤×™×ª×•×—, ××š ×œ×¢×ª×™×, ××” ×œ×¢×©×•×ª, ×”× ×©×•×’×™×. ×”××˜×¨×” ×©×œ ××¢×¨×›×•×ª RAG ×”×™× ×œ×¤×ª×•×¨ ××ª ×”×‘×¢×™×” ×”×–××ª, ×‘×××¦×¢×•×ª ×’×™×©×” ×—×“×©×” ×œ×©×™×¤×•×¨ ×“×™×•×§ ×•×××™× ×•×ª ×”××•×“×œ×™×. ××™×š ×”×Ÿ ×¢×•×©×•×ª ××ª ×–×”, ×”×× ×”×”×™×™×¤ ××•×¦×“×§ ×•×›×™×¦×“ × ×•×›×œ ×œ×‘× ×•×ª ××¢×¨×›×ª RAG ××©×œ× ×•? ×›×“×™ ×œ×¢× ×•×ª ×¢×œ ×”×©××œ×•×ª ×”××œ×”, ×‘×“×§×ª×™ ××ª ×”××ª×’×¨×™× ×‘×’×™×©×” ×–×• ××œ ××•×œ ×”×™×ª×¨×•× ×•×ª ×©×”×™× ××¦×™×¢×” ×‘×©×“×¨×•×’ ×”××™× ×˜×¨××§×¦×™×” ×©×œ× ×• ×¢× AI.×œ×¤× ×™ ×©× ×¦×œ×•×œ, ×‘×•××• × ×“×‘×¨ ×¨×’×¢ ×¢×œ LLMs. ×œ××¢×©×”, ××“×•×‘×¨ ×‘×™×™×¦×•×’ ××ª××˜×™ ×©×œ ×©×¤×” ×˜×‘×¢×™×ª ×‘×××¦×¢×•×ª ×”×¡×ª×‘×¨×•×™×•×ª: ××•×“×œ ×”×©×¤×” ×™×›×•×œ ×œ×—×©×‘ ×”×¡×ª×‘×¨×•×ª ×œ×›×œ ××©×¤×˜ ×‘×©×¤×” ×©×‘×” ××™×× ×• ××•×ª×•, ×•×›×š ×”×•× ×™×›×•×œ ×œ×™×™×¦×¨ ××©×¤×˜×™× ×—×“×©×™×, ××™×œ×” ××—×¨ ××™×œ×”. ××•×“×œ ×”×©×¤×” ×œ××¢×©×” â€œ×××©×™×š ××ª ×”××©×¤×˜â€: ××–×™× ×™× ×”×ª×—×œ×” ×©×œ ××©×¤×˜, ×•×”×•× ×××©×™×š. ×”×©×™××•×©×™× ×”× ×›××¢×˜ ××™× ×¡×•×¤×™×™×: ××¤×©×¨ ×œ×‘×§×© ××”××•×“×œ ×œ×¢× ×•×ª ×¢×œ ×©××œ×•×ª, ×œ×¡×›× ×˜×§×¡×˜×™×, ×œ×—×‘×¨ ×©×™×¨×™×, ×œ×›×ª×•×‘ ×¤×•× ×§×¦×™×•×ª ×‘×©×¤×•×ª ×ª×›× ×•×ª ×©×•× ×•×ª ×•××¤×™×œ×• ×œ×™×™×¦×¨ ×ª××•× ×•×ª ×‘×¦×•×¨×” ×¡×™× ×ª×˜×™×ª.××‘×œ ×œ× ×”×›×œ ××•×©×œ×, ×•××—×“ ×”×¤×’××™× ×‘×ª×¤×§×•×“ ×©×œ LLMs ×”×•× ×”×”×–×™×•×ª ×”××•×›×¨×•×ª ×œ×›×•×œ× ×• â€“ ××•×ª× ××§×¨×™× ×©×‘×”× ××•×“×œ×™ ×©×¤×” ××¡×¤×§×™× ×ª×©×•×‘×•×ª ×©×’×•×™×•×ª ×¢×•×‘×“×ª×™×ª, ×œ× ×¨×œ×•×•× ×˜×™×•×ª ××• ××•×¤×¨×›×•×ª ×œ×—×œ×•×˜×™×Ÿ. ×›×“×™ ×œ×”×•×¡×™×£ ×—×˜× ×¢×œ ×¤×©×¢, ××•×“×œ×™ ×”×©×¤×” ×œ× ×ª××™×“ ××¡×™×™×’×™× ××ª ×ª×©×•×‘×•×ª×™×”× ×•×¢×•× ×™× ×‘×‘×™×˜×—×•×Ÿ ××œ×, ×•×œ×›×Ÿ ×”×”×–×™×•×ª ×”×Ÿ ××¡×•×›× ×•×ª ×•×§×©×•×ª ×œ×’×™×œ×•×™.××—×ª ×”×“×¨×›×™× ×œ×”×ª××•×“×“ ×¢× ×ª×•×¤×¢×ª ×”×”×–×™×•×ª ×”×™× RAG: Retrieval Augmented Generation (××• ×‘×¢×‘×¨×™×ª, â€œ×›×ª×™×‘×ª ×˜×§×¡×˜ ×‘×¢×–×¨×ª ×©×œ×™×¤×” ×•×”×¢×©×¨×” ×‘×™×“×¢â€). ×‘×©×™×˜×” ×–×•, ××•×“×œ ×”×©×¤×” × ×¢×–×¨ ×‘×™×“×¢ ×—×™×¦×•× ×™ ×©××•×ª×• ×”×•× ×©×•×œ×£ ××• ××§×‘×œ ×œ×¦×•×¨×š ×”×©×œ××ª ×”××©×™××”, ×›×“×™ ×©×”×ª×©×•×‘×•×ª ×™×”×™×• ××“×•×™×§×•×ª ×™×•×ª×¨. RAG ×”×•× ×©× ×›×•×œ×œ ×œ×›×œ ×”×˜×›× ×™×§×•×ª ×œ×‘× ×™×™×ª ××¢×¨×›×•×ª ×©×‘×”×Ÿ ××—×‘×¨×™× ××•×“×œ×™ ×©×¤×” ×œ×™×“×¢ ×—×™×¦×•× ×™ ×œ×¦×•×¨×š ×©×™×¤×•×¨ ×”×ª×•×¦××•×ª.××¢×¨×›×ª RAG ×›×•×œ×œ×ª ×××’×¨ ×™×“×¢ ×›×’×•×Ÿ ×‘×¡×™×¡ × ×ª×•× ×™×, ××•×¡×£ ×©×œ ××¡××›×™× ×•××¤×™×œ×• ×›×œ×™ ×—×™×¤×•×© ×‘×’×•×’×œ. ×›×©×× ×—× ×• ××–×™× ×™× ×‘×§×©×” ×œ××¢×¨×›×ª, ×”×™× ×××ª×¨×ª ×¤×™×¡×•×ª ××™×“×¢ ×¨×œ×•×•× ×˜×™×•×ª ×‘×ª×•×š ×××’×¨×™ ×”×™×“×¢. ××œ×” ×¢×•×‘×¨×™× ×™×—×“ ×¢× ×”×‘×§×©×” ×”××§×•×¨×™×ª ××œ ××•×“×œ ×”×©×¤×”, ×•×”×•× ×¢×•× ×” ×¢×œ ×”×‘×§×©×” â€“ ×‘×ª×§×•×•×” ×©×”×ª×©×•×‘×” ×ª×”×™×” ××œ××”, ××“×•×™×§×ª ×•×¨×œ×•×•× ×˜×™×ª, ×‘×–×›×•×ª ×”××™×“×¢ ×©×”××•×“×œ ×§×™×‘×œ. ××—×–×•×¨ ××™×“×¢ (retrieval) ×¨×œ×•×•× ×˜×™ ×•×”×¢×‘×¨×ª×• ×œ××•×“×œ ×”×©×¤×” ××©×¤×¨×™× ×‘××•×¤×Ÿ ××•×›×— ×•××©××¢×•×ª×™ ××ª ×”×‘×™×¦×•×¢×™× ×©×œ ××•×“×œ×™ ×©×¤×” ×‘××’×•×•×Ÿ ××©×™××•×ª ×”××¦×¨×™×›×•×ª ×™×“×¢ â€“ ×›×•×œ×œ ××¢× ×” ×¢×œ ×©××œ×•×ª (Q&amp;A), ××™×•×Ÿ (Classification), ×¡×™×›×•× ×•×¢×•×“.×œ××¢×¨×›×•×ª RAG ×™×©× × ×©×™××•×©×™× ×©×•× ×™×, ×•×”×™×ª×¨×•×Ÿ ×”××•×‘×”×§ ×©×œ×”×Ÿ ×”×•× ×”×™×›×•×œ×ª ×œ×¨×ª×•× ×××’×¨×™ ×™×“×¢ ×‘××˜×¨×” ×œ×”×©×œ×™× ××ª ×”××©×™××”. ×××’×¨×™× ××œ×” ×”× ×”×“×¨×š ×”×¢×™×§×¨×™×ª ×œ×”×•×¡×™×£ ×™×“×¢ ×—×“×© ×•×œ×¢×“×›×Ÿ ×™×“×¢ ×§×™×™× ×‘××•×“×œ×™ ×”×©×¤×”, ×•×œ×›×Ÿ ×”× ××•×¦×œ×—×™× ×›×œ ×›×š. ×‘× ×•×¡×£, ×‘×—×™×¨×” ×“×™× ××™×ª ×©×œ ×××’×¨×™ ×”×™×“×¢ ×¤×•×ª×—×ª ××ª ×”×“×œ×ª ×œ××•×“×œ×™ ×©×¤×” ××•×ª×××™× ××™×©×™×ª.×§×—×• ×œ×“×•×’××” ×¢×•×–×¨ ××™×©×™ (digital assistant). ×‘××§×¨×” ×”×–×” ××¢×¨×›×ª ×”-RAG ××—×•×‘×¨×ª ×œ×××’×¨×™ ×™×“×¢ ××§×¦×•×¢×™×™×. ×‘×–×›×•×ª ×”×™×›×•×œ×•×ª ×©×œ ××•×“×œ ×”×©×¤×”, ×”××¢×¨×›×ª ×™×›×•×œ×” ×œ×©×•×—×— ×‘×©×¤×” ×˜×‘×¢×™×ª ×¢×œ ××’×•×•×Ÿ × ×•×©××™× ×˜×›× ×™×™×, ×œ×™×™×¢×¥ ×•×œ×¢× ×•×ª ×¢×œ ×©××œ×•×ª. ×‘× ×•×¡×£, ×”××¢×¨×›×ª ×™×›×•×œ×” ×œ×”×¤× ×•×ª ××ª ×”××©×ª××© ×œ××§×•×¨×•×ª ×”×™×“×¢ ×¢×¦×× (×‘×××¦×¢×•×ª ×¦×™×˜×•×˜, citation), ×›××¢×™×Ÿ ×¢×•×–×¨ ××—×§×¨.×“×•×’××” × ×•×¡×¤×ª ×”×™× ×©×™××•×© ×‘×™×“×¢ ×©×œ ×”××©×ª××© ×¢×¦××•, ×›×œ×•××¨ â€“ ×××’×¨ ×”×™×“×¢ ××‘×•×¡×¡ ×¢×œ ×”×“××˜×” ×©×œ ×”××©×ª××©, ×•×›×š ××¢×¨×›×ª ×”-RAG ××ª××™××” ×œ×• ××ª ×ª×©×•×‘×•×ª×™×”. ×”××©×ª××© ×™×›×•×œ ×œ×”×¢×œ×•×ª ××•×¡×£ ×§×‘×¦×™× ××™×©×™×™× ×œ××¢×¨×›×ª ×•×œ×‘×§×© ××× ×” ×œ×¡×›× × ×•×©××™× ×”××•×¤×™×¢×™× ×‘××¡××›×™×, ×œ×©××•×œ ×”×™×›×Ÿ × ××¦× ×“×™×•×Ÿ ×‘× ×•×©× ×›×–×” ××• ××—×¨, ×œ×‘×§×© ×©×ª×¨×›×™×‘ ××¦×’×ª ×¡×™×›×•× ××”××¡××›×™× ×•×›×Ÿ ×”×œ××”.×›××©×¨ × ×™×’×©×™× ×œ×‘× ×•×ª ××¢×¨×›×ª RAG ×¦×¨×™×š ×œ×§×‘×œ ×”×—×œ×˜×•×ª ×¨×‘×•×ª. ×‘×©×œ×‘ ×”×¨××©×•×Ÿ ×¢×œ×™× ×• ×œ×‘×—×•×¨ ××•×“×œ ×©×¤×” â€“ ×¤×ª×•×— ××• ×¡×’×•×¨. ××•×“×œ ×¡×’×•×¨ ×”×•× ××•×“×œ ××¡×—×¨×™ ××™×ª×• ×¢×•×‘×“×™× ×‘×¢×–×¨×ª ×××©×§ API, ×•××™×Ÿ ×œ× ×• ×’×™×©×” ×œ××•×¤×Ÿ ×¤×¢×•×œ×ª ×”××•×“×œ ××• ×”××©×§×•×œ×•×ª ×©×œ×•. ×œ×¢×•××ª×•, ×‘××•×“×œ ×¤×ª×•×— ×’× ×”×§×•×“ ×•×’× ×”××©×§×•×œ×•×ª × ×’×™×©×™× ×œ×‘×—×™× ×”, ××™××•×Ÿ ×•×”×’×©×” (inference). ××ª ×”××•×“×œ ×”×¤×ª×•×— × ×™×ª×Ÿ ×œ×”×ª××™× ×œ×¢×•×œ× ×”×ª×•×›×Ÿ ×©×œ× ×• ×‘×¢×–×¨×ª ××™××•×Ÿ fine tuning. ×‘×—×œ×§ ××”××§×¨×™× ×”×’×©×” ×¢×¦××™×ª ×©×œ ××•×“×œ×™× ×¤×ª×•×—×™× ×¢×©×•×™×” ×œ×”×™×•×ª ×¢×“×™×¤×” ×¢×œ ×¤× ×™ ××•×“×œ×™× ×¡×’×•×¨×™×, ××š ×”×™× ××¦×¨×™×›×” ××™×•×× ×•×ª.×”×—×œ×§ ×”×©× ×™ ×‘×‘× ×™×™×ª ××¢×¨×›×ª RAG ×”×•× ×—×™×‘×•×¨ ×œ×××’×¨×™ ×™×“×¢. ×œ×—×™×‘×•×¨ ×”×–×” ×™×©× × ×”×™×‘×˜×™× ×¨×‘×™×, ×”×›×•×œ×œ×™× ××ª ××•×¤×Ÿ ××™× ×“×•×§×¡ ×”×™×“×¢, ×¢×™×‘×•×“ ××§×“×™× ×©×œ ×”×˜×§×¡×˜ ×•×”×ª××•× ×•×ª, ×—×œ×•×§×” ×œ×¤×¡×§××•×ª ××• ××©×¤×˜×™×, × ×™×§×•×™, ×¢×™×‘×•×“ ×©×œ ×“××˜×” ×˜×‘×œ××™, ×”××¨×ª ×”×“××˜×” ×œ×™×™×¦×•×’ ×•×§×˜×•×¨×™ (vector embedding) ×©×™×›×•×œ ×œ×©×¤×¨ ××ª ××™×›×•×ª ×”×—×™×¤×•×©, ×—×™×¤×•×© ××‘×•×¡×¡ ××™×œ×™×, ×—×™×¤×•×© ×¡×× ×˜×™ ××• ×©×™×œ×•×‘ ×©×œ×”×, ××¡×¤×¨ ×”×“×•×’×××•×ª ×œ××—×–×•×¨, ××™×•×Ÿ ×¨×œ×•×•× ×˜×™×•×ª, ×¡×™× ×•×Ÿ, ×©×™×›×ª×•×‘ ×”×“×•×’×××•×ª, ×¡×™×›×•× ×•×¢×•×“. ×–×” ×××© ×¢×œ ×§×¦×” ×”××–×œ×’, ×•×¨×§ ×¢×œ ×”×©×œ×‘ ×”×–×” ×™×›×•×œ×ª×™ ×œ×›×ª×•×‘ ××××¨ ×©×œ×. ×”×—×œ×§ ×”×–×” ×§×¨×™×˜×™, ×›×™ ×›××• ×©××•××¨×™× â€“ garbage in, garbage out: ×× ××—×–×•×¨ ×”×™×“×¢ ×™×”×™×” ×œ× ××“×•×™×§ ×•×œ× ×¨×œ×•×•× ×˜×™ ××¡×¤×™×§, ××™×›×•×ª ×”×ª×©×•×‘×•×ª ×©×œ ××•×“×œ ×”×©×¤×” ×ª×¤×’×¢× ×” ××™×“, ×•×”×¡×™×›×•×™ ×œ×”×–×™×•×ª ×™×’×“×œ.×œ××—×¨ ××›×Ÿ × ×™×ª×Ÿ ×œ×”×•×¡×™×£ ××¨×›×™×‘×™× ×œ××¢×¨×›×ª: ××•×“×œ×™ ×©×¤×” × ×•×¡×¤×™× ×©× ×‘×—×¨×™× ×‘×¦×•×¨×” ×“×™× ××™×ª ×¢×œ ×¤×™ ××•×¤×™ ×”××©×™××”; ×©×™××•×© ×‘×›××” ×××’×¨×™ ×™×“×¢ ×‘××§×‘×™×œ; ×©×™××•×© ×‘×›×œ×™× ×”××•×“×“×™× ××ª ××™×›×•×ª ×”××¡××›×™× ×©× ××¦××•, ×›×š ×©× ×™×ª×Ÿ ×™×”×™×” ×œ×œ×§×˜ ××ª ×§×˜×¢×™ ×”××™×“×¢ ×”×¨×œ×•×•× ×˜×™×™× ×‘×™×•×ª×¨ ××ª×•×š ×”××¡××›×™× ×©×—×–×¨×•; ××•×“×œ×™ ×©×¤×” ×©×™×•×“×¢×™× ×œ×‘×§×© ×‘×™×¦×•×¢ ××—×–×•×¨ × ×•×¡×£ ×× ×”× ×œ× ××¦××• ×¢×“×™×™×Ÿ ××ª ×”×ª×©×•×‘×” ×©×—×™×¤×©×•; ×•×”×¨×©×™××” ×××©×™×›×”, ×›×™××” ×œ×ª×—×•× ××—×§×¨ ×¤×¢×™×œ ×‘×™×•×ª×¨.×”××ª×’×¨ ×”×¢×™×§×¨×™ ×‘×‘× ×™×™×ª ××¢×¨×›×ª RAG ×”×•× ×œ×”×’×™×¢ ×œ×¨××ª ××™×›×•×ª ×•×“×™×•×§ ××§×¡×™××œ×™×™×. ×›×œ ×—×œ×§×™ ×”××¢×¨×›×ª ××©×¤×™×¢×™× ×¢×œ ×”×ª×©×•×‘×•×ª ×”×¡×•×¤×™×•×ª, ×•×œ×›×Ÿ ××•×›×¨×—×™× ×œ×ª×›× ×Ÿ ××•×ª×” ×‘×§×¤×“× ×•×ª. ×›×—×œ×§ ××”×”×›× ×”, ×›×“××™ ×œ×‘×—×•×Ÿ ×•×œ×”×©×•×•×ª ××•×“×œ×™ ×©×¤×” ×¢×œ ×× ×ª ×œ×”×’×™×¢ ×œ×©×™×œ×•×‘ ×¨××•×™ ×©×œ ×¢×œ×•×ª ××•×œ ×“×™×•×§. ×‘×—×™×¨×ª ×××’×¨×™ ×”×™×“×¢, ××•×¤×™ ×”××—×¡×•×Ÿ, ××—×–×•×¨ ×•×”×¦×’×ª ×”×™×“×¢ ×œ××•×“×œ ×”× ×§×¨×™×˜×™×™× ×‘×™×™×¦×•×¨ ×ª×©×•×‘×•×ª × ×›×•× ×•×ª ×•×¨×œ×•×•× ×˜×™×•×ª ×‘×–××Ÿ ×¨×™×¦×” ×¡×‘×™×¨. ×›×“×™ ×œ×‘×—×•×Ÿ ××ª ×˜×™×‘ ×”××¢×¨×›×ª ×©×‘× ×™×ª×, × ×™×ª×Ÿ ×œ×”×©×ª××© ×‘×“×•×’×××•×ª ××ª×•×™×’×•×ª (×“×•×’×××•×ª ×”××›×™×œ×•×ª ×ª×©×•×‘×•×ª ×™×“×•×¢×•×ª ××¨××©) ×•×œ×‘×¦×¢ ×‘×—×™× ×” ×™×“× ×™×ª ××• ××•×˜×•××˜×™×ª ×©×œ ×”×ª×•×¦××•×ª.×”××•×¨×›×‘×•×ª ×”×’×“×•×œ×” × ×•×‘×¢×ª ×‘×¢×™×§×¨ ××”×”×ª× ×”×’×•×ª ×”×œ× ×¦×¤×•×™×” ×œ×¢×ª×™× ×©×œ LLMs. ×œ×“×•×’××”, ×‘×—×œ×§ ×§×˜×Ÿ ××”××§×¨×™× ××•×“×œ ×”×©×¤×” ×™×©×’×”, ××¤×™×œ×• ×©×”×™×“×¢ ×©×§×™×‘×œ ××›×™×œ ××ª ×”×ª×©×•×‘×” ×”× ×›×•× ×”. ×”×¡×™×‘×•×ª ×œ× ×‘×¨×•×¨×•×ª × ×•×‘×¢×•×ª ××¡×ª×™×¨×” ×‘×™×Ÿ ×”×™×“×¢ ×”×¤× ×™××™ ×©×œ ×”××•×“×œ ×œ×™×“×¢ ×”××•×¦×’ ×‘×¤× ×™×•. ×”×¦×•×•×ª ×©×œ× ×• ×—×•×§×¨ ×“×¨×›×™× ×‘×”×Ÿ × ×™×ª×Ÿ ×œ×©×œ×•×˜ ×‘×”×ª× ×”×’×•×ª ×”××•×“×œ×™× ×‘××§×¨×™× ×›××œ×”, ×‘×¢×–×¨×ª ×”×•×¨××•×ª ××•×ª×××•×ª ×•××™××•×Ÿ × ×•×¡×£.×™×—×“ ×¢× ×–××ª, × ×™×ª×Ÿ ×œ×©×¤×¨ ××ª ××™×›×•×ª ×”××¢×¨×›×ª ×›×•×œ×” ×‘×××¦×¢×•×ª ×”×ª×××ª ×”××•×“×œ ×œ×‘×™×¦×•×¢ ××©×™××•×ª RAG. ×œ×¦×•×¨×š ×›×š, ×”×¦×•×•×ª ×©×œ× ×• ×¤×™×ª×— ×›×œ×™ ×§×•×“ ×¤×ª×•×— ×©×××¤×©×¨ ×œ×××Ÿ ×•×œ×©×¤×¨ ××ª ×™×›×•×œ×•×ª ×”-RAG ×©×œ ××•×“×œ×™ ×©×¤×”.×›×“×™ ×œ×‘× ×•×ª ××¢×¨×›×ª RAG ××™×›×•×ª×™×ª ×•××“×•×™×§×ª, × ×“×¨×©×ª ×”×‘× ×” ×¢××•×§×” ×©×œ ×”×”×™×‘×˜×™× ×”×©×•× ×™× ×©×œ×”. ×›××•×‘×Ÿ ×©××™ ××¤×©×¨ ×œ×•×•×ª×¨ ×¢×œ ×ª×”×œ×™×š × ×™×¡×•×™ ×•×˜×¢×™×”, ×©×¢×•×–×¨ ×œ×©×¤×•×š ××•×¨ ×¢×œ ×”×¤×©×¨×•×ª ×”×©×•× ×•×ª ×”×›×¨×•×›×•×ª ×‘×¢×™×¦×•×‘ ×”××¢×¨×›×ª. ×¨×§ ×›×š × ×•×›×œ ×œ×‘× ×•×ª ××ª ×”××¢×¨×›×ª ×”××™×“×™××œ×™×ª ×œ×‘×¢×™×” ×©××•×ª×” ×× ×—× ×• ×× ×¡×™× ×œ×¤×ª×•×¨.××¢×¨×›×•×ª RAG ××™×™×¦×’×•×ª ××¨×›×™×˜×§×˜×•×¨×” ×—×“×©×” ×”××©×œ×‘×ª ××•×“×œ×™ ×©×¤×” ×¢× ×××’×¨×™ × ×ª×•× ×™×. ×‘×–×›×•×ª ×”×©×™×œ×•×‘ ×”×–×” ×™×© ×œ×”× ×¤×•×˜× ×¦×™××œ ×œ×‘×¦×¢ ××©×™××•×ª ×¢×ª×™×¨×•×ª ×™×“×¢, ×›×’×•×Ÿ ×¢×•×–×¨×™× ×“×™×’×™×˜×œ×™×™× ××•×ª×××™× ××™×©×™×ª, ×¦â€™××˜×‘×•×˜ ×‘×©×™×¨×•×ª ×œ×§×•×—×•×ª ×•××¢×¨×›×•×ª ×™×“×¢ ××¨×’×•× ×™×•×ª. ×•×× ×œ×©×¤×•×˜ ×œ×¤×™ ×”××—×§×¨ ×”×¤×¢×™×œ ×‘×ª×—×•×, ×–×•×”×™ ×¨×§ ×”×”×ª×—×œ×”.×”×›×•×ª×‘ ×”×•× ×—×•×§×¨ ×‘××¢×‘×“×ª ×”Ö¾NLP ×‘××¨×’×•×Ÿ Intel Labs. ×‘××¢×‘×“×” × ×—×§×¨×•×ª ×¡×•×’×™×•×ª ×”×§×©×•×¨×•×ª ×œ××•×“×œ×™ ×©×¤×” ×›×’×•×Ÿ RAG ,Efficient Inference, ×¢×‘×•×“×” ×¢× ×§×•× ×˜×§×¡×˜×™× ××¨×•×›×™×, ×©×™××•×© ×‘×¡×•×›× ×™× ×•×¢×•×“.××™× ×˜×œ ×××©×™×›×” ×œ×”×•×‘×™×œ ××ª ×ª×—×•× ×”×‘×™× ×” ×”××œ××›×•×ª×™×ª ×¢× ×”×¤×ª×¨×•× ×•×ª ×”××ª×§×“××™× ×‘×™×•×ª×¨ ×œ×ª×¢×©×™×™×”. ××¢×‘×“×™ Xeon ××”×“×•×¨ ×”×©×™×©×™ ×•×”×××™×¦×™× ×”×™×™×¢×•×“×™×™× Gaudi 3 ×××¤×©×¨×™× ×œ××¨×’×•× ×™× ×œ×”××™×¥ ××ª ×¤×™×ª×•×— ×•×”×˜××¢×ª ×™×™×©×•××™ AI ×‘×§× ×” ××™×“×” ×’×“×•×œ, ×ª×•×š ×©××™×¨×” ×¢×œ ×™×¢×™×œ×•×ª ×’×‘×•×”×” ×•×ª××•×¨×” ×›×œ×›×œ×™×ª ×™×•×¦××ª ×“×•×¤×Ÿ. ××™× ×˜×œ ××¦×™×¢×” ×’×™×©×” ×¤×ª×•×—×” ×•×’××™×©×” ×”×××¤×©×¨×ª ×©×™×œ×•×‘ ×—×œ×§ ×©×œ ×—×•××¨×” ×•×ª×•×›× ×” ×××’×•×•×Ÿ ×¡×¤×§×™×, ×•×‘×›×š × ×•×ª× ×ª ×œ××¨×’×•× ×™× ××ª ×”×›×œ×™× ×”×“×¨×•×©×™× ×œ×”× ×œ×”××¦×ª ×”×©×™××•×© ×‘- GenAI ×•×‘××•×“×œ×™× ×’×“×•×œ×™×, ×›××• ×’× ×œ×”×§×˜× ×ª ×”×ª×œ×•×ª ×‘××¢×¨×›×•×ª ×§× ×™×™× ×™×•×ª ×©×œ ×™×¦×¨× ×™× ××—×¨×™×. ××¨×›×–×™ ×”×¤×™×ª×•×— ×©×œ ××™× ×˜×œ ×‘×™×©×¨××œ, ×©×××•×§××™× ×‘×—×™×¤×”, ×¤×ª×— ×ª×§×•×•×”, ×™×¨×•×©×œ×™× ×•×§×¨×™×™×ª ×’×ª, ××©×—×§×™× ×ª×¤×§×™×“ ××¤×ª×— ×‘×¢×™×¦×•×‘ ×”×“×•×¨ ×”×‘× ×©×œ ×˜×›× ×•×œ×•×’×™×•×ª ×¢×™×‘×•×“ ×•-AI , ×•×××©×™×›×™× ×œ×”× ×™×¢ ××ª ×”×—×“×©× ×•×ª ×”×’×œ×•×‘×œ×™×ª ×©×œ ×”×—×‘×¨×” ×‘×××¦×¢×•×ª ×©×™×œ×•×‘ ×©×œ ×‘×™×¦×•×¢×™×, ×’××™×©×•×ª ×•×—×“×©× ×•×ª ××ª××“×ª.×›××Ÿ ××¤×©×¨ ×œ×‘×—×•×¨ ×ª×—×•××™ ×¢× ×™×™×Ÿ, ×•×× ×—× ×• × ×ª××™× ×œ×š ×›×ª×‘×•×ª ×‘××•×¤×Ÿ ××™×©×™. ×”×›×ª×‘×•×ª ×™×•×¤×™×¢×• ×›××Ÿ ×•×‘×¨×—×‘×™ ×”××ª×¨, ×•×”×¡×™××•×Ÿ ×©×œ× ×• ×™×”×™×” ×”× ×” ×”×›×ª×‘×•×ª ×©×”×ª××× ×• ×œ×š ××™×©×™×ª. ×¨×•×¦×” ×œ×¨×¢× ×Ÿ ×”×¢×“×¤×•×ª? ×‘×‘×§×©×”, ×× ×—× ×• ×œ× ×©×•×¤×˜×™× ×–×” ×”××§×•× ×œ×”×›×™×¨ ××ª ×”×—×‘×¨×•×ª, ×”××©×¨×“×™× ×•×›×œ ××™ ×©×¢×•×©×” ××ª ×”×”×™×™×˜×§ ×‘×™×©×¨××œ (×•×™×© ×’× ××œ× ××©×¨×•×ª ×¤×ª×•×—×•×ª!) #×ª×•×›×Ÿ ××§×•×“××”× ×™×•×–×œ×˜×¨ ×©×œ× ×• ×¢×•×©×” ××ª ×”××§×¡×˜×¨×” ××™×™×œ ×¢× ×”×¢×“×›×•× ×™× ×•×”×—×“×©×•×ª ×©×œ ×”×©×‘×•×¢Â© ×›×œ ×”×–×›×•×™×•×ª ×©××•×¨×•×ª ×œ×’×™×§×˜×™×™××¤×™×ª×•×— ××ª×¨×™×designed by designed byÂ Â | ×¤×™×ª×•×— ××ª×¨×™××‘×’×œ×œ ×–×” ×× ×—× ×• ××§×¤×™×“×™× ×©×”×Ÿ ×œ× ×™×¦×™×§×•, ××‘×œ ×”×Ÿ ×××¤×©×¨×•×ª ×œ× ×• ×œ×ª×ª ×œ×›× ×ª×•×›×Ÿ ×‘×—×™× ×.</p> <p>×¤×¨×¡×•××•×ª ×¢×•×–×¨×•×ª ×œ× ×• ×œ×”×ª×§×™×™× ×•×œ×”×ª××§×“ ×‘××” ×©×—×©×•×‘: ×œ×™×¦×•×¨ ×¢×‘×•×¨×š ×ª×•×›×Ÿ ××§×¦×•×¢×™ ×•××¢× ×™×™×Ÿ. ×›×“×™ ×œ×”××©×™×š ×œ×™×”× ×•×ª ××’×™×§×˜×™×™×, ×›×“××™ ×œ×”×¡×™×¨ ××ª ×”×—×¡×™××” ××”××ª×¨ ×©×œ× ×•. ×× ×—× ×• ××‘×˜×™×—×™× ×œ× ×œ×”×¦×™×£.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[××¢×¨×›×•×ª RAG ×™×›×•×œ×•×ª ×œ×¤×ª×•×¨ ××ª ×ª×•×¤×¢×ª ×”×”×–×™×•×ª ×©×œ LLMs, ××‘×œ ×›×“×™ ×œ×‘× ×•×ª ××—×ª ×˜×•×‘×” ×ª×¦×˜×¨×›×• ×œ×§×‘×œ ×”×¨×‘×” ×”×—×œ×˜×•×ª ×§×¨×™×˜×™×•×ª (×•×œ×§×•×•×ª ×©×ª×’×™×¢×• ×”×›×™ ×§×¨×•×‘ ×œ××•×©×œ×)]]></summary></entry><entry><title type="html">Intel Labs Introduces RAG-FiT Open-Source Framework for Retrieval Augmented Generation in LLMs - Intel Community</title><link href="https://danielfleischer.github.io/blog/2024/intel-labs-introduces-rag-fit-open-source-framework-for-retrieval-augmented-generation-in-llms-intel-community/" rel="alternate" type="text/html" title="Intel Labs Introduces RAG-FiT Open-Source Framework for Retrieval Augmented Generation in LLMs - Intel Community"/><published>2024-10-09T00:00:00+00:00</published><updated>2024-10-09T00:00:00+00:00</updated><id>https://danielfleischer.github.io/blog/2024/intel-labs-introduces-rag-fit-open-source-framework-for-retrieval-augmented-generation-in-llms---intel-community</id><content type="html" xml:base="https://danielfleischer.github.io/blog/2024/intel-labs-introduces-rag-fit-open-source-framework-for-retrieval-augmented-generation-in-llms-intel-community/"><![CDATA[<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>                        Success!  Subscription added.
                    
                   
                
                    
                        Success!  Subscription removed.
                    
                    
                
                    
                        Sorry, you must verify to complete this action. Please click the verification link in your email. You may re-send via your
                        profile.
                    
                Scott Bair is a key voice at Intel Labs, sharing insights into innovative research for inventing tomorrowâ€™s technology.Â Intel Labs researchers Daniel Fleischer, Moshe Berchansky, and Moshe Wasserblat collaborated on RAG-FiT.HighlightsIntel Labs introduces RAG-FiT, an open-source framework for augmenting large language models (LLMs) for retrieval-augmented generation (RAG) use cases. Available under an Apache 2.0 license, RAG-FiT integrates data creation, training, inference, and evaluation into a single workflow, assisting in the creation of data-augmented datasets for training and evaluating LLMs in RAG settings. This integration enables rapid prototyping and experimentation with various RAG techniques, allowing users to easily generate datasets and train RAG models using internal or specialized knowledge sources.The library assists in creating data to train models using parameter-efficient fine-tuning (PEFT), which allows users to finetune a subset of parameters in a model. The Python-based framework is designed to serve as an end-to-end experimentation environment, enabling users to quickly prototype and experiment with different RAG techniques, including data selection, aggregation and filtering, retrieval, text processing, document ranking, few-shot generation, prompt design using templates, fine-tuning, inference, and evaluation.To demonstrate the effectiveness of the RAG-FiT framework (formerly known as RAG Foundry), Intel Labs researchers augmented and fine-tuned Llama 3.0 and Phi-3 models with diverse RAG configurations, showcasing consistent improvements across three knowledge-intensive question-answering tasks.Using RAG Systems to Address LLM LimitationsDespite their impressive capabilities, LLMs have inherent limitations. These models can produce plausible sounding but incorrect or nonsensical answers, struggle with factual accuracy, lack access to up-to-date information after their training cutoff, and struggle in attending to relevant information in large contexts.RAG enhances LLMs performance by integrating external information using retrieval mechanisms. Retrieving specific data from knowledge bases outside the model can effectively address knowledge limitations, which in turn can reduce hallucinations, improve the relevance of generated content, provide interpretability and could be vastly more cost efficient. Furthermore, recent research indicates that fine-tuning LLMs for RAG can achieve state-of-the-art performance, surpassing that of larger proprietary models.How RAG-FiT WorksAs an experimentation environment for researchers, the backbone of the RAG-FiT library consists of four distinct modules: data creation, training, inference, and evaluation. Each module is encapsulated and controlled by a configuration file, ensuring compatibility between the output of one module and the input of the next file. This modular approach allows isolation and independent experimentation on each step, enabling the production of multiple outputs and the concurrent execution of numerous experiments. Evaluation can be conducted on the generated outputs as well as on any feature within the data, including retrieval, ranking, and reasoning.Figure 1. In the RAG-FiT framework, the data augmentation module saves RAG interactions into a dedicated dataset, which is then used for training, inference, and evaluation.Dataset creation: The processing module facilitates the creation of context-enhanced datasets by persisting RAG interactions, which are essential for RAG-oriented training and inference. These interactions encompass dataset loading, column normalization, data aggregation, information retrieval, template-based prompt creation, and various other forms of pre-processing. The processed data can be saved in a consistent, model-independent format, along with all associated metadata, ensuring compatibility and reproducibility across different models and experiments.The processing module supports the handling of multiple datasets at once through global dataset sharing. This feature allows each step of the pipeline to access any of the loaded datasets, enhancing flexibility and allowing for complex processing procedures. Furthermore, the module includes step caching, which caches each pipeline step locally. This improves compute efficiency, and facilitates easy reproduction of results.Training: Users can train any model on the augmented datasets. A training module is used to fine-tune models from the datasets created by the previous processing module. The training module relies on the well-established training framework, TRL, for transformer reinforcement learning. The module also supports advanced efficient training techniques, such as PEFT and low-rank adaptation (LoRA) to customize the LLM for specific use cases without retraining the entire model.Inference: The inference module can generate predictions using the augmented datasets with trained or untrained LLMs. Inference is conceptually separated from the evaluation step, since it is more computationally demanding than evaluation. Additionally, users can run multiple evaluations on a single prepared inference results file.Evaluation: Custom metrics can be easily implemented or users can run current metrics, including Exact Match (EM), F1 Score, ROUGE, BERTScore, DeepEval, Ragas, Hugging Face Evaluate, and classification. Users can run metrics locally on each example, or globally on the entire dataset, such as recall for classification-based metrics. In addition to input and output texts, metrics can utilize any feature in the dataset, such as retrieval results, reasoning, citations, and attributions. In addition, the evaluation module uses a processing step called an Answer Processor, which can implement custom logic and perform many tasks, including cleaning and aligning outputs.Performance of RAG-FiT Augmentation TechniquesTo illustrate the utility of the framework, Intel Labs researchers conducted experiments involving retrieval, fine-tuning, chain-of-thought (CoT) reasoning, and a negative distractor documents technique. The team compared Llama 3.0 and Phi-3, two widely accepted baseline models, using enhancement methods across TriviaQA, PubMedQA, and ASQA, three knowledge-intensive question-answering datasets. The TriviaQA and PubMedQA datasets contain relevant context, while for the ASQA dataset, retrieval was done over a Wikipedia corpus using a dense retriever.The team measured and reported EM for TriviaQA, STR-EM for ASQA, and accuracy and F1 Score for PubMedQA. In addition, researchers evaluated two Ragas metrics: faithfulness (the relation between the generated text and the context) and relevancy (the generated text and the query). Overall, the two models showed consistent improvements across the three knowledge-intensive question-answering tasks.Figure 2. Evaluation results of baseline and different RAG settings for the three datasets and two models tested. In bold are the best configurations per dataset, based on the main metrics.For TriviaQA, retrieved context improved the results, fine-tuning the RAG setting boosted the results, but fine-tuning on CoT reasoning (which includes training on a combination of gold passages and distractor passages) decreased performance. For this dataset, the best method is model dependent. For ASQA, every method improved upon the baseline, CoT reasoning produced consistent improvement in both models, as well as fine-tuning of the CoT configuration, which performed best. Finally, for PubMedQA, almost all methods improved upon the baseline (with one exception), CoT reasoning improved on the untrained RAG setting, but for fine-tuning, the RAG method performed best in both models.Finally, the faithfulness and relevancy scores often did not correlate with the main metrics, or with each other, possibly indicating they capture different aspects of the retrieval and generated results, and represent a trade-off in performance.The results demonstrate the usefulness of RAG techniques for improving performance, as well as the need to carefully evaluate different aspects of a RAG system on a diverse set of datasets.
					You must be a registered user to add a comment. If you've already registered, sign in. Otherwise, register and sign in.
				Community support is provided Monday to Friday. Other contact methods are available here.Intel does not verify all solutions, including but not limited to any file transfers that may appear in this community. Accordingly, Intel disclaims all express and implied warranties, including without limitation, the implied warranties of merchantability, fitness for a particular purpose, and non-infringement, as well as any warranty arising from course of performance, course of dealing, or usage in trade.For more complete information about compiler optimizations, see our Optimization Notice.
</code></pre></div></div>]]></content><author><name></name></author><summary type="html"><![CDATA[Scott Bair is a key voice at Intel Labs , sharing insights into innovative research for inventing tomorrowâ€™s technology. Intel Labs researchers]]></summary></entry><entry><title type="html">Open Domain Q&amp;amp;A using Dense Retrievers in fastRAG | by Daniel Fleischer | Medium</title><link href="https://danielfleischer.github.io/blog/2023/open-domain-qa-using-dense-retrievers-in-fastrag-by-daniel-fleischer-medium/" rel="alternate" type="text/html" title="Open Domain Q&amp;amp;A using Dense Retrievers in fastRAG | by Daniel Fleischer | Medium"/><published>2023-08-23T00:00:00+00:00</published><updated>2023-08-23T00:00:00+00:00</updated><id>https://danielfleischer.github.io/blog/2023/open-domain-qa-using-dense-retrievers-in-fastrag--by-daniel-fleischer--medium</id><content type="html" xml:base="https://danielfleischer.github.io/blog/2023/open-domain-qa-using-dense-retrievers-in-fastrag-by-daniel-fleischer-medium/"><![CDATA[<p>Sign upSign inSign upSign inâ€“1ListenShareRetrieval augmented generation is an advanced technique in the field of natural language processing that combines the power of information retrieval and generative models. It aims to generate more informative and contextually appropriate responses to user queries by leveraging the retrieval of relevant passages or documents. This technique has significant potential in various applications, among them is Open Domain Question Answering; it is a field of research that focuses on developing systems capable of comprehending and answering a wide range of questions posed by users. It involves extracting relevant information from vast amounts of unstructured data using information retrieval techniques.Information retrieval used to rely on sparse techniques based on word statistics. In the traditional approach, documents were represented by a bag-of-words model, where the presence or absence of specific words determined the relevance of a document to a query. Score functions, like BM25 and TF-IDF, use word frequencies to score documents, balancing how frequently a keyword appears in a document versus how prevalent the word is in general. One popular DB is the Elasticsearch DB which uses the Lucene text search engine; it is based on word statistics and edit distance (a syntax method).With advancements in natural language processing and machine learning, information retrieval has shifted towards denser representations using embeddings. Embeddings capture the semantic meaning of words and phrases, allowing for a more nuanced understanding of the content; this enables more accurate matching of queries with relevant documents, as embeddings can capture subtle semantic similarities that traditional word statistics fail to capture. The shift from sparse to dense representations has significantly improved the performance and precision of retrieval systems. For an introduction to neural IR, see (Mitra and Craswell 2018); for a review of IR for Q&amp;A, see (Abbasiantaeb and Momtazi 2021).This blog serves as an introduction to dense retrieval and we focus on two dense document retrieval models: Dense Passage Retrieval (DPR, Karpukhin et al. 2020) and Contextualized Late Interaction over BERT (ColBERT, Khattab and Zaharia 2020). Both models use Semantic Search to find the relevant documents. Semantic search means we use a textâ€™s dense representation to measure similarity between a given query and the potential relevant documents. The two models use different methods of storing the documentsâ€™ vectors and measuring similarity between queries and documents. We will compare the models by measuring accuracy and latency on a known benchmark, called Natural Questions (NQ, Kwiatkowski et al. 2019), a collection of user submitted questions where answers can be found in Wikipedia articles.We would like to introduce fastRAG, a framework developed at Intel Labs and released as an open-source software recently. The goal of the framework is to enable rapid research and development of retrieval-augmented generative AI applications. These can be used for generative tasks such as question answering, summarization, dialogue systems, and content creation, while utilizing information-retrieval components to anchor LLM output using external knowledge.An application is represented by a pipeline, typically comprised of a knowledge-base (KB), retriever, ranker and a reader, typically an LLM, which â€œreadsâ€ the query and retrieved documents, and generates an output. One can experiment with different architectures, models, benchmarking the results for performance and latency. Several of the models we offer are better suited for Intel hardware, achieving lower latency with comparable accuracy; on that in the next blog post.In the field of information retrieval, relatively recent updates promote the use of transformer encoder models as retrievals: documents in the knowledge-base are encoded as vectors and stored in an index. At runtime, the query is encoded as a vector and vector similarity search is used to find the most relevant documents. Similar process is used in re-ranking retrieved documents, where the encoding is done on-the-fly, specifically for the retrieved documents.Among the dense retrievals there are several approaches. One approach is to use a single tokenâ€™s embeddings as a representative of the entire document. DPR (Karpukhin et al. 2020) is an example of that approach, where the encoders are trained to â€œsummarizeâ€ the entire document in the first tokenâ€™s embeddings. The method is a form of a bi-encoder, since it uses two encoders, one for the query and another for the documents; see illustration.Another approach is called Late Interaction, as defined first in ColBERT (Khattab and Zaharia 2020). The idea is to save (and index) the encoded vectors for all the words in the documents. At run-time the query vectors are compared with all the documents wordsâ€™ vectors (hence the â€œlateâ€ in late interaction) thus retrieving more relevant documents than DPR. Notice that indexing every token, instead of just the first token for each document, can increase the index size.Later refinements to this work, namely ColBERT v2 and PLAID (Santhanam, Khattab, Saad-Falcon, et al. 2022; Santhanam, Khattab, Potts, et al. 2022) helped reduce the index size and latency time thanks to two main improvements: first is quantization and compression of the vectors in the index. Secondly is a set of heuristics that cluster the vectors using the K-means algorithm, hierarchically choose the relevant documentsâ€™ tokens for the query tokens based on the clustersâ€™ centroids. ColBERT v2 with PLAID index achieves state of the art retrieval performance with a low latency, close to the order of sparse retrieval (BM25, Lucene, Elasticsearch, etc.) but with much higher accuracy.The first step is creating a documents store of the type PLAIDDocumentStore. The store requires three paths: checkpoint, collection and an index.A ColBERT checkpoint is an encoder model, fine tuned for the task of retrieving. Itâ€™s based on a BERT architecture. One can download a trained checkpoint, for example here, trained by the paper authors. Encoders can be fine-tuned using these instructions: training. Next is the collection of documents which comprise the corpus. The collection should be a signle tsv file with columns: id, text, title (optional). Finally, the index is the vectors index created using the same checkpoint, encoding all tokens in the corpus, compressing and saving the result.We provide a script to create a PLAID vector index using a ColBERT encoder and a documents collection in here.Once we have all the ingredients, we initialize the document store:Next we define a retriever using the document store we just define:We define a pipeline; it has the following form:We can use the Haystack pipeline API to connect with external components. In this example the pipeline contains just the retriever:Running the queries through the pipeline is very easy:The results is a hash map with documents key containing a list of results: documents with relevancy scores.To test ColBERT, we will use the Natural Questions benchmark (Kwiatkowski et al. 2019). The external knowledge is a collection of Wikipedia passages.As a baseline, weâ€™ll use the original implementation of DPR, together with a checkpoint that was fine-tuned on Natural Questions, see download instructions. DPR model is released under the CC-BY-NC 4.0 license.DPR uses the Faiss vector search library (Johnson, Douze, and JÃ©gou 2019). We test two configurations for storing the vectors: flat and HNSW. Flat is slow but accurate, since an exhaustive similarity search is done. HNSW (Malkov and Yashunin 2018) is an approximate vector search method; the vectors are organized into a graph to enable faster than linear search. Building an optimal HNSW graph requires some parameter tuning; these control the trade-off between speed, accuracy and index size.For ColBERT, we use the ColBERTv2 checkpoint from here, which was fine-tuned on the MS MARCO (Bajaj et al. 2018) dataset, which comprised of Bing questions and answers based on web search results.We report recall and MRR values for k values of 5, 10, 20, 50, and 100. We also measure latency (at k=100) (ms/query), and report the vector index size in GBs, as there is a trade-off between performance and accuracy.Measurements done on an Intel AWS instance, with a Xeon processor. AWS instance type is r6i.16xlarge; 32 cores, 512GB RAM, Intel(R) Xeon(R) Platinum 8375C CPU @ 2.90GHz. Conda 23.1.0, python 3.9.16, AMI image ami-0f1a5f5ada0e7da53, Amazon Linux 2, version 5.10.177-158.645.amzn2.x86_64. The Wikipedia text collection size is 13GB.First, we note there is a difference in accuracy between the ColBERT model and DPR. The quality of the embeddings generated from a trained encoder is crucial for high quality retrieval. As the DPR encoders were fine-tuned on the Natural Questions dataset, this is probably one of the reasons explaining the difference.Next, we compare the two indexing methods for DPR: flat and HNSW. Flat index query takes 35x longer than HNSW, at almost 1.5 seconds per query. HNSW is faster, with only a small accuracy penalty; however, index size is bigger, at ~2.3x the size of the flat index.It is notable that although ColBERT encodes and stores all the documents tokens, thanks to its optimizations, the index size is comparable to a flat Faiss index, storing only the first tokenâ€™s embedding for each document.One of the goals was to present the clear trade off between accuracy and performance, more specifically, between recall, latency and memory usage (the index size, as these are stored in-memory). To summarize, we introduced two dense retrieval algorithms, ColBERT with a PLAID index and DPR. We tested these on the open-domain Q&amp;A dataset Natural Questions, measuring accuracy and latency.Experience the capabilities of ColBERT in fastRAG through the following Notebook example. Familiarize yourself with fastRAG by exploring our user-friendly UI demos at Running Demos in fastRAG. Start using the ColBERT encoder, accessible from the HuggingFace hub. Easily create a document index, as detailed in our guide at Indexing in fastRAG. Furthermore, we offer full support for the DPR retriever; see example DPR configuration. Unleash the potential of fastRAG and revolutionize your workflow today!Tests done by Intel on March 14th, 2023.Performance varies by use, configuration and other factors. Learn more at www.Intel.com/PerformanceIndex.Â© Intel Corporation. Intel, the Intel logo, and other Intel marks are trademarks of Intel Corporation or its subsidiaries. Other names and brands may be claimed as the property of others.Abbasiantaeb, Zahra, and Saeedeh Momtazi. 2021. â€œText-Based Question Answering from Information Retrieval and Deep Neural Network Perspectives: A Survey.â€ Wires Data Mining and Knowledge Discovery 11 (6): e1412. https://doi.org/10.1002/widm.1412.Bajaj, Payal, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, et al. 2018. â€œMS MARCO: A Human Generated MAchine Reading COmprehension Dataset.â€ October 31, 2018. https://doi.org/10.48550/arXiv.1611.09268.Johnson, Jeff, Matthijs Douze, and HervÃ© JÃ©gou. 2019. â€œBillion-Scale Similarity Search with GPUs.â€ Ieee Transactions on Big Data 7 (3): 535â€“47. https://doi.org/10.1109/TBDATA.2019.2921572.Karpukhin, Vladimir, Barlas OÄŸuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. â€œDense Passage Retrieval for Open-Domain Question Answering.â€ September 30, 2020. https://doi.org/10.48550/arXiv.2004.04906.Khattab, Omar, and Matei Zaharia. 2020. â€œColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT.â€ June 4, 2020. https://doi.org/10.48550/arXiv.2004.12832.Kwiatkowski, Tom, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, et al. 2019. â€œNatural Questions: A Benchmark for Question Answering Research.â€ Transactions of the Association for Computational Linguistics 7 (August): 453â€“66. https://doi.org/10.1162/tacl_a_00276.Malkov, Yu A., and D. A. Yashunin. 2018. â€œEfficient and Robust Approximate Nearest Neighbor Search Using Hierarchical Navigable Small World Graphs.â€ August 14, 2018. https://doi.org/10.48550/arXiv.1603.09320.Mitra, Bhaskar, and Nick Craswell. 2018. â€œAn Introduction to Neural Information Retrieval.â€ Foundations and TrendsÂ® in Information Retrieval 13 (1): 1â€“126. https://doi.org/10.1561/1500000061.Santhanam, Keshav, Omar Khattab, Christopher Potts, and Matei Zaharia. 2022. â€œPLAID: An Efficient Engine for Late Interaction Retrieval.â€ May 19, 2022. https://doi.org/10.48550/arXiv.2205.09707.Santhanam, Keshav, Omar Khattab, Jon Saad-Falcon, Christopher Potts, and Matei Zaharia. 2022. â€œColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction.â€ July 10, 2022. https://doi.org/10.48550/arXiv.2112.01488.â€”-1Research scientist at Intel LabsHelpStatusAboutCareersPressBlogPrivacyRulesTermsText to speech</p>]]></content><author><name></name></author><summary type="html"><![CDATA[We introduce 2 dense retrieval algorithms, ColBERT with a PLAID index and DPR. We tested these on the open-domain Q&A dataset Natural Questions, measuring accuracy and latency.]]></summary></entry></feed>