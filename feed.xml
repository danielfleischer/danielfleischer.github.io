<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://danielfleischer.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://danielfleischer.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-08-27T16:35:02+00:00</updated><id>https://danielfleischer.github.io/feed.xml</id><title type="html">blank</title><subtitle>Personal website for Daniel Fleischer. </subtitle><entry><title type="html">Built an MCP server for LLMs to search email from terminal | Daniel Fleischer posted on the topic | LinkedIn</title><link href="https://danielfleischer.github.io/blog/2025/built-an-mcp-server-for-llms-to-search-email-from-terminal-daniel-fleischer-posted-on-the-topic-linkedin/" rel="alternate" type="text/html" title="Built an MCP server for LLMs to search email from terminal | Daniel Fleischer posted on the topic | LinkedIn"/><published>2025-06-23T00:00:00+00:00</published><updated>2025-06-23T00:00:00+00:00</updated><id>https://danielfleischer.github.io/blog/2025/built-an-mcp-server-for-llms-to-search-email-from-terminal--daniel-fleischer-posted-on-the-topic--linkedin</id><content type="html" xml:base="https://danielfleischer.github.io/blog/2025/built-an-mcp-server-for-llms-to-search-email-from-terminal-daniel-fleischer-posted-on-the-topic-linkedin/"><![CDATA[<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>          Agree &amp; Join LinkedIn
        
  By clicking Continue to join or sign in, you agree to LinkedInâ€™s User Agreement, Privacy Policy, and Cookie Policy.

            AI Research Engineer @ Intel Labs
        ğŸ“¬ I built an MCP server that lets LLMs search my email from the terminal
</code></pre></div></div> <p>The server connects Claude to email search via the mu CLI tool. Now I just ask it things like: â€œFind emails with PDF attachments from last Aprilâ€ âš¡</p> <p>ğŸ›  No custom frontend. No heavy framework. Just a CLI tool made smarter.</p> <p>ğŸ’¡ I learned that MCP servers are basically API translators â€” they take complex developer SDKs and flatten them into simple function calls that LLMs can actually use.</p> <p>ğŸ¯ The bigger picture: This pattern can breathe new life into existing CLI tools and services. Complex APIs â†’ Simple, declarative functions â†’ Natural language queries.</p> <p>This isnâ€™t a product â€” just an experiment in stitching new capabilities into existing workflows. Code here: https://lnkd.in/eT2fJBSv</p> <p>mu email indexer and searcher: https://github.com/djcb/mu</p> <p>#MCP #LLM #EmailSearch #OpenSource #AI</p> <p>What existing tools would you want to make LLM-friendly? ğŸ¤” To view or add a comment, sign in</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>        1,428 followers
      
                Create your free account or sign in to continue your search
              
          or
        
  By clicking Continue to join or sign in, you agree to LinkedInâ€™s User Agreement, Privacy Policy, and Cookie Policy.

            New to LinkedIn? Join now
          
                      or
                    
                New to LinkedIn? Join now
              
  By clicking Continue to join or sign in, you agree to LinkedInâ€™s User Agreement, Privacy Policy, and Cookie Policy.
</code></pre></div></div>]]></content><author><name></name></author><summary type="html"><![CDATA[ğŸ“¬ I built an MCP server that lets LLMs search my email from the terminal The server connects Claude to email search via the mu CLI tool. Now I just ask it things like: "Find emails with PDF attachments from last April" âš¡ ğŸ›  No custom frontend. No heavy framework. Just a CLI tool made smarter. ğŸ’¡ I learned that MCP servers are basically API translators â€” they take complex developer SDKs and flatten them into simple function calls that LLMs can actually use. ğŸ¯ The bigger picture: This pattern can breathe new life into existing CLI tools and services. Complex APIs â†’ Simple, declarative functions â†’ Natural language queries. This isnâ€™t a product â€” just an experiment in stitching new capabilities into existing workflows. Code here: https://lnkd.in/eT2fJBSv mu email indexer and searcher: https://github.com/djcb/mu #MCP #LLM #EmailSearch #OpenSource #AI What existing tools would you want to make LLM-friendly? ğŸ¤”]]></summary></entry><entry><title type="html">Summarize Hacker News Posts with Haystack &amp;amp; OPEA | Haystack</title><link href="https://danielfleischer.github.io/blog/2025/summarize-hacker-news-posts-with-haystack-opea-haystack/" rel="alternate" type="text/html" title="Summarize Hacker News Posts with Haystack &amp;amp; OPEA | Haystack"/><published>2025-06-10T00:00:00+00:00</published><updated>2025-06-10T00:00:00+00:00</updated><id>https://danielfleischer.github.io/blog/2025/summarize-hacker-news-posts-with-haystack--opea--haystack</id><content type="html" xml:base="https://danielfleischer.github.io/blog/2025/summarize-hacker-news-posts-with-haystack-opea-haystack/"><![CDATA[<p>Build a RAG pipeline to fetch live Hacker News posts and summarize them with a local LLM endpointWelcome to this step-by-step tutorial where weâ€™ll build a simple Retrieval-Augmented Generation (RAG) pipeline using Haystack and OPEA. Weâ€™ll fetch the newest Hacker News posts, feed them to a lightweight LLM endpoint (OPEAGenerator), and generate concise one-sentence summaries (based on this notebook). Letâ€™s dive in! ğŸ‰In modern GenAI applications, having a flexible, performant, and scalable platform is essential. OPEA (Open Platform for Enterprise AI) is an open, model-agnostic framework for building and operating composable GenAI solutions. It provides:In this demo, weâ€™ll use an OPEA LLM endpoint in a Haystack pipeline, giving you:In this tutorial, weâ€™ll build a simple RAG pipeline that fetches the newest Hacker News posts, sends them to a local OPEA endpoint running a Qwen/Qwen2.5-7B-Instruct demo model, and produces concise one-sentence summaries. Of course, you can replace our example model with any other OPEA-served model, making this pattern both lightweight for prototyping and powerful for real-world deployments. Letâ€™s get started! ğŸš€Make sure you have:NOTE: As a reference, here is a Docker Compose recipe to get you started. OPEA LLM service can be configured to use a variety of model serving backends like TGI, vLLM, ollama, OVMSâ€¦ and offers validated runtime settings for good performance on various hardwareâ€™s including Intel Gaudi. In this example, it creates an OPEA LLM service with a TGI backend. See the documentation for LLM Generation. The code is based on OPEA LLM example and OPEA TGI example.To run, call LLM_MODEL_ID=Qwen/Qwen2.5-7B-Instruct docker compose up.Weâ€™ll create a custom Haystack component, HackernewsNewestFetcher, that:We use the OPEAGenerator to call our LLM over HTTP. Here, we point to a local endpoint serving the Qwen/Qwen2.5-7B-Instruct model:Using PromptBuilder, we define a Jinja-style template that:We wire up the components in a Pipeline:Fetch and summarize the top 2 newest Hacker News posts:Beautiful, concise summaries in seconds! âœ¨In this tutorial, we built a full RAG pipeline:Feel free to extend this setup with more advanced retrieval, caching, or different LLM backends. Happy coding! ğŸ› ï¸ğŸ”¥ Building products, technology and solutions for LLM-enabled applications.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Build a RAG pipeline to fetch live Hacker News posts and summarize them with a local LLM endpoint]]></summary></entry><entry><title type="html">×”××•×“×œ ×©×œ×›× ×”×•×–×”? ×›×š ×ª×‘× ×• ×‘×¢×¦××›× ××¢×¨×›×ª RAG | ×’×™×§×˜×™×™×</title><link href="https://danielfleischer.github.io/blog/2024/rag/" rel="alternate" type="text/html" title="×”××•×“×œ ×©×œ×›× ×”×•×–×”? ×›×š ×ª×‘× ×• ×‘×¢×¦××›× ××¢×¨×›×ª RAG | ×’×™×§×˜×™×™×"/><published>2024-12-01T00:00:00+00:00</published><updated>2024-12-01T00:00:00+00:00</updated><id>https://danielfleischer.github.io/blog/2024/-------rag--</id><content type="html" xml:base="https://danielfleischer.github.io/blog/2024/rag/"><![CDATA[<p>×—×™×¤×•×©×™× ×—××™×: ××¢×¨×›×•×ª RAG ×™×›×•×œ×•×ª ×œ×¤×ª×•×¨ ××ª ×ª×•×¤×¢×ª ×”×”×–×™×•×ª ×©×œ LLMs, ××‘×œ ×›×“×™ ×œ×‘× ×•×ª ××—×ª ×˜×•×‘×” ×ª×¦×˜×¨×›×• ×œ×§×‘×œ ×”×¨×‘×” ×”×—×œ×˜×•×ª ×§×¨×™×˜×™×•×ª (×•×œ×§×•×•×ª ×©×ª×’×™×¢×• ×”×›×™ ×§×¨×•×‘ ×œ××•×©×œ×) ×”××ª×’×¨ ×”×¢×™×§×¨×™ ×‘×‘× ×™×™×ª ××¢×¨×›×ª RAG ×”×•× ×œ×”×’×™×¢ ×œ×¨××ª ××™×›×•×ª ×•×“×™×•×§ ××§×¡×™××œ×™×™× (×¦×™×œ×•×: Dreamstime)×××ª ×“× ×™××œ ×¤×œ×™×™×©×¨, ×—×•×§×¨ AI ×‘-Intel Labs××•×“×œ×™ ×©×¤×” ×’×“×•×œ×™× (LLMs) ××¦×™×¢×™× ×¤×ª×¨×•× ×•×ª ××ª×§×“××™× ×œ××ª×’×¨×™ ×¤×™×ª×•×—, ××š ×œ×¢×ª×™×, ××” ×œ×¢×©×•×ª, ×”× ×©×•×’×™×. ×”××˜×¨×” ×©×œ ××¢×¨×›×•×ª RAG ×”×™× ×œ×¤×ª×•×¨ ××ª ×”×‘×¢×™×” ×”×–××ª, ×‘×××¦×¢×•×ª ×’×™×©×” ×—×“×©×” ×œ×©×™×¤×•×¨ ×“×™×•×§ ×•×××™× ×•×ª ×”××•×“×œ×™×. ××™×š ×”×Ÿ ×¢×•×©×•×ª ××ª ×–×”, ×”×× ×”×”×™×™×¤ ××•×¦×“×§ ×•×›×™×¦×“ × ×•×›×œ ×œ×‘× ×•×ª ××¢×¨×›×ª RAG ××©×œ× ×•? ×›×“×™ ×œ×¢× ×•×ª ×¢×œ ×”×©××œ×•×ª ×”××œ×”, ×‘×“×§×ª×™ ××ª ×”××ª×’×¨×™× ×‘×’×™×©×” ×–×• ××œ ××•×œ ×”×™×ª×¨×•× ×•×ª ×©×”×™× ××¦×™×¢×” ×‘×©×“×¨×•×’ ×”××™× ×˜×¨××§×¦×™×” ×©×œ× ×• ×¢× AI.×œ×¤× ×™ ×©× ×¦×œ×•×œ, ×‘×•××• × ×“×‘×¨ ×¨×’×¢ ×¢×œ LLMs. ×œ××¢×©×”, ××“×•×‘×¨ ×‘×™×™×¦×•×’ ××ª××˜×™ ×©×œ ×©×¤×” ×˜×‘×¢×™×ª ×‘×××¦×¢×•×ª ×”×¡×ª×‘×¨×•×™×•×ª: ××•×“×œ ×”×©×¤×” ×™×›×•×œ ×œ×—×©×‘ ×”×¡×ª×‘×¨×•×ª ×œ×›×œ ××©×¤×˜ ×‘×©×¤×” ×©×‘×” ××™×× ×• ××•×ª×•, ×•×›×š ×”×•× ×™×›×•×œ ×œ×™×™×¦×¨ ××©×¤×˜×™× ×—×“×©×™×, ××™×œ×” ××—×¨ ××™×œ×”. ××•×“×œ ×”×©×¤×” ×œ××¢×©×” â€œ×××©×™×š ××ª ×”××©×¤×˜â€: ××–×™× ×™× ×”×ª×—×œ×” ×©×œ ××©×¤×˜, ×•×”×•× ×××©×™×š. ×”×©×™××•×©×™× ×”× ×›××¢×˜ ××™× ×¡×•×¤×™×™×: ××¤×©×¨ ×œ×‘×§×© ××”××•×“×œ ×œ×¢× ×•×ª ×¢×œ ×©××œ×•×ª, ×œ×¡×›× ×˜×§×¡×˜×™×, ×œ×—×‘×¨ ×©×™×¨×™×, ×œ×›×ª×•×‘ ×¤×•× ×§×¦×™×•×ª ×‘×©×¤×•×ª ×ª×›× ×•×ª ×©×•× ×•×ª ×•××¤×™×œ×• ×œ×™×™×¦×¨ ×ª××•× ×•×ª ×‘×¦×•×¨×” ×¡×™× ×ª×˜×™×ª.××‘×œ ×œ× ×”×›×œ ××•×©×œ×, ×•××—×“ ×”×¤×’××™× ×‘×ª×¤×§×•×“ ×©×œ LLMs ×”×•× ×”×”×–×™×•×ª ×”××•×›×¨×•×ª ×œ×›×•×œ× ×• â€“ ××•×ª× ××§×¨×™× ×©×‘×”× ××•×“×œ×™ ×©×¤×” ××¡×¤×§×™× ×ª×©×•×‘×•×ª ×©×’×•×™×•×ª ×¢×•×‘×“×ª×™×ª, ×œ× ×¨×œ×•×•× ×˜×™×•×ª ××• ××•×¤×¨×›×•×ª ×œ×—×œ×•×˜×™×Ÿ. ×›×“×™ ×œ×”×•×¡×™×£ ×—×˜× ×¢×œ ×¤×©×¢, ××•×“×œ×™ ×”×©×¤×” ×œ× ×ª××™×“ ××¡×™×™×’×™× ××ª ×ª×©×•×‘×•×ª×™×”× ×•×¢×•× ×™× ×‘×‘×™×˜×—×•×Ÿ ××œ×, ×•×œ×›×Ÿ ×”×”×–×™×•×ª ×”×Ÿ ××¡×•×›× ×•×ª ×•×§×©×•×ª ×œ×’×™×œ×•×™.××—×ª ×”×“×¨×›×™× ×œ×”×ª××•×“×“ ×¢× ×ª×•×¤×¢×ª ×”×”×–×™×•×ª ×”×™× RAG: Retrieval Augmented Generation (××• ×‘×¢×‘×¨×™×ª, â€œ×›×ª×™×‘×ª ×˜×§×¡×˜ ×‘×¢×–×¨×ª ×©×œ×™×¤×” ×•×”×¢×©×¨×” ×‘×™×“×¢â€). ×‘×©×™×˜×” ×–×•, ××•×“×œ ×”×©×¤×” × ×¢×–×¨ ×‘×™×“×¢ ×—×™×¦×•× ×™ ×©××•×ª×• ×”×•× ×©×•×œ×£ ××• ××§×‘×œ ×œ×¦×•×¨×š ×”×©×œ××ª ×”××©×™××”, ×›×“×™ ×©×”×ª×©×•×‘×•×ª ×™×”×™×• ××“×•×™×§×•×ª ×™×•×ª×¨. RAG ×”×•× ×©× ×›×•×œ×œ ×œ×›×œ ×”×˜×›× ×™×§×•×ª ×œ×‘× ×™×™×ª ××¢×¨×›×•×ª ×©×‘×”×Ÿ ××—×‘×¨×™× ××•×“×œ×™ ×©×¤×” ×œ×™×“×¢ ×—×™×¦×•× ×™ ×œ×¦×•×¨×š ×©×™×¤×•×¨ ×”×ª×•×¦××•×ª.××¢×¨×›×ª RAG ×›×•×œ×œ×ª ×××’×¨ ×™×“×¢ ×›×’×•×Ÿ ×‘×¡×™×¡ × ×ª×•× ×™×, ××•×¡×£ ×©×œ ××¡××›×™× ×•××¤×™×œ×• ×›×œ×™ ×—×™×¤×•×© ×‘×’×•×’×œ. ×›×©×× ×—× ×• ××–×™× ×™× ×‘×§×©×” ×œ××¢×¨×›×ª, ×”×™× ×××ª×¨×ª ×¤×™×¡×•×ª ××™×“×¢ ×¨×œ×•×•× ×˜×™×•×ª ×‘×ª×•×š ×××’×¨×™ ×”×™×“×¢. ××œ×” ×¢×•×‘×¨×™× ×™×—×“ ×¢× ×”×‘×§×©×” ×”××§×•×¨×™×ª ××œ ××•×“×œ ×”×©×¤×”, ×•×”×•× ×¢×•× ×” ×¢×œ ×”×‘×§×©×” â€“ ×‘×ª×§×•×•×” ×©×”×ª×©×•×‘×” ×ª×”×™×” ××œ××”, ××“×•×™×§×ª ×•×¨×œ×•×•× ×˜×™×ª, ×‘×–×›×•×ª ×”××™×“×¢ ×©×”××•×“×œ ×§×™×‘×œ. ××—×–×•×¨ ××™×“×¢ (retrieval) ×¨×œ×•×•× ×˜×™ ×•×”×¢×‘×¨×ª×• ×œ××•×“×œ ×”×©×¤×” ××©×¤×¨×™× ×‘××•×¤×Ÿ ××•×›×— ×•××©××¢×•×ª×™ ××ª ×”×‘×™×¦×•×¢×™× ×©×œ ××•×“×œ×™ ×©×¤×” ×‘××’×•×•×Ÿ ××©×™××•×ª ×”××¦×¨×™×›×•×ª ×™×“×¢ â€“ ×›×•×œ×œ ××¢× ×” ×¢×œ ×©××œ×•×ª (Q&amp;A), ××™×•×Ÿ (Classification), ×¡×™×›×•× ×•×¢×•×“.×œ××¢×¨×›×•×ª RAG ×™×©× × ×©×™××•×©×™× ×©×•× ×™×, ×•×”×™×ª×¨×•×Ÿ ×”××•×‘×”×§ ×©×œ×”×Ÿ ×”×•× ×”×™×›×•×œ×ª ×œ×¨×ª×•× ×××’×¨×™ ×™×“×¢ ×‘××˜×¨×” ×œ×”×©×œ×™× ××ª ×”××©×™××”. ×××’×¨×™× ××œ×” ×”× ×”×“×¨×š ×”×¢×™×§×¨×™×ª ×œ×”×•×¡×™×£ ×™×“×¢ ×—×“×© ×•×œ×¢×“×›×Ÿ ×™×“×¢ ×§×™×™× ×‘××•×“×œ×™ ×”×©×¤×”, ×•×œ×›×Ÿ ×”× ××•×¦×œ×—×™× ×›×œ ×›×š. ×‘× ×•×¡×£, ×‘×—×™×¨×” ×“×™× ××™×ª ×©×œ ×××’×¨×™ ×”×™×“×¢ ×¤×•×ª×—×ª ××ª ×”×“×œ×ª ×œ××•×“×œ×™ ×©×¤×” ××•×ª×××™× ××™×©×™×ª.×§×—×• ×œ×“×•×’××” ×¢×•×–×¨ ××™×©×™ (digital assistant). ×‘××§×¨×” ×”×–×” ××¢×¨×›×ª ×”-RAG ××—×•×‘×¨×ª ×œ×××’×¨×™ ×™×“×¢ ××§×¦×•×¢×™×™×. ×‘×–×›×•×ª ×”×™×›×•×œ×•×ª ×©×œ ××•×“×œ ×”×©×¤×”, ×”××¢×¨×›×ª ×™×›×•×œ×” ×œ×©×•×—×— ×‘×©×¤×” ×˜×‘×¢×™×ª ×¢×œ ××’×•×•×Ÿ × ×•×©××™× ×˜×›× ×™×™×, ×œ×™×™×¢×¥ ×•×œ×¢× ×•×ª ×¢×œ ×©××œ×•×ª. ×‘× ×•×¡×£, ×”××¢×¨×›×ª ×™×›×•×œ×” ×œ×”×¤× ×•×ª ××ª ×”××©×ª××© ×œ××§×•×¨×•×ª ×”×™×“×¢ ×¢×¦×× (×‘×××¦×¢×•×ª ×¦×™×˜×•×˜, citation), ×›××¢×™×Ÿ ×¢×•×–×¨ ××—×§×¨.×“×•×’××” × ×•×¡×¤×ª ×”×™× ×©×™××•×© ×‘×™×“×¢ ×©×œ ×”××©×ª××© ×¢×¦××•, ×›×œ×•××¨ â€“ ×××’×¨ ×”×™×“×¢ ××‘×•×¡×¡ ×¢×œ ×”×“××˜×” ×©×œ ×”××©×ª××©, ×•×›×š ××¢×¨×›×ª ×”-RAG ××ª××™××” ×œ×• ××ª ×ª×©×•×‘×•×ª×™×”. ×”××©×ª××© ×™×›×•×œ ×œ×”×¢×œ×•×ª ××•×¡×£ ×§×‘×¦×™× ××™×©×™×™× ×œ××¢×¨×›×ª ×•×œ×‘×§×© ××× ×” ×œ×¡×›× × ×•×©××™× ×”××•×¤×™×¢×™× ×‘××¡××›×™×, ×œ×©××•×œ ×”×™×›×Ÿ × ××¦× ×“×™×•×Ÿ ×‘× ×•×©× ×›×–×” ××• ××—×¨, ×œ×‘×§×© ×©×ª×¨×›×™×‘ ××¦×’×ª ×¡×™×›×•× ××”××¡××›×™× ×•×›×Ÿ ×”×œ××”.×›××©×¨ × ×™×’×©×™× ×œ×‘× ×•×ª ××¢×¨×›×ª RAG ×¦×¨×™×š ×œ×§×‘×œ ×”×—×œ×˜×•×ª ×¨×‘×•×ª. ×‘×©×œ×‘ ×”×¨××©×•×Ÿ ×¢×œ×™× ×• ×œ×‘×—×•×¨ ××•×“×œ ×©×¤×” â€“ ×¤×ª×•×— ××• ×¡×’×•×¨. ××•×“×œ ×¡×’×•×¨ ×”×•× ××•×“×œ ××¡×—×¨×™ ××™×ª×• ×¢×•×‘×“×™× ×‘×¢×–×¨×ª ×××©×§ API, ×•××™×Ÿ ×œ× ×• ×’×™×©×” ×œ××•×¤×Ÿ ×¤×¢×•×œ×ª ×”××•×“×œ ××• ×”××©×§×•×œ×•×ª ×©×œ×•. ×œ×¢×•××ª×•, ×‘××•×“×œ ×¤×ª×•×— ×’× ×”×§×•×“ ×•×’× ×”××©×§×•×œ×•×ª × ×’×™×©×™× ×œ×‘×—×™× ×”, ××™××•×Ÿ ×•×”×’×©×” (inference). ××ª ×”××•×“×œ ×”×¤×ª×•×— × ×™×ª×Ÿ ×œ×”×ª××™× ×œ×¢×•×œ× ×”×ª×•×›×Ÿ ×©×œ× ×• ×‘×¢×–×¨×ª ××™××•×Ÿ fine tuning. ×‘×—×œ×§ ××”××§×¨×™× ×”×’×©×” ×¢×¦××™×ª ×©×œ ××•×“×œ×™× ×¤×ª×•×—×™× ×¢×©×•×™×” ×œ×”×™×•×ª ×¢×“×™×¤×” ×¢×œ ×¤× ×™ ××•×“×œ×™× ×¡×’×•×¨×™×, ××š ×”×™× ××¦×¨×™×›×” ××™×•×× ×•×ª.×”×—×œ×§ ×”×©× ×™ ×‘×‘× ×™×™×ª ××¢×¨×›×ª RAG ×”×•× ×—×™×‘×•×¨ ×œ×××’×¨×™ ×™×“×¢. ×œ×—×™×‘×•×¨ ×”×–×” ×™×©× × ×”×™×‘×˜×™× ×¨×‘×™×, ×”×›×•×œ×œ×™× ××ª ××•×¤×Ÿ ××™× ×“×•×§×¡ ×”×™×“×¢, ×¢×™×‘×•×“ ××§×“×™× ×©×œ ×”×˜×§×¡×˜ ×•×”×ª××•× ×•×ª, ×—×œ×•×§×” ×œ×¤×¡×§××•×ª ××• ××©×¤×˜×™×, × ×™×§×•×™, ×¢×™×‘×•×“ ×©×œ ×“××˜×” ×˜×‘×œ××™, ×”××¨×ª ×”×“××˜×” ×œ×™×™×¦×•×’ ×•×§×˜×•×¨×™ (vector embedding) ×©×™×›×•×œ ×œ×©×¤×¨ ××ª ××™×›×•×ª ×”×—×™×¤×•×©, ×—×™×¤×•×© ××‘×•×¡×¡ ××™×œ×™×, ×—×™×¤×•×© ×¡×× ×˜×™ ××• ×©×™×œ×•×‘ ×©×œ×”×, ××¡×¤×¨ ×”×“×•×’×××•×ª ×œ××—×–×•×¨, ××™×•×Ÿ ×¨×œ×•×•× ×˜×™×•×ª, ×¡×™× ×•×Ÿ, ×©×™×›×ª×•×‘ ×”×“×•×’×××•×ª, ×¡×™×›×•× ×•×¢×•×“. ×–×” ×××© ×¢×œ ×§×¦×” ×”××–×œ×’, ×•×¨×§ ×¢×œ ×”×©×œ×‘ ×”×–×” ×™×›×•×œ×ª×™ ×œ×›×ª×•×‘ ××××¨ ×©×œ×. ×”×—×œ×§ ×”×–×” ×§×¨×™×˜×™, ×›×™ ×›××• ×©××•××¨×™× â€“ garbage in, garbage out: ×× ××—×–×•×¨ ×”×™×“×¢ ×™×”×™×” ×œ× ××“×•×™×§ ×•×œ× ×¨×œ×•×•× ×˜×™ ××¡×¤×™×§, ××™×›×•×ª ×”×ª×©×•×‘×•×ª ×©×œ ××•×“×œ ×”×©×¤×” ×ª×¤×’×¢× ×” ××™×“, ×•×”×¡×™×›×•×™ ×œ×”×–×™×•×ª ×™×’×“×œ.×œ××—×¨ ××›×Ÿ × ×™×ª×Ÿ ×œ×”×•×¡×™×£ ××¨×›×™×‘×™× ×œ××¢×¨×›×ª: ××•×“×œ×™ ×©×¤×” × ×•×¡×¤×™× ×©× ×‘×—×¨×™× ×‘×¦×•×¨×” ×“×™× ××™×ª ×¢×œ ×¤×™ ××•×¤×™ ×”××©×™××”; ×©×™××•×© ×‘×›××” ×××’×¨×™ ×™×“×¢ ×‘××§×‘×™×œ; ×©×™××•×© ×‘×›×œ×™× ×”××•×“×“×™× ××ª ××™×›×•×ª ×”××¡××›×™× ×©× ××¦××•, ×›×š ×©× ×™×ª×Ÿ ×™×”×™×” ×œ×œ×§×˜ ××ª ×§×˜×¢×™ ×”××™×“×¢ ×”×¨×œ×•×•× ×˜×™×™× ×‘×™×•×ª×¨ ××ª×•×š ×”××¡××›×™× ×©×—×–×¨×•; ××•×“×œ×™ ×©×¤×” ×©×™×•×“×¢×™× ×œ×‘×§×© ×‘×™×¦×•×¢ ××—×–×•×¨ × ×•×¡×£ ×× ×”× ×œ× ××¦××• ×¢×“×™×™×Ÿ ××ª ×”×ª×©×•×‘×” ×©×—×™×¤×©×•; ×•×”×¨×©×™××” ×××©×™×›×”, ×›×™××” ×œ×ª×—×•× ××—×§×¨ ×¤×¢×™×œ ×‘×™×•×ª×¨.×”××ª×’×¨ ×”×¢×™×§×¨×™ ×‘×‘× ×™×™×ª ××¢×¨×›×ª RAG ×”×•× ×œ×”×’×™×¢ ×œ×¨××ª ××™×›×•×ª ×•×“×™×•×§ ××§×¡×™××œ×™×™×. ×›×œ ×—×œ×§×™ ×”××¢×¨×›×ª ××©×¤×™×¢×™× ×¢×œ ×”×ª×©×•×‘×•×ª ×”×¡×•×¤×™×•×ª, ×•×œ×›×Ÿ ××•×›×¨×—×™× ×œ×ª×›× ×Ÿ ××•×ª×” ×‘×§×¤×“× ×•×ª. ×›×—×œ×§ ××”×”×›× ×”, ×›×“××™ ×œ×‘×—×•×Ÿ ×•×œ×”×©×•×•×ª ××•×“×œ×™ ×©×¤×” ×¢×œ ×× ×ª ×œ×”×’×™×¢ ×œ×©×™×œ×•×‘ ×¨××•×™ ×©×œ ×¢×œ×•×ª ××•×œ ×“×™×•×§. ×‘×—×™×¨×ª ×××’×¨×™ ×”×™×“×¢, ××•×¤×™ ×”××—×¡×•×Ÿ, ××—×–×•×¨ ×•×”×¦×’×ª ×”×™×“×¢ ×œ××•×“×œ ×”× ×§×¨×™×˜×™×™× ×‘×™×™×¦×•×¨ ×ª×©×•×‘×•×ª × ×›×•× ×•×ª ×•×¨×œ×•×•× ×˜×™×•×ª ×‘×–××Ÿ ×¨×™×¦×” ×¡×‘×™×¨. ×›×“×™ ×œ×‘×—×•×Ÿ ××ª ×˜×™×‘ ×”××¢×¨×›×ª ×©×‘× ×™×ª×, × ×™×ª×Ÿ ×œ×”×©×ª××© ×‘×“×•×’×××•×ª ××ª×•×™×’×•×ª (×“×•×’×××•×ª ×”××›×™×œ×•×ª ×ª×©×•×‘×•×ª ×™×“×•×¢×•×ª ××¨××©) ×•×œ×‘×¦×¢ ×‘×—×™× ×” ×™×“× ×™×ª ××• ××•×˜×•××˜×™×ª ×©×œ ×”×ª×•×¦××•×ª.×”××•×¨×›×‘×•×ª ×”×’×“×•×œ×” × ×•×‘×¢×ª ×‘×¢×™×§×¨ ××”×”×ª× ×”×’×•×ª ×”×œ× ×¦×¤×•×™×” ×œ×¢×ª×™× ×©×œ LLMs. ×œ×“×•×’××”, ×‘×—×œ×§ ×§×˜×Ÿ ××”××§×¨×™× ××•×“×œ ×”×©×¤×” ×™×©×’×”, ××¤×™×œ×• ×©×”×™×“×¢ ×©×§×™×‘×œ ××›×™×œ ××ª ×”×ª×©×•×‘×” ×”× ×›×•× ×”. ×”×¡×™×‘×•×ª ×œ× ×‘×¨×•×¨×•×ª × ×•×‘×¢×•×ª ××¡×ª×™×¨×” ×‘×™×Ÿ ×”×™×“×¢ ×”×¤× ×™××™ ×©×œ ×”××•×“×œ ×œ×™×“×¢ ×”××•×¦×’ ×‘×¤× ×™×•. ×”×¦×•×•×ª ×©×œ× ×• ×—×•×§×¨ ×“×¨×›×™× ×‘×”×Ÿ × ×™×ª×Ÿ ×œ×©×œ×•×˜ ×‘×”×ª× ×”×’×•×ª ×”××•×“×œ×™× ×‘××§×¨×™× ×›××œ×”, ×‘×¢×–×¨×ª ×”×•×¨××•×ª ××•×ª×××•×ª ×•××™××•×Ÿ × ×•×¡×£.×™×—×“ ×¢× ×–××ª, × ×™×ª×Ÿ ×œ×©×¤×¨ ××ª ××™×›×•×ª ×”××¢×¨×›×ª ×›×•×œ×” ×‘×××¦×¢×•×ª ×”×ª×××ª ×”××•×“×œ ×œ×‘×™×¦×•×¢ ××©×™××•×ª RAG. ×œ×¦×•×¨×š ×›×š, ×”×¦×•×•×ª ×©×œ× ×• ×¤×™×ª×— ×›×œ×™ ×§×•×“ ×¤×ª×•×— ×©×××¤×©×¨ ×œ×××Ÿ ×•×œ×©×¤×¨ ××ª ×™×›×•×œ×•×ª ×”-RAG ×©×œ ××•×“×œ×™ ×©×¤×”.×›×“×™ ×œ×‘× ×•×ª ××¢×¨×›×ª RAG ××™×›×•×ª×™×ª ×•××“×•×™×§×ª, × ×“×¨×©×ª ×”×‘× ×” ×¢××•×§×” ×©×œ ×”×”×™×‘×˜×™× ×”×©×•× ×™× ×©×œ×”. ×›××•×‘×Ÿ ×©××™ ××¤×©×¨ ×œ×•×•×ª×¨ ×¢×œ ×ª×”×œ×™×š × ×™×¡×•×™ ×•×˜×¢×™×”, ×©×¢×•×–×¨ ×œ×©×¤×•×š ××•×¨ ×¢×œ ×”×¤×©×¨×•×ª ×”×©×•× ×•×ª ×”×›×¨×•×›×•×ª ×‘×¢×™×¦×•×‘ ×”××¢×¨×›×ª. ×¨×§ ×›×š × ×•×›×œ ×œ×‘× ×•×ª ××ª ×”××¢×¨×›×ª ×”××™×“×™××œ×™×ª ×œ×‘×¢×™×” ×©××•×ª×” ×× ×—× ×• ×× ×¡×™× ×œ×¤×ª×•×¨.××¢×¨×›×•×ª RAG ××™×™×¦×’×•×ª ××¨×›×™×˜×§×˜×•×¨×” ×—×“×©×” ×”××©×œ×‘×ª ××•×“×œ×™ ×©×¤×” ×¢× ×××’×¨×™ × ×ª×•× ×™×. ×‘×–×›×•×ª ×”×©×™×œ×•×‘ ×”×–×” ×™×© ×œ×”× ×¤×•×˜× ×¦×™××œ ×œ×‘×¦×¢ ××©×™××•×ª ×¢×ª×™×¨×•×ª ×™×“×¢, ×›×’×•×Ÿ ×¢×•×–×¨×™× ×“×™×’×™×˜×œ×™×™× ××•×ª×××™× ××™×©×™×ª, ×¦â€™××˜×‘×•×˜ ×‘×©×™×¨×•×ª ×œ×§×•×—×•×ª ×•××¢×¨×›×•×ª ×™×“×¢ ××¨×’×•× ×™×•×ª. ×•×× ×œ×©×¤×•×˜ ×œ×¤×™ ×”××—×§×¨ ×”×¤×¢×™×œ ×‘×ª×—×•×, ×–×•×”×™ ×¨×§ ×”×”×ª×—×œ×”.×”×›×•×ª×‘ ×”×•× ×—×•×§×¨ ×‘××¢×‘×“×ª ×”Ö¾NLP ×‘××¨×’×•×Ÿ Intel Labs. ×‘××¢×‘×“×” × ×—×§×¨×•×ª ×¡×•×’×™×•×ª ×”×§×©×•×¨×•×ª ×œ××•×“×œ×™ ×©×¤×” ×›×’×•×Ÿ RAG ,Efficient Inference, ×¢×‘×•×“×” ×¢× ×§×•× ×˜×§×¡×˜×™× ××¨×•×›×™×, ×©×™××•×© ×‘×¡×•×›× ×™× ×•×¢×•×“.××™× ×˜×œ ×××©×™×›×” ×œ×”×•×‘×™×œ ××ª ×ª×—×•× ×”×‘×™× ×” ×”××œ××›×•×ª×™×ª ×¢× ×”×¤×ª×¨×•× ×•×ª ×”××ª×§×“××™× ×‘×™×•×ª×¨ ×œ×ª×¢×©×™×™×”. ××¢×‘×“×™ Xeon ××”×“×•×¨ ×”×©×™×©×™ ×•×”×××™×¦×™× ×”×™×™×¢×•×“×™×™× Gaudi 3 ×××¤×©×¨×™× ×œ××¨×’×•× ×™× ×œ×”××™×¥ ××ª ×¤×™×ª×•×— ×•×”×˜××¢×ª ×™×™×©×•××™ AI ×‘×§× ×” ××™×“×” ×’×“×•×œ, ×ª×•×š ×©××™×¨×” ×¢×œ ×™×¢×™×œ×•×ª ×’×‘×•×”×” ×•×ª××•×¨×” ×›×œ×›×œ×™×ª ×™×•×¦××ª ×“×•×¤×Ÿ. ××™× ×˜×œ ××¦×™×¢×” ×’×™×©×” ×¤×ª×•×—×” ×•×’××™×©×” ×”×××¤×©×¨×ª ×©×™×œ×•×‘ ×—×œ×§ ×©×œ ×—×•××¨×” ×•×ª×•×›× ×” ×××’×•×•×Ÿ ×¡×¤×§×™×, ×•×‘×›×š × ×•×ª× ×ª ×œ××¨×’×•× ×™× ××ª ×”×›×œ×™× ×”×“×¨×•×©×™× ×œ×”× ×œ×”××¦×ª ×”×©×™××•×© ×‘- GenAI ×•×‘××•×“×œ×™× ×’×“×•×œ×™×, ×›××• ×’× ×œ×”×§×˜× ×ª ×”×ª×œ×•×ª ×‘××¢×¨×›×•×ª ×§× ×™×™× ×™×•×ª ×©×œ ×™×¦×¨× ×™× ××—×¨×™×. ××¨×›×–×™ ×”×¤×™×ª×•×— ×©×œ ××™× ×˜×œ ×‘×™×©×¨××œ, ×©×××•×§××™× ×‘×—×™×¤×”, ×¤×ª×— ×ª×§×•×•×”, ×™×¨×•×©×œ×™× ×•×§×¨×™×™×ª ×’×ª, ××©×—×§×™× ×ª×¤×§×™×“ ××¤×ª×— ×‘×¢×™×¦×•×‘ ×”×“×•×¨ ×”×‘× ×©×œ ×˜×›× ×•×œ×•×’×™×•×ª ×¢×™×‘×•×“ ×•-AI , ×•×××©×™×›×™× ×œ×”× ×™×¢ ××ª ×”×—×“×©× ×•×ª ×”×’×œ×•×‘×œ×™×ª ×©×œ ×”×—×‘×¨×” ×‘×××¦×¢×•×ª ×©×™×œ×•×‘ ×©×œ ×‘×™×¦×•×¢×™×, ×’××™×©×•×ª ×•×—×“×©× ×•×ª ××ª××“×ª.×›××Ÿ ××¤×©×¨ ×œ×‘×—×•×¨ ×ª×—×•××™ ×¢× ×™×™×Ÿ, ×•×× ×—× ×• × ×ª××™× ×œ×š ×›×ª×‘×•×ª ×‘××•×¤×Ÿ ××™×©×™. ×”×›×ª×‘×•×ª ×™×•×¤×™×¢×• ×›××Ÿ ×•×‘×¨×—×‘×™ ×”××ª×¨, ×•×”×¡×™××•×Ÿ ×©×œ× ×• ×™×”×™×” ×”× ×” ×”×›×ª×‘×•×ª ×©×”×ª××× ×• ×œ×š ××™×©×™×ª. ×¨×•×¦×” ×œ×¨×¢× ×Ÿ ×”×¢×“×¤×•×ª? ×‘×‘×§×©×”, ×× ×—× ×• ×œ× ×©×•×¤×˜×™× ×–×” ×”××§×•× ×œ×”×›×™×¨ ××ª ×”×—×‘×¨×•×ª, ×”××©×¨×“×™× ×•×›×œ ××™ ×©×¢×•×©×” ××ª ×”×”×™×™×˜×§ ×‘×™×©×¨××œ (×•×™×© ×’× ××œ× ××©×¨×•×ª ×¤×ª×•×—×•×ª!) #×ª×•×›×Ÿ ××§×•×“××”× ×™×•×–×œ×˜×¨ ×©×œ× ×• ×¢×•×©×” ××ª ×”××§×¡×˜×¨×” ××™×™×œ ×¢× ×”×¢×“×›×•× ×™× ×•×”×—×“×©×•×ª ×©×œ ×”×©×‘×•×¢Â© ×›×œ ×”×–×›×•×™×•×ª ×©××•×¨×•×ª ×œ×’×™×§×˜×™×™××¤×™×ª×•×— ××ª×¨×™×designed by designed byÂ Â | ×¤×™×ª×•×— ××ª×¨×™××‘×’×œ×œ ×–×” ×× ×—× ×• ××§×¤×™×“×™× ×©×”×Ÿ ×œ× ×™×¦×™×§×•, ××‘×œ ×”×Ÿ ×××¤×©×¨×•×ª ×œ× ×• ×œ×ª×ª ×œ×›× ×ª×•×›×Ÿ ×‘×—×™× ×.</p> <p>×¤×¨×¡×•××•×ª ×¢×•×–×¨×•×ª ×œ× ×• ×œ×”×ª×§×™×™× ×•×œ×”×ª××§×“ ×‘××” ×©×—×©×•×‘: ×œ×™×¦×•×¨ ×¢×‘×•×¨×š ×ª×•×›×Ÿ ××§×¦×•×¢×™ ×•××¢× ×™×™×Ÿ. ×›×“×™ ×œ×”××©×™×š ×œ×™×”× ×•×ª ××’×™×§×˜×™×™×, ×›×“××™ ×œ×”×¡×™×¨ ××ª ×”×—×¡×™××” ××”××ª×¨ ×©×œ× ×•. ×× ×—× ×• ××‘×˜×™×—×™× ×œ× ×œ×”×¦×™×£.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[××¢×¨×›×•×ª RAG ×™×›×•×œ×•×ª ×œ×¤×ª×•×¨ ××ª ×ª×•×¤×¢×ª ×”×”×–×™×•×ª ×©×œ LLMs, ××‘×œ ×›×“×™ ×œ×‘× ×•×ª ××—×ª ×˜×•×‘×” ×ª×¦×˜×¨×›×• ×œ×§×‘×œ ×”×¨×‘×” ×”×—×œ×˜×•×ª ×§×¨×™×˜×™×•×ª (×•×œ×§×•×•×ª ×©×ª×’×™×¢×• ×”×›×™ ×§×¨×•×‘ ×œ××•×©×œ×)]]></summary></entry><entry><title type="html">Intel Labs Introduces RAG-FiT Open-Source Framework for Retrieval Augmented Generation in LLMs - Intel Community</title><link href="https://danielfleischer.github.io/blog/2024/intel-labs-introduces-rag-fit-open-source-framework-for-retrieval-augmented-generation-in-llms-intel-community/" rel="alternate" type="text/html" title="Intel Labs Introduces RAG-FiT Open-Source Framework for Retrieval Augmented Generation in LLMs - Intel Community"/><published>2024-10-09T00:00:00+00:00</published><updated>2024-10-09T00:00:00+00:00</updated><id>https://danielfleischer.github.io/blog/2024/intel-labs-introduces-rag-fit-open-source-framework-for-retrieval-augmented-generation-in-llms---intel-community</id><content type="html" xml:base="https://danielfleischer.github.io/blog/2024/intel-labs-introduces-rag-fit-open-source-framework-for-retrieval-augmented-generation-in-llms-intel-community/"><![CDATA[<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>                        Success!  Subscription added.
                    
                   
                
                    
                        Success!  Subscription removed.
                    
                    
                
                    
                        Sorry, you must verify to complete this action. Please click the verification link in your email. You may re-send via your
                        profile.
                    
                Scott Bair is a key voice at Intel Labs, sharing insights into innovative research for inventing tomorrowâ€™s technology.Â Intel Labs researchers Daniel Fleischer, Moshe Berchansky, and Moshe Wasserblat collaborated on RAG-FiT.HighlightsIntel Labs introduces RAG-FiT, an open-source framework for augmenting large language models (LLMs) for retrieval-augmented generation (RAG) use cases. Available under an Apache 2.0 license, RAG-FiT integrates data creation, training, inference, and evaluation into a single workflow, assisting in the creation of data-augmented datasets for training and evaluating LLMs in RAG settings. This integration enables rapid prototyping and experimentation with various RAG techniques, allowing users to easily generate datasets and train RAG models using internal or specialized knowledge sources.The library assists in creating data to train models using parameter-efficient fine-tuning (PEFT), which allows users to finetune a subset of parameters in a model. The Python-based framework is designed to serve as an end-to-end experimentation environment, enabling users to quickly prototype and experiment with different RAG techniques, including data selection, aggregation and filtering, retrieval, text processing, document ranking, few-shot generation, prompt design using templates, fine-tuning, inference, and evaluation.To demonstrate the effectiveness of the RAG-FiT framework (formerly known as RAG Foundry), Intel Labs researchers augmented and fine-tuned Llama 3.0 and Phi-3 models with diverse RAG configurations, showcasing consistent improvements across three knowledge-intensive question-answering tasks.Using RAG Systems to Address LLM LimitationsDespite their impressive capabilities, LLMs have inherent limitations. These models can produce plausible sounding but incorrect or nonsensical answers, struggle with factual accuracy, lack access to up-to-date information after their training cutoff, and struggle in attending to relevant information in large contexts.RAG enhances LLMs performance by integrating external information using retrieval mechanisms. Retrieving specific data from knowledge bases outside the model can effectively address knowledge limitations, which in turn can reduce hallucinations, improve the relevance of generated content, provide interpretability and could be vastly more cost efficient. Furthermore, recent research indicates that fine-tuning LLMs for RAG can achieve state-of-the-art performance, surpassing that of larger proprietary models.How RAG-FiT WorksAs an experimentation environment for researchers, the backbone of the RAG-FiT library consists of four distinct modules: data creation, training, inference, and evaluation. Each module is encapsulated and controlled by a configuration file, ensuring compatibility between the output of one module and the input of the next file. This modular approach allows isolation and independent experimentation on each step, enabling the production of multiple outputs and the concurrent execution of numerous experiments. Evaluation can be conducted on the generated outputs as well as on any feature within the data, including retrieval, ranking, and reasoning.Figure 1. In the RAG-FiT framework, the data augmentation module saves RAG interactions into a dedicated dataset, which is then used for training, inference, and evaluation.Dataset creation: The processing module facilitates the creation of context-enhanced datasets by persisting RAG interactions, which are essential for RAG-oriented training and inference. These interactions encompass dataset loading, column normalization, data aggregation, information retrieval, template-based prompt creation, and various other forms of pre-processing. The processed data can be saved in a consistent, model-independent format, along with all associated metadata, ensuring compatibility and reproducibility across different models and experiments.The processing module supports the handling of multiple datasets at once through global dataset sharing. This feature allows each step of the pipeline to access any of the loaded datasets, enhancing flexibility and allowing for complex processing procedures. Furthermore, the module includes step caching, which caches each pipeline step locally. This improves compute efficiency, and facilitates easy reproduction of results.Training: Users can train any model on the augmented datasets. A training module is used to fine-tune models from the datasets created by the previous processing module. The training module relies on the well-established training framework, TRL, for transformer reinforcement learning. The module also supports advanced efficient training techniques, such as PEFT and low-rank adaptation (LoRA) to customize the LLM for specific use cases without retraining the entire model.Inference: The inference module can generate predictions using the augmented datasets with trained or untrained LLMs. Inference is conceptually separated from the evaluation step, since it is more computationally demanding than evaluation. Additionally, users can run multiple evaluations on a single prepared inference results file.Evaluation: Custom metrics can be easily implemented or users can run current metrics, including Exact Match (EM), F1 Score, ROUGE, BERTScore, DeepEval, Ragas, Hugging Face Evaluate, and classification. Users can run metrics locally on each example, or globally on the entire dataset, such as recall for classification-based metrics. In addition to input and output texts, metrics can utilize any feature in the dataset, such as retrieval results, reasoning, citations, and attributions. In addition, the evaluation module uses a processing step called an Answer Processor, which can implement custom logic and perform many tasks, including cleaning and aligning outputs.Performance of RAG-FiT Augmentation TechniquesTo illustrate the utility of the framework, Intel Labs researchers conducted experiments involving retrieval, fine-tuning, chain-of-thought (CoT) reasoning, and a negative distractor documents technique. The team compared Llama 3.0 and Phi-3, two widely accepted baseline models, using enhancement methods across TriviaQA, PubMedQA, and ASQA, three knowledge-intensive question-answering datasets. The TriviaQA and PubMedQA datasets contain relevant context, while for the ASQA dataset, retrieval was done over a Wikipedia corpus using a dense retriever.The team measured and reported EM for TriviaQA, STR-EM for ASQA, and accuracy and F1 Score for PubMedQA. In addition, researchers evaluated two Ragas metrics: faithfulness (the relation between the generated text and the context) and relevancy (the generated text and the query). Overall, the two models showed consistent improvements across the three knowledge-intensive question-answering tasks.Figure 2. Evaluation results of baseline and different RAG settings for the three datasets and two models tested. In bold are the best configurations per dataset, based on the main metrics.For TriviaQA, retrieved context improved the results, fine-tuning the RAG setting boosted the results, but fine-tuning on CoT reasoning (which includes training on a combination of gold passages and distractor passages) decreased performance. For this dataset, the best method is model dependent. For ASQA, every method improved upon the baseline, CoT reasoning produced consistent improvement in both models, as well as fine-tuning of the CoT configuration, which performed best. Finally, for PubMedQA, almost all methods improved upon the baseline (with one exception), CoT reasoning improved on the untrained RAG setting, but for fine-tuning, the RAG method performed best in both models.Finally, the faithfulness and relevancy scores often did not correlate with the main metrics, or with each other, possibly indicating they capture different aspects of the retrieval and generated results, and represent a trade-off in performance.The results demonstrate the usefulness of RAG techniques for improving performance, as well as the need to carefully evaluate different aspects of a RAG system on a diverse set of datasets.
					You must be a registered user to add a comment. If you've already registered, sign in. Otherwise, register and sign in.
				Community support is provided Monday to Friday. Other contact methods are available here.Intel does not verify all solutions, including but not limited to any file transfers that may appear in this community. Accordingly, Intel disclaims all express and implied warranties, including without limitation, the implied warranties of merchantability, fitness for a particular purpose, and non-infringement, as well as any warranty arising from course of performance, course of dealing, or usage in trade.For more complete information about compiler optimizations, see our Optimization Notice.
</code></pre></div></div>]]></content><author><name></name></author><summary type="html"><![CDATA[Scott Bair is a key voice at Intel Labs , sharing insights into innovative research for inventing tomorrowâ€™s technology. Intel Labs researchers]]></summary></entry><entry><title type="html">Open Domain Q&amp;amp;A using Dense Retrievers in fastRAG | by Daniel Fleischer | Medium</title><link href="https://danielfleischer.github.io/blog/2023/open-domain-qa-using-dense-retrievers-in-fastrag-by-daniel-fleischer-medium/" rel="alternate" type="text/html" title="Open Domain Q&amp;amp;A using Dense Retrievers in fastRAG | by Daniel Fleischer | Medium"/><published>2023-08-23T00:00:00+00:00</published><updated>2023-08-23T00:00:00+00:00</updated><id>https://danielfleischer.github.io/blog/2023/open-domain-qa-using-dense-retrievers-in-fastrag--by-daniel-fleischer--medium</id><content type="html" xml:base="https://danielfleischer.github.io/blog/2023/open-domain-qa-using-dense-retrievers-in-fastrag-by-daniel-fleischer-medium/"><![CDATA[<p>Sign upSign inSign upSign inâ€“1ListenShareRetrieval augmented generation is an advanced technique in the field of natural language processing that combines the power of information retrieval and generative models. It aims to generate more informative and contextually appropriate responses to user queries by leveraging the retrieval of relevant passages or documents. This technique has significant potential in various applications, among them is Open Domain Question Answering; it is a field of research that focuses on developing systems capable of comprehending and answering a wide range of questions posed by users. It involves extracting relevant information from vast amounts of unstructured data using information retrieval techniques.Information retrieval used to rely on sparse techniques based on word statistics. In the traditional approach, documents were represented by a bag-of-words model, where the presence or absence of specific words determined the relevance of a document to a query. Score functions, like BM25 and TF-IDF, use word frequencies to score documents, balancing how frequently a keyword appears in a document versus how prevalent the word is in general. One popular DB is the Elasticsearch DB which uses the Lucene text search engine; it is based on word statistics and edit distance (a syntax method).With advancements in natural language processing and machine learning, information retrieval has shifted towards denser representations using embeddings. Embeddings capture the semantic meaning of words and phrases, allowing for a more nuanced understanding of the content; this enables more accurate matching of queries with relevant documents, as embeddings can capture subtle semantic similarities that traditional word statistics fail to capture. The shift from sparse to dense representations has significantly improved the performance and precision of retrieval systems. For an introduction to neural IR, see (Mitra and Craswell 2018); for a review of IR for Q&amp;A, see (Abbasiantaeb and Momtazi 2021).This blog serves as an introduction to dense retrieval and we focus on two dense document retrieval models: Dense Passage Retrieval (DPR, Karpukhin et al. 2020) and Contextualized Late Interaction over BERT (ColBERT, Khattab and Zaharia 2020). Both models use Semantic Search to find the relevant documents. Semantic search means we use a textâ€™s dense representation to measure similarity between a given query and the potential relevant documents. The two models use different methods of storing the documentsâ€™ vectors and measuring similarity between queries and documents. We will compare the models by measuring accuracy and latency on a known benchmark, called Natural Questions (NQ, Kwiatkowski et al. 2019), a collection of user submitted questions where answers can be found in Wikipedia articles.We would like to introduce fastRAG, a framework developed at Intel Labs and released as an open-source software recently. The goal of the framework is to enable rapid research and development of retrieval-augmented generative AI applications. These can be used for generative tasks such as question answering, summarization, dialogue systems, and content creation, while utilizing information-retrieval components to anchor LLM output using external knowledge.An application is represented by a pipeline, typically comprised of a knowledge-base (KB), retriever, ranker and a reader, typically an LLM, which â€œreadsâ€ the query and retrieved documents, and generates an output. One can experiment with different architectures, models, benchmarking the results for performance and latency. Several of the models we offer are better suited for Intel hardware, achieving lower latency with comparable accuracy; on that in the next blog post.In the field of information retrieval, relatively recent updates promote the use of transformer encoder models as retrievals: documents in the knowledge-base are encoded as vectors and stored in an index. At runtime, the query is encoded as a vector and vector similarity search is used to find the most relevant documents. Similar process is used in re-ranking retrieved documents, where the encoding is done on-the-fly, specifically for the retrieved documents.Among the dense retrievals there are several approaches. One approach is to use a single tokenâ€™s embeddings as a representative of the entire document. DPR (Karpukhin et al. 2020) is an example of that approach, where the encoders are trained to â€œsummarizeâ€ the entire document in the first tokenâ€™s embeddings. The method is a form of a bi-encoder, since it uses two encoders, one for the query and another for the documents; see illustration.Another approach is called Late Interaction, as defined first in ColBERT (Khattab and Zaharia 2020). The idea is to save (and index) the encoded vectors for all the words in the documents. At run-time the query vectors are compared with all the documents wordsâ€™ vectors (hence the â€œlateâ€ in late interaction) thus retrieving more relevant documents than DPR. Notice that indexing every token, instead of just the first token for each document, can increase the index size.Later refinements to this work, namely ColBERT v2 and PLAID (Santhanam, Khattab, Saad-Falcon, et al. 2022; Santhanam, Khattab, Potts, et al. 2022) helped reduce the index size and latency time thanks to two main improvements: first is quantization and compression of the vectors in the index. Secondly is a set of heuristics that cluster the vectors using the K-means algorithm, hierarchically choose the relevant documentsâ€™ tokens for the query tokens based on the clustersâ€™ centroids. ColBERT v2 with PLAID index achieves state of the art retrieval performance with a low latency, close to the order of sparse retrieval (BM25, Lucene, Elasticsearch, etc.) but with much higher accuracy.The first step is creating a documents store of the type PLAIDDocumentStore. The store requires three paths: checkpoint, collection and an index.A ColBERT checkpoint is an encoder model, fine tuned for the task of retrieving. Itâ€™s based on a BERT architecture. One can download a trained checkpoint, for example here, trained by the paper authors. Encoders can be fine-tuned using these instructions: training. Next is the collection of documents which comprise the corpus. The collection should be a signle tsv file with columns: id, text, title (optional). Finally, the index is the vectors index created using the same checkpoint, encoding all tokens in the corpus, compressing and saving the result.We provide a script to create a PLAID vector index using a ColBERT encoder and a documents collection in here.Once we have all the ingredients, we initialize the document store:Next we define a retriever using the document store we just define:We define a pipeline; it has the following form:We can use the Haystack pipeline API to connect with external components. In this example the pipeline contains just the retriever:Running the queries through the pipeline is very easy:The results is a hash map with documents key containing a list of results: documents with relevancy scores.To test ColBERT, we will use the Natural Questions benchmark (Kwiatkowski et al. 2019). The external knowledge is a collection of Wikipedia passages.As a baseline, weâ€™ll use the original implementation of DPR, together with a checkpoint that was fine-tuned on Natural Questions, see download instructions. DPR model is released under the CC-BY-NC 4.0 license.DPR uses the Faiss vector search library (Johnson, Douze, and JÃ©gou 2019). We test two configurations for storing the vectors: flat and HNSW. Flat is slow but accurate, since an exhaustive similarity search is done. HNSW (Malkov and Yashunin 2018) is an approximate vector search method; the vectors are organized into a graph to enable faster than linear search. Building an optimal HNSW graph requires some parameter tuning; these control the trade-off between speed, accuracy and index size.For ColBERT, we use the ColBERTv2 checkpoint from here, which was fine-tuned on the MS MARCO (Bajaj et al. 2018) dataset, which comprised of Bing questions and answers based on web search results.We report recall and MRR values for k values of 5, 10, 20, 50, and 100. We also measure latency (at k=100) (ms/query), and report the vector index size in GBs, as there is a trade-off between performance and accuracy.Measurements done on an Intel AWS instance, with a Xeon processor. AWS instance type is r6i.16xlarge; 32 cores, 512GB RAM, Intel(R) Xeon(R) Platinum 8375C CPU @ 2.90GHz. Conda 23.1.0, python 3.9.16, AMI image ami-0f1a5f5ada0e7da53, Amazon Linux 2, version 5.10.177-158.645.amzn2.x86_64. The Wikipedia text collection size is 13GB.First, we note there is a difference in accuracy between the ColBERT model and DPR. The quality of the embeddings generated from a trained encoder is crucial for high quality retrieval. As the DPR encoders were fine-tuned on the Natural Questions dataset, this is probably one of the reasons explaining the difference.Next, we compare the two indexing methods for DPR: flat and HNSW. Flat index query takes 35x longer than HNSW, at almost 1.5 seconds per query. HNSW is faster, with only a small accuracy penalty; however, index size is bigger, at ~2.3x the size of the flat index.It is notable that although ColBERT encodes and stores all the documents tokens, thanks to its optimizations, the index size is comparable to a flat Faiss index, storing only the first tokenâ€™s embedding for each document.One of the goals was to present the clear trade off between accuracy and performance, more specifically, between recall, latency and memory usage (the index size, as these are stored in-memory). To summarize, we introduced two dense retrieval algorithms, ColBERT with a PLAID index and DPR. We tested these on the open-domain Q&amp;A dataset Natural Questions, measuring accuracy and latency.Experience the capabilities of ColBERT in fastRAG through the following Notebook example. Familiarize yourself with fastRAG by exploring our user-friendly UI demos at Running Demos in fastRAG. Start using the ColBERT encoder, accessible from the HuggingFace hub. Easily create a document index, as detailed in our guide at Indexing in fastRAG. Furthermore, we offer full support for the DPR retriever; see example DPR configuration. Unleash the potential of fastRAG and revolutionize your workflow today!Tests done by Intel on March 14th, 2023.Performance varies by use, configuration and other factors. Learn more at www.Intel.com/PerformanceIndex.Â© Intel Corporation. Intel, the Intel logo, and other Intel marks are trademarks of Intel Corporation or its subsidiaries. Other names and brands may be claimed as the property of others.Abbasiantaeb, Zahra, and Saeedeh Momtazi. 2021. â€œText-Based Question Answering from Information Retrieval and Deep Neural Network Perspectives: A Survey.â€ Wires Data Mining and Knowledge Discovery 11 (6): e1412. https://doi.org/10.1002/widm.1412.Bajaj, Payal, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, et al. 2018. â€œMS MARCO: A Human Generated MAchine Reading COmprehension Dataset.â€ October 31, 2018. https://doi.org/10.48550/arXiv.1611.09268.Johnson, Jeff, Matthijs Douze, and HervÃ© JÃ©gou. 2019. â€œBillion-Scale Similarity Search with GPUs.â€ Ieee Transactions on Big Data 7 (3): 535â€“47. https://doi.org/10.1109/TBDATA.2019.2921572.Karpukhin, Vladimir, Barlas OÄŸuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. â€œDense Passage Retrieval for Open-Domain Question Answering.â€ September 30, 2020. https://doi.org/10.48550/arXiv.2004.04906.Khattab, Omar, and Matei Zaharia. 2020. â€œColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT.â€ June 4, 2020. https://doi.org/10.48550/arXiv.2004.12832.Kwiatkowski, Tom, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, et al. 2019. â€œNatural Questions: A Benchmark for Question Answering Research.â€ Transactions of the Association for Computational Linguistics 7 (August): 453â€“66. https://doi.org/10.1162/tacl_a_00276.Malkov, Yu A., and D. A. Yashunin. 2018. â€œEfficient and Robust Approximate Nearest Neighbor Search Using Hierarchical Navigable Small World Graphs.â€ August 14, 2018. https://doi.org/10.48550/arXiv.1603.09320.Mitra, Bhaskar, and Nick Craswell. 2018. â€œAn Introduction to Neural Information Retrieval.â€ Foundations and TrendsÂ® in Information Retrieval 13 (1): 1â€“126. https://doi.org/10.1561/1500000061.Santhanam, Keshav, Omar Khattab, Christopher Potts, and Matei Zaharia. 2022. â€œPLAID: An Efficient Engine for Late Interaction Retrieval.â€ May 19, 2022. https://doi.org/10.48550/arXiv.2205.09707.Santhanam, Keshav, Omar Khattab, Jon Saad-Falcon, Christopher Potts, and Matei Zaharia. 2022. â€œColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction.â€ July 10, 2022. https://doi.org/10.48550/arXiv.2112.01488.â€”-1Research scientist at Intel LabsHelpStatusAboutCareersPressBlogPrivacyRulesTermsText to speech</p>]]></content><author><name></name></author><summary type="html"><![CDATA[We introduce 2 dense retrieval algorithms, ColBERT with a PLAID index and DPR. We tested these on the open-domain Q&A dataset Natural Questions, measuring accuracy and latency.]]></summary></entry></feed>